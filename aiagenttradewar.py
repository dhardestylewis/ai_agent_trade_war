# -*- coding: utf-8 -*-
"""AIAgentTradeWar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p5r66bly9PPOvQJ6uXdGtFCtX7j3gQm5

# Technical Whitepaper: A Simulation of Global Trade Dynamics using Agent-Based Modeling and GPT-4 Integration

## 1. Introduction and Abstract

In an era where global trade policies and their implications are becoming increasingly complex, a detailed and dynamic simulation tool can provide invaluable insights for policymakers, business leaders, and researchers. This whitepaper presents an innovative simulation framework designed to model global trade dynamics, political decision-making, and economic outcomes using a combination of agent-based modeling (ABM) and GPT-4 large language model (LLM) integration.

Our simulation uses real-world economic, political, and social data from a variety of authoritative sources to create agents representing governments, companies, consumers, and intermediaries. These agents act according to data-driven "disposition matrices" (scenario-action probability tables) and "modifier matrices" (relation modifiers) that are partially generated by GPT-4 based on relevant context. Through iterative simulation steps, the model explores a wide range of probable future scenarios over a six-month horizon, updating policies and outcomes in response to changing conditions such as trade volumes, political climates, economic indicators, public sentiment, and upcoming elections.

This whitepaper comprehensively describes the data sources used (including UN Comtrade, World Bank, IMF, WTO, GTA, USITC, Trading Economics, U.S. Census Bureau, political data from Wikipedia, and more), the variables ingested from these sources, the agent-based modeling framework, and the integration of GPT-4 to drive agent behaviors and scenario generation. It details how scenario branching is performed to capture different future possibilities and how we measure outcomes for scenario selection, iteration, and further simulations.

## 2. Background and Motivation

### 2.1 Need for Advanced Trade Simulations

Global trade dynamics and policies are influenced by a wide range of economic, political, and social factors. Traditional modeling approaches often rely on static or simplified assumptions that do not adequately capture the complexity and interplay of factors affecting international trade. Our simulation model addresses these complexities by combining detailed data-driven approaches with advanced agent-based modeling, allowing for realistic and adaptive policymaking simulations.

### 2.2 Integrating Agent-Based Modeling and GPT-4

Agent-based modeling (ABM) provides a framework where individual agents, representing countries, companies, and consumers, interact with each other and evolve based on defined rules and behaviors. GPT-4, a large language model, can enhance ABM by dynamically generating decision rules (actions and outcomes) based on scenarios extracted from real-world data. GPT-4 can synthesize complex datasets into coherent strategies and policies that government agents might realistically undertake, providing a more human-like decision-making process within the simulation.

### 2.3 Goals of the Simulation

The primary goals of this simulation framework are to:

- Predict how changes in tariffs, trade agreements, or political climates affect global trade volumes and economic outcomes.
- Evaluate the influence of political scenarios, like upcoming elections or political scandals, on policy decisions and their consequent economic impacts.
- Explore how public sentiment shifts, driven by real-time news and social media data, can influence government policies.
- Provide a range of plausible scenarios and outcomes by branching through various future events and measures of trade volume, ensuring robust scenario analysis for policymakers and stakeholders.

## 3. Detailed Architecture and Data Usage

### 3.1 Data Sources and Variables Acquired

Our simulation model relies on an extensive range of data sources to ensure accuracy and relevance. The data acquisition process is designed to pull economic, political, social, and legislative data from primary and backup sources. Below is an overview of each data source, the variables we pull from them, and their application in the simulation:

**1. UN Comtrade Data**
   - **Source**: United Nations Comtrade Database
   - **API Endpoints**:
     - `comtrade_data_endpoint`: For final trade data.
     - `comtrade_tariffline_endpoint`: For tariff line data.
     - `comtrade_bulk_endpoint`: For bulk data.
     - `comtrade_suv_endpoint`: For Standard Unit Values (SUV) data.
     - `comtrade_ais_endpoint`: For experimental AIS (ship tracking) data.
     - `comtrade_metadata_endpoint`: For metadata and publication notes.
     - `comtrade_reference_endpoint`: For reference data like commodity codes and country references.
   - **Variables**:
     - Trade values (imports, exports) for different commodities (cmdCode) and countries (reporterCode, partnerCode).
     - Tariff line information by commodity and country.
     - Commodity classifications and reference data (HS codes).
     - Standard Unit Values (SUV) by period and commodity codes.
     - AIS data for maritime tracking (under development).
   - **Use**: This data helps determine current trade volumes, tariff lines, and commodity classifications, enriching the economic context for government and company agents.

**2. World Bank Data**
   - **Source**: World Bank Open Data API
   - **Variables**:
     - GDP (`NY.GDP.MKTP.CD`)
     - Trade as a percentage of GDP (`NE.TRD.GNFS.ZS`)
     - Additional economic indicators (like unemployment, inflation, etc. if needed)
   - **Use**: Economic indicators (such as GDP, trade percentages) used to initialize government agents' economic conditions and to monitor changes over time.

**3. USITC HTS Data**
   - **Source**: United States International Trade Commission (USITC) HTS API
   - **Variables**:
     - Tariff classifications and rates by commodity codes
   - **Use**: Detailed tariff rate data informs how government agent policies on tariffs could shift and affect industries.

**4. Global Trade Alert (GTA) Data**
   - **Source**: Global Trade Alert API
   - **Variables**:
     - Active trade measures and interventions globally.
     - Timelines of policy changes (like tariff increases, trade agreements).
   - **Use**: Provides additional context on current global trade measures and interventions, influencing scenario creation and government agent policies.

**5. International Monetary Fund (IMF) Data**
   - **Source**: IMF API
   - **Variables**:
     - Various macroeconomic indicators (e.g., GDP growth rates, inflation rates, current account balances) not covered by the World Bank data.
   - **Use**: Additional macroeconomic data to ensure the simulation's economic context is robust and updated.

**6. World Trade Organization (WTO) Data**
   - **Source**: WTO data APIs or datasets
   - **Variables**:
     - Trade data and tariffs under various trade agreements
     - Information on international trade policies and conflicts
   - **Use**: Backup or complementary data to ensure completeness when other sources are missing or incomplete.

**7. Trading Economics Data**
   - **Source**: Trading Economics API
   - **Variables**:
     - Economic indicators (like interest rates, exchange rates, commodity prices) not covered elsewhere.
   - **Use**: Enhances economic context (e.g., the cost of commodities, interest rates affecting company investments, and consumer spending).

**8. US Census Data**
   - **Source**: U.S. Census Bureau International Trade API
   - **Variables**:
     - Detailed U.S. export and import data by commodity codes and country
   - **Use**: Provides detailed trade flow data, essential for accurate trade volume calculations and scenario generation.

**9. Consolidated Screening List Data**
   - **Source**: The Consolidated Screening List (CSL) from the International Trade Administration
   - **Variables**:
     - Entities and individuals restricted or sanctioned in international trade
   - **Use**: Helps model the effects of sanctions and restricted parties on trade decisions of government agents and companies.

**10. Political Data**
   - **Source**: Data fetched from official government sources, Wikipedia's API, or specialized political data APIs.
   - **Variables**:
     - Lists of political parties and their ideologies
     - Upcoming elections data and timetables
     - Current government policies
   - **Use**: Helps determine the political scenario for government agents, influence upcoming elections on policy changes, and incorporate political party ideologies into decision-making.

**11. News Data**
   - **Source**: NewsAPI or other news aggregator APIs
   - **Variables**:
     - Headlines and summaries of articles related to trade policies and economic conditions
     - Public sentiment derived from news article sentiment analysis
   - **Use**: News data helps gauge current public sentiment and ongoing events that can influence government policy decisions and scenario building.

**12. Social Media Data**
   - **Source**: Twitter API, potential other social media APIs
   - **Variables**:
     - Public sentiment derived from social media posts
     - Key topics and public opinions on trade, economy, and government policies
   - **Use**: Social media sentiment helps assess real-time public reaction to policies, influencing government agent actions and scenario branches.

**13. Legislative Data**
   - **Source**: Official government legislative tracking, third-party legislative data providers
   - **Variables**:
     - Current trade and economic legislation, bill names, statuses, introduction dates
   - **Use**: Current legislative environment informs how future policies might evolve and how government agents act.

**14. Developer Trade Gov Data**
   - **Source**: Data from the `api.trade.gov` for trade data and analytics
   - **Variables**:
     - Data on trade barriers, market insights, compliance, etc.
   - **Use**: Provides extra context on trade restrictions and compliance issues impacting trade flows and government decisions.

### 3.2 Data Processing and Integration

A Data Acquisition module systematically fetches data from these sources, handles rate limits, concurrency, and data validation. The Data Processing module subsequently cleans, normalizes, and stores this data in a structured format suitable for the simulation. Key steps include:

- **Data Cleaning**: Removing duplicates, handling missing values, and ensuring data types are consistent.
- **Data Validation**: Checking data ranges and consistency across sources.
- **Integration**: Merging data from various sources into unified structures (e.g., a combined dataframe for economic indicators per country).
- **Normalization**: Transforming different metrics and units into a unified format (e.g., converting currency units or normalizing sentiment scales).

This processed data is then ready to initialize agents in the simulation, as well as to update the scenario and decision logic as the simulation progresses.

## 4. Core Innovation: LLM-Based Agent Behavior and Scenario-Driven Decision Logic

### 4.1 Government Agent Dispositions and Decision Logic

Each Government Agent in the simulation is assigned dispositions and decision-making rules that determine their policy actions. These rules are captured in two key matrices:

1. **Disposition Matrix (Scenario-Action Probability Table)**:
   - **Description**: This matrix outlines how likely a government is to perform a particular action under different scenarios.
   - **Columns**:
     - *Decision_ID*: Unique identifier for each decision.
     - *Action*: The policy action to be considered (e.g., "Increase Tariffs", "Form Trade Alliances").
     - *Scenario*: The scenario in which this action might be taken (e.g., "Recession", "Upcoming Elections").
     - *Probability(%)*: The probability (in percentage) that the government will take this action if the given scenario applies.
   - **Example**:
     ```
     ### Disposition Matrix
     Decision_ID,Action,Scenario,Probability(%)
     1,Increase Tariffs,Recession,70
     2,Form Trade Alliances,Economic Boom,50
     3,Finalize Trade Agreements,Trade Deficit Increase,60
     4,Decrease Tariffs,Economic Boom,30
     5,Implement Subsidies,Recession,40
     6,Implement Climate Change Policies,Climate Change Policies,80
     7,Launch Election Campaign,Upcoming Elections,75
     8,Handle Political Scandal,Political Scandals,65
     9,Address Public Protests,Public Protests,55
     10,Enhance Disaster Response,Natural Disasters,60
     ```

2. **Modifier Matrix (Relation Modifier Table)**:
   - **Description**: This matrix adjusts the probabilities of actions based on relationships, alliances, and other contextual factors.
   - **Columns**:
     - *Relation_Type*: The type of relation (e.g., "Alliance", "Industry").
     - *Relation_Name*: The name of the related entity or industry.
     - *Scenario*: The scenario to which the relation applies.
     - *Modifier_Value*: The numeric modifier (positive or negative) to apply to the base probability. For example, `0.10` represents a 10% increase and `-0.05` represents a 5% decrease.
   - **Example**:
     ```
     ### Modifier Matrix
     Relation_Type,Relation_Name,Scenario,Modifier_Value
     Alliance,EU,Recession,0.05
     Alliance,CAN,Economic Boom,0.10
     Industry,Automotive,Recession,0.15
     Industry,Technology,Climate Change Policies,-0.10
     ```

### 4.2 GPT-4 Integration for Disposition Matrices

GPT-4 is used to generate these scenario-action probability and modifier tables. Specifically:

- **Context Extraction**: For each government agent (country and political party), a detailed context JSON string is created. This context includes:
  - Economic indicators (like GDP and trade percentages) sourced from the World Bank and other economic data providers.
  - Political climate information (party ideologies, upcoming elections, current policies).
  - Public sentiment derived from news and social media data.
  - Legislative context and trade alliance information.
- **GPT-4 Prompting**: Using this context, GPT-4 is given a predefined template prompt to produce:
  1. A **Disposition Matrix** with scenario-action probabilities, considering the party's ideology, current policies, and economic/political context.
  2. A **Modifier Matrix** indicating how specific relationships (alliances, industries) influence the probability of actions under various scenarios.

**Example Prompt Template**:
```
You are an expert political strategist tasked with generating two matrices for a political party based on the provided context. The matrices should adhere strictly to the defined lists below.

### Comprehensive Lists

**1. Scenarios**

- **Economic Scenarios:**
  - Recession
  - Economic Boom
  - Inflation Surge
  - Trade Deficit Increase

- **Political Scenarios:**
  - Upcoming Elections
  - Political Scandals
  - Leadership Changes

- **Social Scenarios:**
  - Public Protests
  - Shifts in Public Sentiment

- **Environmental Scenarios:**
  - Natural Disasters
  - Climate Change Policies

**2. Actions**

- **Economic Actions:**
  - Increase Tariffs
  - Decrease Tariffs
  - Implement Subsidies
  - Reduce Subsidies
  - Increase Taxes
  - Decrease Taxes
  - Stimulate Economic Growth
  - Implement Austerity Measures

- **Political Actions:**
  - Launch Election Campaign
  - Initiate Policy Reforms
  - Handle Political Scandals
  - Resign Leadership
  - Appoint New Leaders

- **Social Actions:**
  - Address Public Protests
  - Launch Public Awareness Campaigns
  - Implement Social Welfare Programs

- **Environmental Actions:**
  - Implement Climate Change Policies
  - Enhance Disaster Response Mechanisms
  - Promote Renewable Energy Initiatives

- **Trade Actions:**
  - Form Trade Alliances
  - Finalize Trade Agreements
  - Impose Trade Sanctions
  - Lift Trade Sanctions

- **Defense and Security Actions:**
  - Increase Defense Spending
  - Decrease Defense Spending
  - Strengthen Security Measures

**3. Parties/Countries/Industries/Companies**

- **Countries (ISO Alpha-3 Codes):**
  - USA (United States of America)
  - CAN (Canada)
  - CHN (China)
  - DEU (Germany)
  - FRA (France)
  - JPN (Japan)
  - GBR (United Kingdom)
  - ITA (Italy)
  - RUS (Russia)
  - IND (India)
  - BRA (Brazil)
  - AUS (Australia)
  - KOR (South Korea)
  - ESP (Spain)
  - MEX (Mexico)

- **Political Parties:**
  - **USA:**
    - Democratic Party (Liberal)
    - Republican Party (Conservative)
  - **CAN:**
    - Liberal Party (Liberal)
    - Conservative Party (Conservative)
  - **GER:**
    - Christian Democratic Union (Conservative)
    - Social Democratic Party (Liberal)

- **Industries:**
  - Automotive
  - Technology
  - Energy
  - Healthcare
  - Manufacturing
  - Agriculture
  - Finance
  - Telecommunications
  - Pharmaceuticals
  - Construction

- **Companies:**
  - **Automotive:**
    - General Motors (USA)
    - Toyota (JPN)
    - Volkswagen (DEU)
  - **Technology:**
    - Apple (USA)
    - Samsung (KOR)
    - Huawei (CHN)
  - **Energy:**
    - ExxonMobil (USA)
    - Shell (NLD)
    - BP (GBR)
  - **Healthcare:**
    - Pfizer (USA)
    - Roche (CHE)
    - Johnson & Johnson (USA)

### Context

{context_json}

### Scenarios

- {"\n- ".join(scenarios)}

### Instructions

Based on the above context and scenarios, generate two CSV-formatted tables strictly using the terms defined in the Comprehensive Lists.

1. **Disposition Matrix** with columns: Decision_ID, Action, Scenario, Probability(%).
2. **Modifier Matrix** with columns: Relation_Type, Relation_Name, Scenario, Modifier_Value.

**Ensure that:**

- Each decision is unique and reflects the party's ideology, current policies, economic indicators, public sentiment, existing trade alliances, and relationships with partners and industries.
- Modifier values are normalized (e.g., 0.1 for +10%, -0.05 for -5%) and reflect how specific relationships influence the probability of actions under each scenario.
- **Only** use the scenarios, actions, parties, countries, industries, and companies defined in the Comprehensive Lists.

### Output
```
### Disposition Matrix
Decision_ID,Action,Scenario,Probability(%)
1,Increase Tariffs,Recession,70
2,Form Trade Alliances,Economic Boom,50
3,Finalize Trade Agreements,Trade Deficit Increase,60
4,Decrease Tariffs,Economic Boom,30
5,Implement Subsidies,Recession,40
6,Implement Climate Change Policies,Climate Change Policies,80
7,Launch Election Campaign,Upcoming Elections,75
8,Handle Political Scandal,Political Scandals,65
9,Address Public Protests,Public Protests,55
10,Enhance Disaster Response,Natural Disasters,60

### Modifier Matrix
Relation_Type,Relation_Name,Scenario,Modifier_Value
Alliance,EU,Recession,0.05
Alliance,CAN,Economic Boom,0.10
Industry,Automotive,Recession,0.15
Industry,Technology,Climate Change Policies,-0.10
```
```

### 4.3 Example of GPT-4 Output

Given the prompt and a context for the USA's Democratic Party, GPT-4 produces a disposition matrix and a modifier matrix. An example output (abbreviated) might look like:

```
### Disposition Matrix
Decision_ID,Action,Scenario,Probability(%)
1,Increase Tariffs,Recession,70
2,Decrease Tariffs,Economic Boom,30
3,Implement Subsidies,Recession,50
4,Form Trade Alliances,Upcoming Elections,60
5,Implement Climate Change Policies,Climate Change Policies,80
...
```

and

```
### Modifier Matrix
Relation_Type,Relation_Name,Scenario,Modifier_Value
Alliance,CAN,Economic Boom,0.10
Industry,Technology,Climate Change Policies,-0.10
...
```

### 4.4 Application of Disposition and Modifier Matrices

1. **Scenario and Probability Calculation**:
   - The simulation identifies current active scenarios based on economic and political data.
   - For each government agent, the relevant subset of their disposition matrix is selected.
   - The base probability of each action is derived for each scenario.

2. **Relation Modifiers**:
   - The modifier matrix is applied to adjust these probabilities based on alliances (like with Canada), industries impacted by policies (like automotive or technology), and other relationships.
   - For example, if an agent has a strong alliance with Canada and an "Economic Boom" scenario, a `0.10` (10% increase) modifier might increase the base probability of certain actions like forming trade alliances.

3. **Action Decision**:
   - The final adjusted probabilities are used to determine which actions the agent will implement stochastically. For instance, if an "Increase Tariffs" action has a 70% probability under "Recession" scenario and alliance modifiers add 5%, the final probability might be 75%.

### 4.5 Example: Generating and Using the Tables

For the Democratic Party of the USA (given the context of a mild recession, upcoming elections, and alliances):
- **GPT-4 Output**:
  - A disposition matrix listing probabilities for various actions under relevant scenarios.
  - A modifier matrix showing how alliances and industries adjust these probabilities.
- **Simulation Step**:
  - If the scenario is "Recession" and the "Increase Tariffs" action has a 70% base probability:
    - If the alliance with the EU in a recession scenario adds a 5% modifier, the final probability is `70% + 5% = 75%`.
  - The government agent may then decide to "Increase Tariffs" with a 75% chance in that step.

## 5. Agent-Based Modeling with Data-Driven and Scenario-Driven Logic

### 5.1 Agent Types and Behaviors

1. **Government Agents**:
   - Represent countries and their policies.
   - Have disposal and modifier matrices that guide actions under different scenarios.
   - Adjust policies like tariffs, subsidies, trade alliances, and spending based on economic indicators, political pressures, and public sentiment.

2. **Company Agents**:
   - Represent businesses in different sectors (e.g., manufacturing, technology).
   - Decisions influenced by government policies (like tariffs), production capacity, financial health, and market demand.
   - They adjust sourcing (domestic vs. imported) and production according to changing trade policies and economic conditions.
   - Might invest in industries to increase competitiveness or adapt to new policies.

3. **Consumer Agents**:
   - Represent individual or aggregate consumer behavior.
   - Adjust consumption preferences based on income level, tariffs on imported goods, and other economic conditions.
   - For instance, if tariffs on imported luxury goods rise, consumers might shift towards domestic alternatives.

4. **Intermediary Agents**:
   - Represent entities like logistics providers and financial institutions that facilitate trade.
   - Their services and costs can influence how companies and governments operate within global supply chains.

### 5.2 Detailed Agent Logic

**GovernmentAgentEnhanced**:
- **Economic_indicators**: Values like GDP, trade deficit, GDP growth used in scenario determination.
- **Political_data**: Includes party ideology, upcoming election dates, public sentiment, and how these may influence decision probabilities.
- **Disposition_matrix & modifier_matrix**: Determine action probabilities under given scenarios, adjusting for relationships like alliances or industry support.

**CompanyAgent**:
- **Production_capacity & financial_health**: Track health and capabilities. If demand is high, a company increases production. If demand falls, production might decrease.
- **Supply_chain**: Decides whether to source domestically or internationally, factoring in tariffs and trade policies.
- **Invest_in_industry**: At random intervals, invests in improving financial health or production capacity.

**ConsumerAgent**:
- **Income_level**: Affects the ratio of spending on essential vs. luxury goods.
- **Demand_preferences**: Adjusted by policy changes (like tariffs on imports) and economic conditions. For instance, higher tariffs on imports might lead to increased demand for domestic goods.

### 5.3 The Simulation Model Architecture

**TradeModelEnhanced**:
- Brings all agents together in a simulation environment.
- Gathers processed data for each country, initializes agents with relevant data.
- Each step:
  - Government agents decide policies.
  - Company agents adjust production, sourcing, and investments.
  - Consumer agents adjust consumption preferences.
- Each batch of steps:
  - The model collects data (like trade volume).
  - Identifies scenarios (like average or outlier outcomes).
- Introduces new events (like future news or policies) to simulate evolving conditions.

### 5.4 Example Agent Interactions

A simplified example:
- A GovernmentAgent faces a "Recession" scenario.
  - The disposition matrix gives a 70% base probability for "Increase Tariffs".
  - The modifier matrix (like alliance with EU giving a 5% increase) raises it to 75%.
  - The government decides to increase tariffs.
- CompanyAgent sees an increase in tariffs on imports:
  - Company might shift sourcing from imported to domestic resources to avoid higher import costs.
- ConsumerAgent experiences higher prices on imported luxury goods:
  - If tariffs significantly raise these prices, consumers shift consumption, influencing future production decisions of CompanyAgents.

## 6. Simulation Process and Scenario Analysis

### 6.1 The Step-by-Step Simulation

1. **Initialization**:
   - Agents are created with data from sources (economic indicators, political climates).
   - Government agent's disposition and modifier matrices are generated by GPT-4 using the initial data context.
   - Company and consumer agents set up with initial parameters (production capacity, demand preferences).

2. **Single Simulation Step**:
   - Government agents:
     - Assess current scenario.
     - Determine actions (like adjusting tariffs) using their disposition and modifier matrices.
     - Implement chosen actions.
   - Company agents:
     - Observe changes in policies (like tariff changes).
     - Adjust sourcing and production capacity accordingly.
     - Possibly invest in capacity to improve financial health.
   - Consumer agents:
     - Adjust consumption preferences based on new government policies.
   - Intermediary agents:
     - Provide services (like adjusting shipping rates or financial products) based on the scenario.

3. **Data Collection**:
   - After each step, the model collects metrics such as trade volume, average tariff rates, and others for analysis and scenario determination.

4. **Batch Simulation**:
   - The model runs for several steps (e.g., 50 steps to simulate 6 months).
   - At the end of the batch, the model analyzes outcomes:
     - **Mean and Median outcomes**: The average/median scenario.
     - **1 Sigma Scenarios**: Scenarios around one standard deviation from the mean.
     - **High-Impact Outliers**: Scenarios beyond two standard deviations.

5. **Scenario Branching**:
   - Based on identified outcomes, the model creates branches for plausible future scenarios:
     - For the mean scenario, 1-sigma scenarios, and outlier scenarios.
   - For each scenario branch:
     - The model uses GPT-4 to generate updated disposition and modifier matrices given the new scenario context.
     - Agents continue the simulation for the next batch of steps.

### 6.2 Detailed Scenario Branching Logic

**Example**:
- After the first 6-month simulation:
  - The average trade volume scenario (`Mean Outcome`) is \$1 trillion.
  - A 1-sigma scenario is \$1.1 trillion.
  - Another 1-sigma scenario is \$0.9 trillion.
  - An outlier scenario might be \$1.2 trillion.

**Branching**:
- The simulation creates branches:
  1. **Mean scenario** (Trade volume \$1 trillion)
  2. **1 Sigma Above** scenario (Trade volume \$1.1 trillion)
  3. **1 Sigma Below** scenario (Trade volume \$0.9 trillion)
  4. **High-Impact Outlier** scenario (Trade volume \$1.2 trillion)

For each branch:
- GPT-4 is provided an updated context (like which scenario is unfolding) and asked to generate new disposition and modifier matrices.
- The model reinitializes with these updated matrices and runs another batch of simulation steps.

### 6.3 Iteration and Depth of Simulation

The simulation is designed to run multiple batches to cover a certain horizon (e.g., 6 months per batch for a total horizon of 2.5 years or 5 batches):
- **Total Batches**: 5.
- **Steps per Batch**: Each batch may run for 50 steps, simulating daily or weekly increments within a 6-month period.
- **Branching Depth**: Up to a maximum depth of 3 scenario branches to avoid combinatorial explosion.

At each branching step, the simulation explores different plausible futures, updating conditions and decisions. This branching allows the model to explore a variety of future states and their likelihood.

### 6.4 Integration of Future Events using GPT-4

At the end of each batch:
- The simulation identifies key outcomes and possible future events (like new trade deals, changes in public sentiment, or political shifts).
- GPT-4 generates plausible future news articles and social media posts reflecting these outcomes and sentiments.
- These future events are scheduled into the model to update agent decision-making in subsequent steps, reflecting dynamic changes in the environment over time.

### 6.5 Example: Full Prompt to GPT-4 with Example Output

**Initial Prompt**:
```
You are an expert political strategist tasked with generating two matrices for a political party based on the provided context...
...
```

**Initial Context**:
```
{
  'Country': 'USA',
  'Party': 'Democratic Party',
  'Ideology': 'Liberal',
  'Recent_News_Sentiments': {'Positive': 10, 'Negative': 5},
  'Recent_Social_Media_Sentiments': {'Positive': 8, 'Negative': 7},
  'Current_Policies': [
    {'Policy': 'Tariff Rate Adjustment', 'Status': 'Increasing'},
    {'Policy': 'Climate Change Initiative', 'Status': 'Active'}
  ],
  'Legislation': [
    {'Bill_Name': 'Trade Cooperation Act', 'Status': 'active', 'Date_Introduced': '2023-03-22'},
    {'Bill_Name': 'Green Energy Promotion Act', 'Status': 'active', 'Date_Introduced': '2022-07-15'}
  ],
  'Upcoming_Elections': [
    {'Date_Upcoming_Election': '2024-11-05'}
  ]
}
```

**GPT-4 Output**:
```
### Disposition Matrix
Decision_ID,Action,Scenario,Probability(%)
1,Implement Climate Change Policies,Climate Change Policies,80
2,Increase Tariffs,Recession,70
3,Form Trade Alliances,Economic Boom,50
4,Initiate Policy Reforms,Upcoming Elections,65
5,Address Public Protests,Public Protests,55

### Modifier Matrix
Relation_Type,Relation_Name,Scenario,Modifier_Value
Alliance,CAN,Economic Boom,0.10
Industry,Technology,Climate Change Policies,-0.05
```

## 7. Conclusion and Future Work

This agent-based modeling system, enhanced with GPT-4 integration for generating scenario-driven decision matrices, represents a significant advancement in simulating and predicting the outcomes of complex global trade scenarios. By leveraging detailed data inputs (from economic to political data) and exploring various plausible futures through scenario branching, this tool provides valuable insights for policymakers, businesses, and researchers alike.

**Key Contributions**:
- A comprehensive system that uses real-world data, expert political strategies simulated through GPT-4, and ABM to explore future trade scenarios.
- Iterative scenario branching to capture a wide range of potential futures and measure their economic and political implications over a 6-month horizon.

**Future Enhancements** may include:
- Incorporation of more diverse and real-time data sources, such as global commodity prices or real-time news sentiment.
- Refinement of scenario selection and branching based on more sophisticated statistical methods and domain expert feedback.
- Expanding the horizon beyond 6 months with additional policy decisions and global events.
- Increasing the fidelity of agent behaviors, especially for companies and consumers, using more nuanced economic modeling and microeconomic data.
- Enhanced detail on how legislative changes, trade alliances, and global political shifts affect government policies and outcomes.

By continually refining the model, incorporating feedback from domain experts, and updating data inputs, we aim to maintain the simulation's accuracy, relevance, and utility in shaping informed policy and strategic business decisions in an ever-evolving global trade landscape.
"""

# trade_wars_simulation.py

# Import necessary libraries
import os
import requests
import pandas as pd
import numpy as np
import wbgapi as wb
import time
import logging
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from mesa import Agent, Model
from mesa.time import RandomActivation
from mesa.datacollection import DataCollector
import matplotlib.pyplot as plt
import networkx as nx
import plotly.graph_objs as go
import plotly.express as px
from flask import Flask, request, jsonify
import threading
import unittest
import pycountry
import random
from scipy import stats
from dotenv import load_dotenv
import requests_cache
from typing import Optional, Dict, Any, List, Union, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask_cors import CORS
import zipfile
import spacy
from transformers import pipeline, Pipeline
from apscheduler.schedulers.background import BackgroundScheduler
import json
import io
import openai
import sys
import re
from unittest.mock import patch, MagicMock
from datetime import datetime, timedelta

# =================================
# 1. Setup and Configuration
# =================================

# Ensure OpenAI API key is set
def load_openai_api_key(env_file='.env'):
    """
    Loads the OpenAI API key from the specified .env file.
    """
    load_dotenv(env_file)
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logging.error("OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.")
        sys.exit(1)
    openai.api_key = api_key

def setup_logging(log_file='simulation.log', log_level=logging.INFO):
    """
    Sets up the logging configuration.

    Args:
        log_file (str): The file to which logs will be written.
        log_level (int): The logging level.
    """
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s %(levelname)s:%(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )

# Define custom exceptions for clarity in error handling
class RateLimitError(Exception):
    """Exception raised when hitting rate limit of API."""
    pass

# Ensure OpenAI API key is set
openai.api_key = os.getenv('OPENAI_API_KEY')
if not openai.api_key:
    raise ValueError("OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.")

# =================================
# 1. Data Acquisition Module
# =================================

class DataAcquisition:
    """
    Fetches data from various external sources, prioritizing primary providers and using backups as needed.
    Enhanced to handle UN Comtrade API functionalities, including advanced parameterization, error handling, data validation,
    concurrency for improved performance, and integration with additional trade-related APIs and political data.
    """
    def __init__(self, proxy_url: Optional[str] = None):
        # Initialize API keys from environment variables or configuration
        self.gta_api_key = os.getenv('GTA_API_KEY')
        self.usitc_api_key = os.getenv('USITC_API_KEY')
        self.imf_api_key = os.getenv('IMF_API_KEY')
        self.wto_api_key = os.getenv('WTO_API_KEY')
        self.trading_economics_api_key = os.getenv('TRADING_ECONOMICS_API_KEY')
        self.census_api_key = os.getenv('CENSUS_API_KEY')
        self.comtrade_api_key = os.getenv('COMTRADE_API_KEY')
        self.consolidated_screening_list_api_key = os.getenv('CONSOLIDATED_SCREENING_LIST_API_KEY')
        self.newsapi_key = os.getenv('NEWSAPI_KEY')
        self.twitter_api_key = os.getenv('TWITTER_API_KEY')
        self.twitter_api_secret = os.getenv('TWITTER_API_SECRET')
        self.twitter_access_token = os.getenv('TWITTER_ACCESS_TOKEN')
        self.twitter_access_secret = os.getenv('TWITTER_ACCESS_SECRET')

        # Proxy configuration if needed
        self.proxy_url = proxy_url

        # Setup session with proxies if provided
        self.session = requests.Session()
        if proxy_url:
            self.session.proxies.update({
                'http': proxy_url,
                'https': proxy_url
            })

        # API endpoints
        self.comtrade_data_endpoint = 'https://comtradeapi.un.org/data/v1/get'
        self.comtrade_tariffline_endpoint = 'https://comtradeapi.un.org/data/v1/tariffline'
        self.comtrade_bulk_endpoint = 'https://comtradeapi.un.org/bulk/v1/get'
        self.comtrade_async_endpoint = 'https://comtradeapi.un.org/async/v1/get'
        self.comtrade_suv_endpoint = 'https://comtradeapi.un.org/data/v1/suv'  # Corrected endpoint
        self.comtrade_ais_endpoint = 'https://comtradeapi.un.org/experimental/v1/getAIS'
        self.comtrade_metadata_endpoint = 'https://comtradeapi.un.org/public/v1/getMetadata'
        self.comtrade_reference_endpoint = 'https://comtradeapi.un.org/public/v1/getReference'
        self.comtrade_liveupdate_endpoint = 'https://comtradeapi.un.org/data/v1/getLiveUpdate'
        self.gta_endpoint = 'https://api.globaltradealert.org/api/v1/data/'
        self.usitc_hts_endpoint = 'https://api.usitc.gov/v1/hts'
        self.imf_endpoint = 'https://api.imf.org/v1/data'
        self.wto_endpoint = 'https://data.wto.org/api/'
        self.trading_economics_endpoint = 'https://api.tradingeconomics.com/'
        self.developer_trade_gov_endpoint = 'https://api.trade.gov/'
        self.census_export_endpoint = 'https://api.census.gov/data/timeseries/intltrade/exports/hs'
        self.census_import_endpoint = 'https://api.census.gov/data/timeseries/intltrade/imports/hs'
        self.consolidated_screening_list_endpoint = 'https://api.wto.org/tfad/transparency/procedures_contacts_single_window'

        # Headers for some APIs
        self.gta_headers = {
            'Content-Type': 'application/json',
            'Authorization': f'APIKey {self.gta_api_key}'
        }
        self.usitc_headers = {
            'Authorization': f'Bearer {self.usitc_api_key}'
        }
        self.imf_headers = {
            'Authorization': f'Bearer {self.imf_api_key}'
        }
        self.wto_headers = {
            'Authorization': f'Bearer {self.wto_api_key}'
        }
        self.trading_economics_headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.trading_economics_api_key}'
        }
        self.developer_trade_gov_headers = {
            'Authorization': f'Bearer {os.getenv("DEVELOPER_TRADE_GOV_API_KEY")}'
        }
        self.census_headers = {
            'Content-Type': 'application/json'
        }
        self.consolidated_screening_list_headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.consolidated_screening_list_api_key}'
        }

        # Additional configuration parameters
        self.default_max_records = 5000  # Default max records for partial data retrieval
        self.default_wait_time = 1.5  # Wait time between requests to avoid rate limit issues

        # Set up a session with retries for robust requests to handle network or rate limit issues
        retries = requests.adapters.Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        adapter = requests.adapters.HTTPAdapter(max_retries=retries)
        self.session.mount('https://', adapter)
        self.session.mount('http://', adapter)

        # Set up caching
        requests_cache.install_cache('trade_cache', backend='sqlite', expire_after=86400)  # Cache expires after 1 day

        # Initialize placeholders for data retrieval tasks
        self.fetched_data = {}

        # Define primary and backup data sources
        self.primary_data_sources = {
            'world_bank': self.fetch_world_bank_data,
            'un_comtrade': self.fetch_un_comtrade_data,
            'usitc_hts': self.fetch_usitc_hts_data,
            'gta': self.fetch_gta_data,
            'imf': self.fetch_imf_data,
            'wto_api': self.fetch_wto_data,
            'trading_economics': self.fetch_trading_economics_data,
            'census_exports': self.fetch_census_export_data,
            'census_imports': self.fetch_census_import_data,
            'consolidated_screening_list': self.fetch_consolidated_screening_list_data,
            'developer_trade_gov': self.fetch_developer_trade_gov_data,
            # Political and media data
            'political_data': self.fetch_all_political_data,
            'news_data': self.fetch_all_news_data,
            'social_media_data': self.fetch_all_social_media_data,
            'legislation_data': self.fetch_all_legislation_data
        }

        self.backup_data_sources = {
            'world_bank': self.fetch_world_bank_data_backup,
            'un_comtrade': self.fetch_un_comtrade_data_backup,
            # Define backups for other sources similarly
        }

    def schedule_data_refresh(self, interval_minutes: int = 60):
        """
        Schedules periodic data refreshes to keep simulation data up-to-date.
        """
        scheduler = BackgroundScheduler()
        scheduler.add_job(self.fetch_all_data, 'interval', minutes=interval_minutes)
        scheduler.start()
        logging.info(f'Data refresh scheduled every {interval_minutes} minutes.')

    # Placeholder backup methods
    def fetch_world_bank_data_backup(self):
        logging.info('Fetching World Bank data from backup source...')
        return self.fetch_world_bank_data()

    def fetch_un_comtrade_data_backup(self):
        logging.info('Fetching UN Comtrade data from backup source...')
        return self.fetch_un_comtrade_data()

    def _handle_response_errors(self, response: requests.Response):
        """
        Handles any errors in the response from external APIs and raises appropriate exceptions.
        """
        if response.status_code == 400:
            raise ValueError("Bad Request: The server could not understand the request.")
        elif response.status_code == 401:
            raise PermissionError("Unauthorized: Invalid or missing API key.")
        elif response.status_code == 403:
            raise PermissionError("Forbidden: Access to the requested resource is denied.")
        elif response.status_code == 404:
            raise LookupError("Not Found: The requested resource could not be found.")
        elif response.status_code == 429:
            raise RateLimitError("Too Many Requests: Rate limit exceeded.")
        elif response.status_code >= 500:
            raise ConnectionError(f"Server Error: {response.status_code} - {response.reason}")
        # Raise for any other unsuccessful status
        response.raise_for_status()

    def _fetch_data_concurrently(self, tasks: List[Dict[str, Any]]) -> pd.DataFrame:
        """
        Executes multiple API requests concurrently using ThreadPoolExecutor for performance optimization.
        This method is used for fetching large datasets in parallel where tasks are splitted for each period or parameter subset.
        """
        data_frames = []
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            futures = {executor.submit(self._fetch_data_task, **task_params): task_params for task_params in tasks}
            for future in as_completed(futures):
                task_params = futures[future]
                try:
                    result_df = future.result()
                    if result_df is not None and not result_df.empty:
                        data_frames.append(result_df)
                except Exception as e:
                    logging.error(f"Error fetching data for task {task_params}: {e}")
        if data_frames:
            return pd.concat(data_frames, ignore_index=True)
        return pd.DataFrame()

    def _fetch_data_task(self, endpoint: str, params: Dict[str, Any], method: str = 'GET', headers: Dict[str, str] = None, body: Dict[str, Any] = None, max_records: int = None) -> Optional[pd.DataFrame]:
        """
        Fetches data for a single task by making an API request to the specified endpoint.
        """
        if not headers:
            headers = {}
        if self.comtrade_api_key:
            headers.update({"Authorization": f"Bearer {self.comtrade_api_key}"})

        # If max_records is not specified, use default
        if max_records is None:
            max_records = self.default_max_records

        # Rate limiting attempt
        time.sleep(self.default_wait_time)  # wait to respect rate limits
        if method.upper() == 'GET':
            response = self.session.get(endpoint, params=params, headers=headers)
        else:
            response = self.session.post(endpoint, json=body, headers=headers, params=params)
        self._handle_response_errors(response)

        try:
            data = response.json()
            if not data or 'data' not in data:
                logging.warning(f"No data found for parameters: {params}")
                return None
            df = pd.json_normalize(data['data'])
            return df
        except Exception as e:
            logging.error(f"Error processing response data: {e}")
            return None

    def fetch_comtrade_data(self,
                            type_code: str = 'C',
                            freq_code: str = 'A',
                            cl_code: str = 'HS',
                            reporter_code: Optional[str] = None,
                            period: Optional[str] = None,
                            partner_code: Optional[str] = None,
                            partner2_code: Optional[str] = None,
                            cmd_code: Optional[str] = None,
                            flow_code: Optional[str] = None,
                            customs_code: Optional[str] = None,
                            mot_code: Optional[str] = None,
                            max_records: Optional[int] = None,
                            aggregate_by: Optional[str] = None,
                            breakdown_mode: Optional[str] = None,
                            include_desc: bool = True) -> Optional[pd.DataFrame]:
        """
        Fetches final trade data from the UN Comtrade API using the standard data endpoint.
        This method supports advanced parameterization for thorough data retrieval.
        """
        logging.info('Fetching data from UN Comtrade API final data endpoint...')
        endpoint = f'{self.comtrade_data_endpoint}/{type_code}/{freq_code}/{cl_code}'

        # Build parameter dictionary for the API call
        params = {}
        if reporter_code:
            params['reporterCode'] = reporter_code
        if period:
            params['period'] = period
        if partner_code:
            params['partnerCode'] = partner_code
        if partner2_code:
            params['partner2Code'] = partner2_code
        if cmd_code:
            params['cmdCode'] = cmd_code
        if flow_code:
            params['flowCode'] = flow_code
        if customs_code:
            params['customsCode'] = customs_code
        if mot_code:
            params['motCode'] = mot_code
        if aggregate_by:
            params['aggregateBy'] = aggregate_by
        if breakdown_mode:
            params['breakdownMode'] = breakdown_mode
        params['includeDesc'] = str(include_desc).lower()

        df = self._fetch_data_task(endpoint, params, method='GET', max_records=max_records)
        if df is not None:
            logging.info('UN Comtrade final data fetched successfully.')
        return df

    def fetch_comtrade_tariffline_data(self,
                                       type_code: str = 'C',
                                       freq_code: str = 'A',
                                       cl_code: str = 'HS',
                                       reporter_code: Optional[str] = None,
                                       period: Optional[str] = None,
                                       partner_code: Optional[str] = None,
                                       partner2_code: Optional[str] = None,
                                       cmd_code: Optional[str] = None,
                                       flow_code: Optional[str] = None,
                                       customs_code: Optional[str] = None,
                                       mot_code: Optional[str] = None,
                                       max_records: Optional[int] = None,
                                       include_desc: bool = True) -> Optional[pd.DataFrame]:
        """
        Fetches tariff line data from the UN Comtrade API's Tariffline data endpoint.
        """
        logging.info('Fetching tariff line data from UN Comtrade API...')
        endpoint = f'{self.comtrade_tariffline_endpoint}/{type_code}/{freq_code}/{cl_code}'

        # Build parameter dictionary similarly as above
        params = {}
        if reporter_code:
            params['reporterCode'] = reporter_code
        if period:
            params['period'] = period
        if partner_code:
            params['partnerCode'] = partner_code
        if partner2_code:
            params['partner2Code'] = partner2_code
        if cmd_code:
            params['cmdCode'] = cmd_code
        if flow_code:
            params['flowCode'] = flow_code
        if customs_code:
            params['customsCode'] = customs_code
        if mot_code:
            params['motCode'] = mot_code
        params['includeDesc'] = str(include_desc).lower()

        df = self._fetch_data_task(endpoint, params, method='GET', max_records=max_records)
        if df is not None:
            logging.info('UN Comtrade tariff line data fetched successfully.')
        return df

    def fetch_comtrade_bulk_data(self,
                                 type_code: str,
                                 freq_code: str,
                                 cl_code: str,
                                 reporter_code: Optional[str] = None,
                                 period: Optional[str] = None,
                                 decompress: bool = True,
                                 published_date_from: Optional[str] = None,
                                 published_date_to: Optional[str] = None,
                                 directory: Optional[str] = None) -> None:
        """
        Downloads bulk data files from the UN Comtrade API's bulk data endpoint.
        This method handles large datasets by downloading the data in compressed files.
        """
        logging.info('Fetching bulk data from UN Comtrade API...')
        if not self.comtrade_api_key:
            logging.error("A valid COMTRADE_API_KEY is required to download bulk data.")
            return
        endpoint = f'{self.comtrade_bulk_endpoint}/{type_code}/{freq_code}/{cl_code}'

        # Build parameters dictionary
        params = {}
        if reporter_code:
            params['reporterCode'] = reporter_code
        if period:
            params['period'] = period
        if published_date_from:
            params['publishedDateFrom'] = published_date_from
        if published_date_to:
            params['publishedDateTo'] = published_date_to

        # Obtain bulk data availability
        data_availability = self._fetch_data_task(endpoint, params, method='GET')
        if data_availability is None or data_availability.empty:
            logging.warning("No bulk data available for the specified parameters.")
            return

        # Download each file from the bulk data availability info
        if directory is None:
            directory = os.getcwd()
        if not os.path.exists(directory):
            os.makedirs(directory)
        for idx, row in data_availability.iterrows():
            file_url = row.get('downloadUrl')
            if not file_url:
                logging.warning(f"No file URL found for record {idx}.")
                continue
            logging.info(f"Downloading file from {file_url}...")
            response = self.session.get(file_url)
            self._handle_response_errors(response)
            content_disposition = response.headers.get('Content-Disposition')
            if content_disposition:
                filename = content_disposition.split('filename=')[1].strip('"')
            else:
                filename = f"bulk_data_{idx}.zip"
            file_path = os.path.join(directory, filename)
            with open(file_path, 'wb') as f:
                f.write(response.content)
            if decompress and filename.endswith('.zip'):
                # Decompress the file
                with zipfile.ZipFile(file_path, 'r') as zip_ref:
                    zip_ref.extractall(directory)
                os.remove(file_path)
            logging.info(f"File {filename} downloaded and processed.")

    def fetch_comtrade_metadata(self,
                                type_code: str,
                                freq_code: str,
                                cl_code: str,
                                period: str,
                                reporter_code: Optional[str] = None,
                                show_history: bool = False) -> Optional[pd.DataFrame]:
        """
        Fetches metadata and publication notes from the UN Comtrade API.
        """
        logging.info('Fetching metadata from UN Comtrade API...')
        endpoint = self.comtrade_metadata_endpoint

        params = {
            'typeCode': type_code,
            'freqCode': freq_code,
            'clCode': cl_code,
            'period': period,
            'showHistory': str(show_history).lower()
        }
        if reporter_code:
            params['reporterCode'] = reporter_code
        df = self._fetch_data_task(endpoint, params, method='GET')
        if df is not None:
            logging.info('Metadata fetched successfully.')
        return df

    def fetch_comtrade_suv(self,
                           period: str,
                           cmd_code: str,
                           flow_code: Optional[str] = None,
                           qty_unit_code: Optional[int] = None) -> Optional[pd.DataFrame]:
        """
        Fetches the Standard Unit Values (SUV) from the UN Comtrade API.
        """
        logging.info('Fetching Standard Unit Values (SUV) from UN Comtrade API...')
        endpoint = self.comtrade_suv_endpoint

        params = {
            'period': period,
            'cmdCode': cmd_code
        }
        if flow_code:
            params['flowCode'] = flow_code
        if qty_unit_code:
            params['qtyUnitCode'] = str(qty_unit_code)
        df = self._fetch_data_task(endpoint, params, method='GET')
        if df is not None:
            logging.info('SUV data fetched successfully.')
        return df

    def fetch_comtrade_ais(self,
                           country_area_code: str,
                           vessel_type_code: Optional[str] = None,
                           date_from: str = '',
                           date_to: str = '',
                           flow_code: Optional[str] = None) -> Optional[pd.DataFrame]:
        """
        Fetches experimental data from the AIS (ships tracking movement) from the UN Comtrade API.
        """
        logging.info('Fetching AIS data from UN Comtrade API...')
        endpoint = self.comtrade_ais_endpoint
        if not (date_from and date_to):
            raise ValueError("Both date_from and date_to are required for AIS data retrieval.")

        params = {
            'countryAreaCode': country_area_code,
            'dateFrom': date_from,
            'dateTo': date_to
        }
        if vessel_type_code:
            params['vesselTypeCode'] = vessel_type_code
        if flow_code:
            params['flowCode'] = flow_code
        df = self._fetch_data_task(endpoint, params, method='GET')
        if df is not None:
            logging.info('AIS data fetched successfully.')
        return df

    def fetch_commodity_reference(self, category: str) -> Optional[pd.DataFrame]:
        """
        Fetches reference data (commodity categories, country codes, etc.) from the UN Comtrade API.
        """
        logging.info(f'Fetching reference data for category {category} from UN Comtrade API...')
        endpoint = self.comtrade_reference_endpoint
        params = {
            'category': category
        }
        df = self._fetch_data_task(endpoint, params, method='GET')
        if df is not None:
            logging.info('Reference data fetched successfully.')
        return df

    def commodity_lookup(self, search_terms: List[str], return_code: bool = False, return_char: bool = True, update: bool = False) -> Union[List[str], Dict[str, List[str]]]:
        """
        Looks up commodities based on the given search terms. If `return_code` is True, returns a list or dictionary
        of commodity codes. If `return_char` is True, returns a list or dictionary of commodity descriptions.
        """
        logging.info(f'Looking up commodities for search terms: {search_terms}')
        # If needed, update the reference data (not implemented fully here)

        # We'll assume we have a reference data set for commodity classification code "HS"
        commodity_ref = self.fetch_commodity_reference('cmd:HS')
        if commodity_ref is None or commodity_ref.empty:
            logging.warning('No commodity reference data available to search.')
            return []

        results = {}
        for term in search_terms:
            term_lower = term.lower()
            matching_entries = commodity_ref[commodity_ref['text'].str.lower().str.contains(term_lower, na=False)]
            if return_code and 'id' in matching_entries.columns:
                codes = matching_entries['id'].astype(str).tolist()
                if return_char:
                    descriptions = matching_entries['text'].tolist()
                    combined = [f"{c} - {d}" for c, d in zip(codes, descriptions)]
                    results[term] = combined
                else:
                    results[term] = codes
            else:
                # If not returning codes, just return descriptions if available.
                results[term] = matching_entries['text'].tolist() if 'text' in matching_entries.columns else []

        empty_terms = [term for term, match_list in results.items() if not match_list]
        if empty_terms:
            logging.warning(f'No matching results found for input terms: {empty_terms}')
        if len(search_terms) == 1:
            # Return a list of matches if only one term was requested
            return results[search_terms[0]]
        return results

    # Enhanced parameterization for the Census API, with dynamic parameters
    def fetch_census_export_data_enhanced(self, **kwargs) -> pd.DataFrame:
        """
        Enhanced method for fetching data from the Census API using dynamic parameters.
        """
        logging.info('Fetching enhanced Census export data with dynamic parameters...')
        # We'll use fetch_census_export_data under the hood with passed kwargs.
        return self.fetch_census_export_data(**kwargs)

    def fetch_census_import_data_enhanced(self, **kwargs) -> pd.DataFrame:
        """
        Enhanced method for fetching data from the Census API using dynamic parameters.
        """
        logging.info('Fetching enhanced Census import data with dynamic parameters...')
        # We'll use fetch_census_import_data under the hood with passed kwargs.
        return self.fetch_census_import_data(**kwargs)

    # Additional concurrency and advanced features can be implemented as needed.

    # Methods to fetch data from the World Bank, USITC, GTA, IMF, WTO, Trading Economics, and others can be included as they are from earlier code or further enhanced as needed.

    def fetch_world_bank_data(self) -> Optional[pd.DataFrame]:
        """
        Fetches GDP and trade indicators from the World Bank API for all countries.
        """
        try:
            logging.info('Fetching data from World Bank API...')
            countries = [country.alpha_3 for country in pycountry.countries]
            indicators = ['NY.GDP.MKTP.CD', 'NE.TRD.GNFS.ZS']  # GDP, Trade (% of GDP)
            data_frames = []
            for indicator in indicators:
                df = wb.data.DataFrame(indicator, countries, mrv=10)
                df = df.reset_index()
                df['Indicator'] = indicator
                data_frames.append(df)
            combined_df = pd.concat(data_frames, ignore_index=True)
            logging.info('World Bank data fetched successfully.')
            return combined_df
        except Exception as e:
            logging.error(f'Error fetching World Bank data: {e}')
            return None

    def fetch_un_comtrade_data(self, reporter: str = 'all', partner: str = 'all', year: int = 2021, freq: str = 'A') -> Optional[pd.DataFrame]:
        """
        Fetches trade data from the UN Comtrade API.
        """
        try:
            logging.info('Fetching data from UN Comtrade API...')
            url = f'https://comtradeapi.un.org/public/v1/preview/{reporter}/{partner}/{year}/HS'
            params = {
                'reporterCode': reporter,
                'partnerCode': partner,
                'period': year,
                'classification': 'HS',
                'typeCode': 'C',
                'freq': freq
            }
            response = self.session.get(url, params=params)
            if response.status_code == 200:
                data = response.json()
                # Parse JSON to DataFrame
                trade_data = data.get('dataset', [])
                if not trade_data:
                    logging.warning('No trade data found in UN Comtrade response.')
                    return None
                df = pd.json_normalize(trade_data)
                logging.info('UN Comtrade data fetched successfully.')
                return df
            else:
                logging.error(f'UN Comtrade API Error: {response.status_code} - {response.text}')
                return None
        except Exception as e:
            logging.error(f'Error fetching UN Comtrade data: {e}')
            return None

    def fetch_usitc_hts_data(self) -> Optional[pd.DataFrame]:
        """
        Fetches tariff classifications and rates from USITC HTS API.
        """
        try:
            logging.info('Fetching data from USITC HTS API...')
            url = f'{self.usitc_hts_endpoint}/tariffs'
            params = {
                'api_key': self.usitc_api_key,
                # Add additional parameters as needed
            }
            response = self.session.get(url, headers=self.usitc_headers, params=params)
            if response.status_code == 200:
                data = response.json()
                # Assuming data is a list of tariff records
                df = pd.json_normalize(data)
                logging.info('USITC HTS data fetched successfully.')
                return df
            else:
                logging.error(f'USITC HTS API Error: {response.status_code} - {response.text}')
                return None
        except Exception as e:
            logging.error(f'Error fetching USITC HTS data: {e}')
            return None

    def fetch_gta_data(self, limit: int = 1000, offset: int = 0, sorting: str = '-date_announced', filters: Dict[str, Any] = {}) -> Optional[pd.DataFrame]:
        """
        Fetches active trade measures from Global Trade Alert (GTA) API.
        Handles pagination by using limit and offset.
        """
        try:
            logging.info('Fetching data from GTA API...')
            url = self.gta_endpoint
            payload = {
                'limit': limit,
                'offset': offset,
                'sorting': sorting,
                'request_data': filters
            }
            response = self.session.post(url, headers=self.gta_headers, json=payload)
            if response.status_code == 200:
                data = response.json()
                df = pd.json_normalize(data.get('data', []))
                logging.info('GTA data fetched successfully.')
                return df
            else:
                logging.error(f'GTA API Error: {response.status_code} - {response.text}')
                return None
        except Exception as e:
            logging.error(f'Error fetching GTA data: {e}')
            return None

    def fetch_imf_data(self, database_id: str, params: Dict[str, Any]) -> Optional[pd.DataFrame]:
        """
        Fetches data from the IMF API.
        """
        try:
            logging.info('Fetching data from IMF API...')
            url = f'{self.imf_endpoint}/{database_id}'
            response = self.session.get(url, headers=self.imf_headers, params=params)
            if response.status_code == 200:
                data = response.json()
                # Parse JSON to DataFrame
                df = pd.json_normalize(data.get('data', []))
                logging.info('IMF data fetched successfully.')
                return df
            else:
                logging.error(f'IMF API Error: {response.status_code} - {response.text}')
                return None
        except Exception as e:
            logging.error(f'Error fetching IMF data: {e}')
            return None

    def fetch_wto_data(self, endpoint: str, params: Dict[str, Any]) -> Optional[pd.DataFrame]:
        """
        Fetches trade data from WTO API as a backup.
        """
        try:
            logging.info('Fetching data from WTO API...')
            url = f'{self.wto_endpoint}/{endpoint}'
            response = self.session.get(url, headers=self.wto_headers, params=params)
            if response.status_code == 200:
                data = response.json()
                df = pd.json_normalize(data.get('results', []))
                logging.info('WTO data fetched successfully.')
                return df
            else:
                logging.error(f'WTO API Error: {response.status_code} - {response.text}')
                return None
        except Exception as e:
            logging.error(f'Error fetching WTO data: {e}')
            return None

    def fetch_trading_economics_data(self, indicator: str, country: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """
        Fetches economic indicators from Trading Economics API.
        """
        try:
            logging.info('Fetching data from Trading Economics API...')
            url = f'{self.trading_economics_endpoint}/historical/country/{country}/indicator/{indicator}'
            params = {
                'c': self.trading_economics_api_key,
                's': start_date,
                'e': end_date
            }
            response = self.session.get(url, headers=self.trading_economics_headers, params=params)
            if response.status_code == 200:
                data = response.json()
                df = pd.json_normalize(data)
                logging.info('Trading Economics data fetched successfully.')
                return df
            else:
                logging.error(f'Trading Economics API Error: {response.status_code} - {response.text}')
                return None
        except Exception as e:
            logging.error(f'Error fetching Trading Economics data: {e}')
            return None

    def fetch_census_export_data(self, year: str, month: str, comm_lvl: str = 'HS6', commodity_codes: Optional[List[str]] = None, date_range: Optional[List[str]] = None) -> Optional[pd.DataFrame]:
        """
        Fetches export data from the Census API with dynamic parameters.
        """
        try:
            logging.info('Fetching export data from Census API...')
            params = {
                'get': 'E_COMMODITY,ALL_VAL_MO,ALL_VAL_YR',
                'time': f'{year}-{month}',
                'COMM_LVL': comm_lvl,
                'key': self.census_api_key
            }
            if commodity_codes:
                params['E_COMMODITY'] = ','.join(commodity_codes)
            if date_range:
                params['time'] = '-'.join(date_range)
            response = self.session.get(self.census_export_endpoint, headers=self.census_headers, params=params)
            if response.status_code == 200:
                data = response.json()
                df = pd.DataFrame(data[1:], columns=data[0])
                logging.info('Census export data fetched successfully.')
                return df
            else:
                logging.error(f'Census Export API Error: {response.status_code} - {response.text}')
                return None
        except Exception as e:
            logging.error(f'Error fetching Census Export data: {e}')
            return None

    def fetch_census_import_data(self, year: str, month: str, comm_lvl: str = 'HS6', commodity_codes: Optional[List[str]] = None, date_range: Optional[List[str]] = None) -> Optional[pd.DataFrame]:
        """
        Fetches import data from the Census API with dynamic parameters.
        """
        try:
            logging.info('Fetching import data from Census API...')
            params = {
                'get': 'I_COMMODITY,GEN_VAL_MO,GEN_VAL_YR',
                'time': f'{year}-{month}',
                'COMM_LVL': comm_lvl,
                'key': self.census_api_key
            }
            if commodity_codes:
                params['I_COMMODITY'] = ','.join(commodity_codes)
            if date_range:
                params['time'] = '-'.join(date_range)
            response = self.session.get(self.census_import_endpoint, headers=self.census_headers, params=params)
            if response.status_code == 200:
                data = response.json()
                df = pd.DataFrame(data[1:], columns=data[0])
                logging.info('Census import data fetched successfully.')
                return df
            else:
                logging.error(f'Census Import API Error: {response.status_code} - {response.text}')
                return None
        except Exception as e:
            logging.error(f'Error fetching Census Import data: {e}')
            return None

    def fetch_consolidated_screening_list_data(self, countries: Optional[List[str]] = None) -> Optional[pd.DataFrame]:
        """
        Fetches data from the WTO Trade Facilitation Agreement Database (TFAD) API.
        """
        try:
            logging.info('Fetching data from Consolidated Screening List API...')
            url = self.consolidated_screening_list_endpoint
            params = {}
            if countries:
                params['countries[]'] = countries  # Assuming the API expects a list
            response = self.session.get(url, headers=self.consolidated_screening_list_headers, params=params)
            if response.status_code == 200:
                data = response.json()
                df = pd.json_normalize(data.get('data', []))
                logging.info('Consolidated Screening List data fetched successfully.')
                return df
            else:
                logging.error(f'Consolidated Screening List API Error: {response.status_code} - {response.text}')
                return None
        except Exception as e:
            logging.error(f'Error fetching Consolidated Screening List data: {e}')
            return None

    def fetch_developer_trade_gov_data(self) -> Optional[pd.DataFrame]:
        """
        Fetches data from Developer.trade.gov API as a backup.
        """
        try:
            logging.info('Fetching data from Developer.trade.gov API...')
            # Placeholder for actual API call implementation
            # Simulate data
            data = {
                'Country': ['USA', 'CHN', 'DEU', 'JPN'],
                'Trade Volume': [600e9, 500e9, 200e9, 150e9],
                'Year': [2021] * 4
            }
            df = pd.DataFrame(data)
            logging.info('Developer.trade.gov data fetched successfully.')
            return df
        except Exception as e:
            logging.error(f'Error fetching Developer.trade.gov data: {e}')
            return None

    # =================================
    # 1.2. Political Data Fetching Methods
    # =================================

    def fetch_political_parties(self, country: str) -> Optional[pd.DataFrame]:
        """
        Fetches political parties and their ideologies for a given country using the Wikipedia API.
        """
        try:
            logging.info(f'Fetching political parties for {country} from Wikipedia API...')
            search_url = "https://en.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'list': 'categorymembers',
                'cmtitle': f'Category:Political_parties_in_{country}',
                'cmlimit': 500,
                'format': 'json'
            }
            response = self.session.get(search_url, params=params)
            self._handle_response_errors(response)
            data = response.json()
            parties = data.get('query', {}).get('categorymembers', [])
            if not parties:
                logging.warning(f'No political parties found for {country}.')
                return None
            # Extract party names and potentially fetch their ideologies
            party_names = [party['title'] for party in parties]
            # Placeholder: Fetch ideologies (requires further implementation, possibly scraping or another API)
            ideologies = [self.fetch_party_ideology(party) for party in party_names]
            df = pd.DataFrame({
                'Party': party_names,
                'Ideology': ideologies
            })
            logging.info(f'Fetched {len(df)} political parties for {country}.')
            return df
        except Exception as e:
            logging.error(f'Error fetching political parties for {country}: {e}')
            return None

    def fetch_party_ideology(self, party_name: str) -> Optional[str]:
        """
        Fetches the ideology of a given political party. This can be done via Wikipedia scraping or another reliable source.
        """
        try:
            logging.info(f'Fetching ideology for party: {party_name}')
            # Example using Wikipedia page summary
            search_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{party_name.replace(' ', '_')}"
            response = self.session.get(search_url)
            self._handle_response_errors(response)
            data = response.json()
            description = data.get('extract', '').lower()
            # Simple keyword-based ideology extraction (for demonstration purposes)
            if 'conservative' in description:
                return 'Conservative'
            elif 'liberal' in description:
                return 'Liberal'
            elif 'socialist' in description:
                return 'Socialist'
            elif 'green' in description:
                return 'Green'
            elif 'nationalist' in description:
                return 'Nationalist'
            else:
                return 'Other'
        except Exception as e:
            logging.error(f'Error fetching ideology for party {party_name}: {e}')
            return None

    def fetch_upcoming_elections(self, country: str) -> Optional[pd.DataFrame]:
        """
        Fetches information about upcoming elections for a given country.
        This can be sourced from APIs like ElectionGuide or scraped from reliable websites.
        """
        try:
            logging.info(f'Fetching upcoming elections for {country}...')
            # Placeholder: Using Wikipedia's Upcoming Elections page
            search_url = "https://en.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'titles': 'Upcoming_elections',
                'prop': 'extracts',
                'explaintext': True,
                'format': 'json'
            }
            response = self.session.get(search_url, params=params)
            self._handle_response_errors(response)
            data = response.json()
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                extract = page_data.get('extract', '')
                # Parsing logic to extract election dates (requires advanced parsing or NLP)
                # Placeholder: Return None
                logging.warning('Election data parsing not implemented yet.')
                return None
        except Exception as e:
            logging.error(f'Error fetching upcoming elections for {country}: {e}')
            return None

    def fetch_government_policies(self, country: str) -> Optional[pd.DataFrame]:
        """
        Fetches current policies of the government in a given country.
        This can be sourced from official government APIs or policy databases.
        """
        try:
            logging.info(f'Fetching current policies for {country}...')
            # Placeholder: Implement actual data fetching logic
            # For demonstration, return a dummy DataFrame
            policies = {
                'Policy': ['Tariff Rate Adjustment', 'Trade Agreements', 'Domestic Industry Support'],
                'Status': ['Increasing', 'Negotiating', 'Stable']
            }
            df = pd.DataFrame(policies)
            logging.info(f'Fetched {len(df)} policies for {country}.')
            return df
        except Exception as e:
            logging.error(f'Error fetching policies for {country}: {e}')
            return None

    def fetch_all_political_data(self, countries: List[str]) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        Fetches all political-related data for a list of countries.
        """
        political_data = {}
        for country in countries:
            parties = self.fetch_political_parties(country)
            elections = self.fetch_upcoming_elections(country)
            policies = self.fetch_government_policies(country)
            political_data[country] = {
                'Political_Parties': parties,
                'Upcoming_Elections': elections,
                'Current_Policies': policies
            }
        return political_data

    # =================================
    # 1.3. News and Social Media Data Fetching Methods
    # =================================

    def fetch_news_data(self, country: str, query: str = 'trade policy', from_date: str = None, to_date: str = None) -> Optional[pd.DataFrame]:
        """
        Fetches news articles related to trade policies for a given country using NewsAPI.
        """
        try:
            logging.info(f'Fetching news data for {country}...')
            api_key = self.newsapi_key
            if not api_key:
                logging.error('NEWSAPI_KEY not set in environment variables.')
                return None
            url = 'https://newsapi.org/v2/everything'
            params = {
                'q': query,
                'apiKey': api_key,
                'language': 'en',
                'sortBy': 'relevance',
                'pageSize': 100
            }
            if from_date:
                params['from'] = from_date
            if to_date:
                params['to'] = to_date
            response = self.session.get(url, params=params)
            self._handle_response_errors(response)
            data = response.json()
            articles = data.get('articles', [])
            if not articles:
                logging.warning(f'No news articles found for {country}.')
                return None
            df = pd.json_normalize(articles)
            logging.info(f'Fetched {len(df)} news articles for {country}.')
            return df
        except Exception as e:
            logging.error(f'Error fetching news data for {country}: {e}')
            return None

    def fetch_social_media_data(self, country: str, query: str = 'trade policy', max_tweets: int = 1000) -> Optional[pd.DataFrame]:
        """
        Fetches tweets related to trade policies for a given country using Twitter API.
        """
        try:
            logging.info(f'Fetching social media data for {country}...')
            api_key = self.twitter_api_key
            api_secret = self.twitter_api_secret
            access_token = self.twitter_access_token
            access_secret = self.twitter_access_secret
            if not all([api_key, api_secret, access_token, access_secret]):
                logging.error('Twitter API credentials not set in environment variables.')
                return None
            # Initialize Tweepy (assuming it's installed)
            import tweepy
            auth = tweepy.OAuth1UserHandler(api_key, api_secret, access_token, access_secret)
            api = tweepy.API(auth, wait_on_rate_limit=True)
            tweets = tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(max_tweets)
            tweet_data = [{
                'created_at': tweet.created_at,
                'user': tweet.user.screen_name,
                'text': tweet.full_text,
                'retweets': tweet.retweet_count,
                'favorites': tweet.favorite_count
            } for tweet in tweets]
            if not tweet_data:
                logging.warning(f'No tweets found for {country}.')
                return None
            df = pd.DataFrame(tweet_data)
            logging.info(f'Fetched {len(df)} tweets for {country}.')
            return df
        except Exception as e:
            logging.error(f'Error fetching social media data for {country}: {e}')
            return None

    def fetch_all_news_data(self, countries: List[str]) -> Dict[str, pd.DataFrame]:
        """
        Fetches all news data for a list of countries.
        """
        news_data = {}
        for country in countries:
            df = self.fetch_news_data(country)
            if df is not None:
                news_data[country] = df
        return news_data

    def fetch_all_social_media_data(self, countries: List[str]) -> Dict[str, pd.DataFrame]:
        """
        Fetches all social media data for a list of countries.
        """
        social_media_data = {}
        for country in countries:
            df = self.fetch_social_media_data(country)
            if df is not None:
                social_media_data[country] = df
        return social_media_data

    # =================================
    # 1.4. Legislative Tracking Data Fetching Methods
    # =================================

    def fetch_legislation_data(self, country: str, status: str = 'active') -> Optional[pd.DataFrame]:
        """
        Fetches current legislation for a given country.
        """
        try:
            logging.info(f'Fetching legislation data for {country}...')
            # Placeholder: Implement actual data fetching logic using specific legislative APIs
            # For demonstration, return a dummy DataFrame
            legislation = {
                'Bill_Name': ['Trade Tariff Adjustment Act', 'International Trade Cooperation Act'],
                'Status': [status, status],
                'Date_Introduced': ['2023-01-15', '2023-03-22']
            }
            df = pd.DataFrame(legislation)
            logging.info(f'Fetched {len(df)} legislation records for {country}.')
            return df
        except Exception as e:
            logging.error(f'Error fetching legislation data for {country}: {e}')
            return None

    def fetch_all_legislation_data(self, countries: List[str]) -> Dict[str, pd.DataFrame]:
        """
        Fetches all legislative data for a list of countries.
        """
        legislation_data = {}
        for country in countries:
            df = self.fetch_legislation_data(country)
            if df is not None:
                legislation_data[country] = df
        return legislation_data

    # =================================
    # 1.5. Fetch All Data Method
    # =================================

    def fetch_all_data(self) -> Dict[str, Any]:
        """
        Fetches data from all primary sources, using backups if needed.
        Utilizes parallel processing to speed up data fetching.
        """
        data = {}
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_source = {}
            for source_name, fetch_function in self.primary_data_sources.items():
                if source_name in ['political_data', 'news_data', 'social_media_data', 'legislation_data']:
                    # Assuming we have a list of countries to fetch political and media data for
                    # Using World Bank data to get the list of countries
                    world_bank_df = self.raw_data.get('world_bank')
                    if world_bank_df is not None and not world_bank_df.empty:
                        countries = world_bank_df['Country'].unique().tolist()
                    else:
                        countries = ['USA']  # Default to USA if no data
                    future = executor.submit(fetch_function, countries)
                    future_to_source[future] = source_name
                else:
                    future = executor.submit(fetch_function)
                    future_to_source[future] = source_name
            for future in as_completed(future_to_source):
                source_name = future_to_source[future]
                try:
                    result = future.result()
                    if source_name in ['political_data', 'news_data', 'social_media_data', 'legislation_data']:
                        # result is a dict with country as key
                        data[source_name] = result
                    elif result is not None and not result.empty:
                        data[source_name] = result
                        logging.info(f'{source_name} data fetched and added to dataset.')
                    else:
                        # Use backup sources if primary fails
                        backup_function = self.backup_data_sources.get(source_name)
                        if backup_function:
                            backup_result = backup_function()
                            if backup_result is not None and not backup_result.empty:
                                data[source_name] = backup_result
                                logging.info(f'Backup for {source_name} used successfully.')
                except Exception as e:
                    logging.error(f'Error fetching data for {source_name}: {e}')
        return data

# =================================
# 2. Data Processing and Storage Module
# =================================

class DataProcessing:
    """
    Cleans, normalizes, and stores the fetched data for efficient retrieval and analysis.
    Employs data validation, concurrency, and integration with a robust database for scalability.
    """
    def __init__(self, data: Dict[str, Any]):
        self.raw_data = data
        self.processed_data = {}
        # Initialize database connection (PostgreSQL for scalability)
        database_url = os.getenv('DATABASE_URL', 'sqlite:///trade_data.db')
        self.engine = create_engine(database_url)
        self.Session = sessionmaker(bind=self.engine)

        # Initialize NLP models
        self.sentiment_analyzer: Pipeline = pipeline('sentiment-analysis')
        try:
            self.nlp = spacy.load('en_core_web_sm')
        except OSError:
            # If the model is not found, download it
            from spacy.cli import download
            download('en_core_web_sm')
            self.nlp = spacy.load('en_core_web_sm')

    def analyze_sentiment(self, text: str) -> str:
        """
        Analyzes the sentiment of a given text and returns it as 'Positive', 'Negative', or 'Neutral'.
        """
        try:
            result = self.sentiment_analyzer(text[:512])[0]  # Truncate to 512 tokens
            label = result['label']
            if label in ['POSITIVE', 'NEGATIVE']:
                return label.capitalize()
            return 'Neutral'
        except Exception as e:
            logging.error(f'Error analyzing sentiment: {e}')
            return 'Neutral'

    def extract_entities(self, text: str) -> List[str]:
        """
        Extracts entities (e.g., political figures, parties) from the text using spaCy.
        """
        try:
            doc = self.nlp(text)
            entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON']]
            return entities
        except Exception as e:
            logging.error(f'Error extracting entities: {e}')
            return []

    def process_news_data(self, news_df: pd.DataFrame) -> pd.DataFrame:
        """
        Processes news articles to extract sentiments and entities.
        """
        try:
            logging.info('Processing news data for sentiment and entity extraction...')
            news_df['Sentiment'] = news_df['description'].apply(self.analyze_sentiment)
            news_df['Entities'] = news_df['description'].apply(self.extract_entities)
            logging.info('News data processed successfully.')
            return news_df
        except Exception as e:
            logging.error(f'Error processing news data: {e}')
            return news_df

    def process_social_media_data(self, tweets_df: pd.DataFrame) -> pd.DataFrame:
        """
        Processes tweets to extract sentiments and entities.
        """
        try:
            logging.info('Processing social media data for sentiment and entity extraction...')
            tweets_df['Sentiment'] = tweets_df['text'].apply(self.analyze_sentiment)
            tweets_df['Entities'] = tweets_df['text'].apply(self.extract_entities)
            logging.info('Social media data processed successfully.')
            return tweets_df
        except Exception as e:
            logging.error(f'Error processing social media data: {e}')
            return tweets_df

    def process_legislation_data(self, legislation_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """
        Processes legislation data, potentially extracting sentiments or categorizing bills.
        """
        try:
            logging.info('Processing legislation data...')
            processed_legislation = {}
            for country, df in legislation_data.items():
                # Example: Extract keywords or categorize bills based on names
                df['Category'] = df['Bill_Name'].apply(lambda x: 'Trade' if 'Trade' in x else 'Other')
                processed_legislation[country] = df
            logging.info('Legislation data processed successfully.')
            return processed_legislation
        except Exception as e:
            logging.error(f'Error processing legislation data: {e}')
            return legislation_data

    def clean_political_data(self):
        """
        Cleans and processes political data, including sentiment analysis of news and social media.
        """
        try:
            logging.info('Cleaning political data...')
            for country, data in self.raw_data.get('political_data', {}).items():
                # Process news data
                news_df = self.raw_data.get('news_data', {}).get(country)
                if news_df is not None and not news_df.empty:
                    processed_news = self.process_news_data(news_df)
                    self.processed_data[f'{country}_news'] = processed_news

                # Process social media data
                tweets_df = self.raw_data.get('social_media_data', {}).get(country)
                if tweets_df is not None and not tweets_df.empty:
                    processed_tweets = self.process_social_media_data(tweets_df)
                    self.processed_data[f'{country}_tweets'] = processed_tweets

                # Process legislation data
                legislation_df = self.raw_data.get('legislation_data', {}).get(country)
                if legislation_df is not None and not legislation_df.empty:
                    processed_legislation = self.process_legislation_data({country: legislation_df})
                    self.processed_data[f'{country}_Legislation'] = processed_legislation.get(country)

                # Existing cleaning methods
                political_parties = data.get('Political_Parties')
                upcoming_elections = data.get('Upcoming_Elections')
                current_policies = data.get('Current_Policies')
                if political_parties is not None and not political_parties.empty:
                    self.processed_data[f'{country}_Political_Parties'] = political_parties
                if upcoming_elections is not None and not upcoming_elections.empty:
                    self.processed_data[f'{country}_Upcoming_Elections'] = upcoming_elections
                if current_policies is not None and not current_policies.empty:
                    self.processed_data[f'{country}_Current_Policies'] = current_policies
            logging.info('Political data cleaned and processed successfully.')
        except Exception as e:
            logging.error(f'Error cleaning political data: {e}')

    def clean_world_bank_data(self):
        """
        Cleans and normalizes World Bank data.
        """
        try:
            logging.info('Cleaning World Bank data...')
            df = self.raw_data.get('world_bank')
            if df is None or df.empty:
                logging.warning('No World Bank data to process.')
                return
            df = df.rename(columns={'economy': 'Country', 'time': 'Year', 'Value': 'Value'})
            indicator_mapping = {
                'NY.GDP.MKTP.CD': 'GDP',
                'NE.TRD.GNFS.ZS': 'Trade_Percentage_GDP'
            }
            df['Indicator'] = df['Indicator'].map(indicator_mapping)
            df_pivot = df.pivot_table(values='Value', index=['Country', 'Year'], columns='Indicator').reset_index()
            self.processed_data['world_bank'] = df_pivot
            logging.info('World Bank data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing World Bank data: {e}')

    def clean_un_comtrade_data(self):
        """
        Cleans and normalizes UN Comtrade data.
        """
        try:
            logging.info('Cleaning UN Comtrade data...')
            df = self.raw_data.get('un_comtrade')
            if df is None or df.empty:
                logging.warning('No UN Comtrade data to process.')
                return
            # Example cleaning steps for UN Comtrade data structure
            df = df.rename(columns={
                'Reporter': 'Reporter',
                'Partner': 'Partner',
                'Commodity Code': 'Commodity_Code',
                'Trade Value': 'Trade_Value',
                'Year': 'Year'
            })
            df['Trade_Value'] = pd.to_numeric(df['Trade_Value'], errors='coerce')
            df.dropna(subset=['Trade_Value'], inplace=True)
            self.processed_data['un_comtrade'] = df
            logging.info('UN Comtrade data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing UN Comtrade data: {e}')

    def clean_usitc_hts_data(self):
        """
        Cleans and normalizes USITC HTS data.
        """
        try:
            logging.info('Cleaning USITC HTS data...')
            df = self.raw_data.get('usitc_hts')
            if df is None or df.empty:
                logging.warning('No USITC HTS data to process.')
                return
            df = df.rename(columns={
                'HTS Code': 'HTS_Code',
                'Description': 'Description',
                'Tariff Rate': 'Tariff_Rate'
            })
            df['Tariff_Rate'] = pd.to_numeric(df['Tariff_Rate'], errors='coerce')
            df.dropna(subset=['Tariff_Rate'], inplace=True)
            self.processed_data['usitc_hts'] = df
            logging.info('USITC HTS data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing USITC HTS data: {e}')

    def clean_gta_data(self):
        """
        Cleans and normalizes GTA data.
        """
        try:
            logging.info('Cleaning GTA data...')
            df = self.raw_data.get('gta')
            if df is None or df.empty:
                logging.warning('No GTA data to process.')
                return
            # Example cleaning steps for GTA data structure
            df = df.rename(columns={
                'intervention_id': 'Intervention_ID',
                'state_act_id': 'State_Act_ID',
                'state_act_title': 'State_Act_Title',
                'intervention_url': 'Intervention_URL',
                'state_act_url': 'State_Act_URL',
                'gta_evaluation': 'GTA_Evaluation',
                'date_announced': 'Date_Announced',
                'date_published': 'Date_Published',
                'date_implemented': 'Date_Implemented',
                'date_removed': 'Date_Removed',
                'is_in_force': 'Is_In_Force'
            })
            # Convert date columns to datetime type for uniformity and ease of analysis
            date_columns = ['Date_Announced', 'Date_Published', 'Date_Implemented', 'Date_Removed']
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], errors='coerce')
            self.processed_data['gta'] = df
            logging.info('GTA data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing GTA data: {e}')

    def clean_imf_data(self):
        """
        Cleans and normalizes IMF data.
        """
        try:
            logging.info('Cleaning IMF data...')
            df = self.raw_data.get('imf')
            if df is None or df.empty:
                logging.warning('No IMF data to process.')
                return
            # Example cleaning steps for IMF data structure
            df = df.rename(columns={
                'Country': 'Country',
                'Indicator': 'Indicator',
                'Year': 'Year',
                'Value': 'Value'
            })
            df['Value'] = pd.to_numeric(df['Value'], errors='coerce')
            df.dropna(subset=['Value'], inplace=True)
            self.processed_data['imf'] = df
            logging.info('IMF data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing IMF data: {e}')

    def clean_trading_economics_data(self):
        """
        Cleans and normalizes Trading Economics data.
        """
        try:
            logging.info('Cleaning Trading Economics data...')
            df = self.raw_data.get('trading_economics')
            if df is None or df.empty:
                logging.warning('No Trading Economics data to process.')
                return
            # Example cleaning steps for Trading Economics data structure
            df = df.rename(columns={
                'date': 'Date',
                'value': 'Value',
                'symbol': 'Symbol',
                'country': 'Country'
            })
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            df['Value'] = pd.to_numeric(df['Value'], errors='coerce')
            df.dropna(subset=['Value'], inplace=True)
            self.processed_data['trading_economics'] = df
            logging.info('Trading Economics data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing Trading Economics data: {e}')

    def clean_census_export_data(self):
        """
        Cleans and normalizes Census export data.
        """
        try:
            logging.info('Cleaning Census Export data...')
            df = self.raw_data.get('census_exports')
            if df is None or df.empty:
                logging.warning('No Census Export data to process.')
                return
            df = df.rename(columns={
                'E_COMMODITY': 'Commodity_Code',
                'ALL_VAL_MO': 'All_Val_Month',
                'ALL_VAL_YR': 'All_Val_Year'
            })
            df['All_Val_Month'] = pd.to_numeric(df['All_Val_Month'], errors='coerce')
            df['All_Val_Year'] = pd.to_numeric(df['All_Val_Year'], errors='coerce')
            df.dropna(subset=['All_Val_Month', 'All_Val_Year'], inplace=True)
            self.processed_data['census_exports'] = df
            logging.info('Census Export data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing Census Export data: {e}')

    def clean_census_import_data(self):
        """
        Cleans and normalizes Census import data.
        """
        try:
            logging.info('Cleaning Census Import data...')
            df = self.raw_data.get('census_imports')
            if df is None or df.empty:
                logging.warning('No Census Import data to process.')
                return
            df = df.rename(columns={
                'I_COMMODITY': 'Commodity_Code',
                'GEN_VAL_MO': 'General_Val_Month',
                'GEN_VAL_YR': 'General_Val_Year'
            })
            df['General_Val_Month'] = pd.to_numeric(df['General_Val_Month'], errors='coerce')
            df['General_Val_Year'] = pd.to_numeric(df['General_Val_Year'], errors='coerce')
            df.dropna(subset=['General_Val_Month', 'General_Val_Year'], inplace=True)
            self.processed_data['census_imports'] = df
            logging.info('Census Import data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing Census Import data: {e}')

    def clean_consolidated_screening_list_data(self):
        """
        Cleans and normalizes Consolidated Screening List data.
        """
        try:
            logging.info('Cleaning Consolidated Screening List data...')
            df = self.raw_data.get('consolidated_screening_list')
            if df is None or df.empty:
                logging.warning('No Consolidated Screening List data to process.')
                return
            # Example cleaning steps
            df = df.rename(columns={
                'id': 'ID',
                'country_name': 'Country_Name',
                'country_code': 'Country_Code',
                'trade_vol': 'Trade_Volume',
                # Adjust as needed based on actual data structure from the API response
            })
            df['Trade_Volume'] = pd.to_numeric(df['Trade_Volume'], errors='coerce')
            df.dropna(subset=['Trade_Volume'], inplace=True)
            self.processed_data['consolidated_screening_list'] = df
            logging.info('Consolidated Screening List data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing Consolidated Screening List data: {e}')

    def clean_wto_data(self):
        """
        Cleans and normalizes WTO data.
        """
        try:
            logging.info('Cleaning WTO data...')
            df = self.raw_data.get('wto_api')
            if df is None or df.empty:
                logging.warning('No WTO data to process.')
                return
            # Example cleaning steps if the WTO data structure is known (placeholder)
            self.processed_data['wto'] = df
            logging.info('WTO data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing WTO data: {e}')

    def clean_developer_trade_gov_data(self):
        """
        Cleans and normalizes data from Developer.trade.gov API.
        """
        try:
            logging.info('Cleaning Developer.trade.gov data...')
            df = self.raw_data.get('developer_trade_gov')
            if df is None or df.empty:
                logging.warning('No Developer.trade.gov data to process.')
                return
            # Example cleaning steps as placeholders for actual data structure from Developer.trade.gov API
            df = df.rename(columns={
                'Country': 'Country',
                'Trade Volume': 'Trade_Volume',
                'Year': 'Year'
            })
            df['Trade_Volume'] = pd.to_numeric(df['Trade_Volume'], errors='coerce')
            df.dropna(subset=['Trade_Volume'], inplace=True)
            self.processed_data['developer_trade_gov'] = df
            logging.info('Developer.trade.gov data cleaned successfully.')
        except Exception as e:
            logging.error(f'Error processing Developer.trade_gov data: {e}')

    def clean_all_data(self):
        """
        Cleans and processes all data, including trade, political, news, and social media data.
        """
        try:
            logging.info('Starting comprehensive data cleaning and processing...')
            # Clean trade-related data
            self.clean_world_bank_data()
            self.clean_un_comtrade_data()
            self.clean_usitc_hts_data()
            self.clean_gta_data()
            self.clean_imf_data()
            self.clean_trading_economics_data()
            self.clean_census_export_data()
            self.clean_census_import_data()
            self.clean_consolidated_screening_list_data()
            self.clean_wto_data()
            self.clean_developer_trade_gov_data()

            # Clean political and media-related data
            self.clean_political_data()

            logging.info('All data cleaned and processed successfully.')
        except Exception as e:
            logging.error(f'Error during comprehensive data cleaning: {e}')

    def save_to_database(self):
        """
        Saves processed data to the database, ensuring data integrity and logging relevant issues.
        """
        try:
            logging.info('Saving data to the database...')
            for table_name, df in self.processed_data.items():
                # Data validation: remove duplicates, missing values, etc.
                if df.duplicated().any():
                    df = df.drop_duplicates()
                    logging.warning(f'Duplicates found and removed in {table_name} data.')
                if df.isnull().values.any():
                    df = df.dropna()
                    logging.warning(f'Missing values found and removed in {table_name} data.')

                df.to_sql(table_name, self.engine, if_exists='replace', index=False)
                logging.info(f'Data saved to table: {table_name}')
        except Exception as e:
            logging.error(f'Error saving data to database: {e}')

    def process_all_data(self):
        """
        Orchestrates the data cleaning and processing tasks for all data sources concurrently.
        Uses concurrency for efficient data processing and stores the final cleaned data in a database.
        """
        tasks = [
            {'func': self.clean_world_bank_data},
            {'func': self.clean_un_comtrade_data},
            {'func': self.clean_usitc_hts_data},
            {'func': self.clean_gta_data},
            {'func': self.clean_imf_data},
            {'func': self.clean_trading_economics_data},
            {'func': self.clean_census_export_data},
            {'func': self.clean_census_import_data},
            {'func': self.clean_consolidated_screening_list_data},
            {'func': self.clean_wto_data},
            {'func': self.clean_developer_trade_gov_data},
            {'func': self.clean_political_data},  # New cleaning task
        ]
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(task['func']) for task in tasks]
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logging.error(f'Error in data processing task: {e}')
        self.save_to_database()

# =================================
# 3. Agent-Based Modeling Module
# =================================

class GovernmentAgent(Agent):
    """
    Government agents represent countries and their economic policies with advanced economic strategies and communications.
    """
    def __init__(self, unique_id, model, country_code, economic_indicators, political_parties: Optional[pd.DataFrame], upcoming_elections: Optional[pd.DataFrame], current_policies: Optional[pd.DataFrame]):
        super().__init__(unique_id, model)
        self.country_code = country_code
        self.economic_indicators = economic_indicators  # e.g., GDP, trade balance, etc.
        self.policies = {'Tariff_Rate': 0}  # Initialize with default tariff rate
        self.trade_alliances = set()  # Countries this government has alliances with

        # New Attributes
        self.political_parties = political_parties  # DataFrame with Party and Ideology
        self.upcoming_elections = upcoming_elections  # DataFrame with election details
        self.current_policies = current_policies  # DataFrame with current policy statuses
        self.governing_party = self.determine_governing_party()
        self.policy_shift_probability = self.determine_policy_shift_probability()

        # Attach news and social media data if available
        self.news_data = self.model.processed_data.get(f'{self.country_code}_news')
        self.tweets_data = self.model.processed_data.get(f'{self.country_code}_tweets')
        self.legislation_data = self.model.processed_data.get(f'{self.country_code}_Legislation')

    def determine_governing_party(self) -> Optional[str]:
        """
        Determines the current governing party based on existing data.
        """
        # Placeholder: Assume the first party is governing; implement actual logic based on data
        if self.political_parties is not None and not self.political_parties.empty:
            return self.political_parties.iloc[0]['Party']
        return None

    def determine_policy_shift_probability(self) -> float:
        """
        Determines the probability of a policy shift based on factors like upcoming elections and economic indicators.
        """
        base_probability = 0.05  # Base 5% chance of policy shift each step
        # Increase probability if an upcoming election is near
        if self.upcoming_elections is not None and not self.upcoming_elections.empty:
            # Placeholder: Assuming 'Date_Upcoming_Election' column exists
            # Here, we use the first upcoming election date
            election_date = pd.to_datetime(self.upcoming_elections.iloc[0].get('Date_Upcoming_Election', pd.NaT))
            if pd.notna(election_date):
                days_until_election = (election_date - pd.Timestamp.today()).days
                if days_until_election <= 180:  # 6 months
                    base_probability += 0.10  # Additional 10% chance
        # Modify based on economic indicators
        gdp_growth = self.economic_indicators.get('GDP_Growth', 0)
        if gdp_growth < 0:
            base_probability += 0.05  # Additional 5% chance to shift policies during economic downturn
        return min(base_probability, 1.0)

    def implement_policy(self):
        """
        Decides and implements policies based on economic indicators, political ideology, public sentiment, and other factors.
        Includes dynamic tariff adjustments, responding to trade balances, and domestic industries protection.
        """
        # Incorporate political ideology into policy decisions
        ideology = self.political_parties[self.political_parties['Party'] == self.governing_party]['Ideology'].values
        ideology = ideology[0] if len(ideology) > 0 else 'Other'

        trade_percentage = self.economic_indicators.get('Trade_Percentage_GDP', 0)

        # Analyze public sentiment from news and social media
        public_sentiment = self.calculate_public_sentiment()

        # Policy logic influenced by ideology and public sentiment
        if ideology == 'Conservative':
            if trade_percentage < 20 or public_sentiment == 'Negative':
                self.policies['Tariff_Rate'] = min(self.policies.get('Tariff_Rate', 0) + 1, 25)
                logging.info(f'Government {self.country_code} (Conservative): Increasing tariff rate to {self.policies["Tariff_Rate"]}% due to low trade percentage or negative public sentiment.')
            elif trade_percentage > 40 and public_sentiment == 'Positive':
                self.policies['Tariff_Rate'] = max(self.policies.get('Tariff_Rate', 0) - 1, 0)
                logging.info(f'Government {self.country_code} (Conservative): Decreasing tariff rate to {self.policies["Tariff_Rate"]}% due to high trade percentage and positive public sentiment.')
            else:
                logging.info(f'Government {self.country_code} (Conservative): Maintaining tariff rate at {self.policies["Tariff_Rate"]}%.')
        elif ideology == 'Liberal':
            if trade_percentage < 20 or public_sentiment == 'Negative':
                self.policies['Tariff_Rate'] = min(self.policies.get('Tariff_Rate', 0) + 0.5, 20)
                logging.info(f'Government {self.country_code} (Liberal): Slightly increasing tariff rate to {self.policies["Tariff_Rate"]}% due to low trade percentage or negative public sentiment.')
            elif trade_percentage > 40 and public_sentiment == 'Positive':
                self.policies['Tariff_Rate'] = max(self.policies.get('Tariff_Rate', 0) - 2, 0)
                logging.info(f'Government {self.country_code} (Liberal): Decreasing tariff rate to {self.policies["Tariff_Rate"]}% due to high trade percentage and positive public sentiment.')
            else:
                logging.info(f'Government {self.country_code} (Liberal): Maintaining tariff rate at {self.policies["Tariff_Rate"]}%.')
        elif ideology == 'Mixed':
            if trade_percentage < 20 or public_sentiment == 'Negative':
                self.policies['Tariff_Rate'] = min(self.policies.get('Tariff_Rate', 0) + 0.5, 20)
                logging.info(f'Government {self.country_code} (Mixed): Increasing tariff rate to {self.policies["Tariff_Rate"]}% due to low trade percentage or negative public sentiment.')
            elif trade_percentage > 40 and public_sentiment == 'Positive':
                self.policies['Tariff_Rate'] = max(self.policies.get('Tariff_Rate', 0) - 1.5, 0)
                logging.info(f'Government {self.country_code} (Mixed): Decreasing tariff rate to {self.policies["Tariff_Rate"]}% due to high trade percentage and positive public sentiment.')
            else:
                logging.info(f'Government {self.country_code} (Mixed): Maintaining tariff rate at {self.policies["Tariff_Rate"]}%.')
        else:
            # Default policy logic
            if trade_percentage < 20 or public_sentiment == 'Negative':
                self.policies['Tariff_Rate'] = min(self.policies.get('Tariff_Rate', 0) + 1, 25)
                logging.info(f'Government {self.country_code} (Other): Increasing tariff rate to {self.policies["Tariff_Rate"]}% due to low trade percentage or negative public sentiment.')
            elif trade_percentage > 40 and public_sentiment == 'Positive':
                self.policies['Tariff_Rate'] = max(self.policies.get('Tariff_Rate', 0) - 1, 0)
                logging.info(f'Government {self.country_code} (Other): Decreasing tariff rate to {self.policies["Tariff_Rate"]}% due to high trade percentage and positive public sentiment.')
            else:
                logging.info(f'Government {self.country_code} (Other): Maintaining tariff rate at {self.policies["Tariff_Rate"]}%.')

        # Implement policy shifts based on probability
        if random.random() < self.policy_shift_probability:
            self.shift_policy(ideology)

    def shift_policy(self, ideology: str):
        """
        Shifts policies based on political ideology and other factors.
        """
        logging.info(f'Government {self.country_code}: Shifting policies based on ideology {ideology}.')
        if ideology == 'Conservative':
            # Implement conservative policy shifts, e.g., further tariffs, trade protection
            old_tariff = self.policies['Tariff_Rate']
            self.policies['Tariff_Rate'] = min(self.policies['Tariff_Rate'] + 1, 30)
            logging.info(f'Government {self.country_code} (Conservative): Further increasing tariff rate from {old_tariff}% to {self.policies["Tariff_Rate"]}%.')
        elif ideology == 'Liberal':
            # Implement liberal policy shifts, e.g., reducing tariffs, promoting free trade
            old_tariff = self.policies['Tariff_Rate']
            self.policies['Tariff_Rate'] = max(self.policies['Tariff_Rate'] - 2, 0)
            logging.info(f'Government {self.country_code} (Liberal): Further decreasing tariff rate from {old_tariff}% to {self.policies["Tariff_Rate"]}%.')
        elif ideology == 'Mixed':
            # Implement mixed policy shifts
            old_tariff = self.policies['Tariff_Rate']
            self.policies['Tariff_Rate'] = max(self.policies['Tariff_Rate'] - 1, 0)
            logging.info(f'Government {self.country_code} (Mixed): Decreasing tariff rate from {old_tariff}% to {self.policies["Tariff_Rate"]}%.')
        else:
            # Default or other ideologies
            old_tariff = self.policies['Tariff_Rate']
            self.policies['Tariff_Rate'] = max(self.policies['Tariff_Rate'] - 1, 0)
            logging.info(f'Government {self.country_code} (Other): Decreasing tariff rate from {old_tariff}% to {self.policies["Tariff_Rate"]}%.')

    def implement_trade_agreements(self):
        """
        Implements or renegotiates trade agreements based on current policies and alliances.
        """
        # Placeholder for advanced trade agreement logic based on policies and alliances
        pass

    def calculate_public_sentiment(self) -> str:
        """
        Calculates the overall public sentiment based on news and social media data.
        Returns 'Positive', 'Negative', or 'Neutral'.
        """
        sentiments = []
        # Analyze news sentiment
        if self.news_data is not None and not self.news_data.empty:
            sentiments += self.news_data['Sentiment'].tolist()
        # Analyze social media sentiment
        if self.tweets_data is not None and not self.tweets_data.empty:
            sentiments += self.tweets_data['Sentiment'].tolist()
        if not sentiments:
            return 'Neutral'
        # Calculate the most common sentiment
        sentiment_counts = pd.Series(sentiments).value_counts()
        dominant_sentiment = sentiment_counts.idxmax()
        return dominant_sentiment

    def implement_policy_shift_logic(self, decision_df: pd.DataFrame):
        """
        Implements decisions from the decision logic DataFrame.
        """
        for _, decision in decision_df.iterrows():
            trigger_event = decision.get('Trigger_Event')
            # Evaluate the trigger event; this requires defining how trigger events are represented
            # For example, trigger_event could be a condition based on model state
            # Here, we'll assume trigger_event is a Python expression as a string
            try:
                # Using eval is dangerous; ensure that trigger_event is sanitized and controlled
                if eval(trigger_event, {"model": self.model, "agent": self}):
                    self.execute_decision(decision)
            except Exception as e:
                logging.error(f"Error evaluating trigger event '{trigger_event}' for decision '{decision.get('Decision_ID')}': {e}")

    def execute_decision(self, decision):
        """
        Executes the decision by updating policies or performing actions.

        Args:
            decision (pd.Series): A row from the decision logic DataFrame.
        """
        decision_id = decision.get('Decision_ID')
        description = decision.get('Description')
        expected_outcome = decision.get('Expected_Outcome')
        implementation_date = decision.get('Implementation_Date')

        # Example implementation: Update tariff rate based on decision description
        if 'increase tariff' in description.lower():
            old_tariff = self.policies.get('Tariff_Rate', 0)
            self.policies['Tariff_Rate'] = min(old_tariff + 1, 25)  # Increment tariff rate
            logging.info(f"Decision {decision_id}: {description} - Tariff rate increased from {old_tariff}% to {self.policies['Tariff_Rate']}%.")
        elif 'decrease tariff' in description.lower():
            old_tariff = self.policies.get('Tariff_Rate', 0)
            self.policies['Tariff_Rate'] = max(old_tariff - 1, 0)  # Decrement tariff rate
            logging.info(f"Decision {decision_id}: {description} - Tariff rate decreased from {old_tariff}% to {self.policies['Tariff_Rate']}%.")
        # Add more decision execution logic as needed

    def implement_decisions(self, decision_df: pd.DataFrame):
        """
        Implements decisions from the decision logic DataFrame based on trigger events.
        """
        self.implement_policy_shift_logic(decision_df)

    def step(self):
        """
        Defines the agent's behavior each simulation step, including policy implementation and reacting to events.
        """
        self.implement_policy()
        self.implement_trade_agreements()
        # Implement decisions from decision logic
        decision_df = self.model.decision_logics.get((self.country_code, self.governing_party))
        if decision_df is not None and not decision_df.empty:
            self.implement_decisions(decision_df)

        # Check for upcoming elections within the prediction horizon (e.g., 6 months)
        if self.upcoming_elections is not None and not self.upcoming_elections.empty:
            election_date = pd.to_datetime(self.upcoming_elections.iloc[0].get('Date_Upcoming_Election', pd.NaT))
            if pd.notna(election_date):
                days_until_election = (election_date - pd.Timestamp.today()).days
                if 0 < days_until_election <= 180:
                    self.policy_shift_probability += 0.05  # Increase chance of policy shift as election approaches
                    logging.info(f'Government {self.country_code}: Election upcoming in {days_until_election} days. Increased policy shift probability.')

        # Ensure probability doesn't exceed 1
        self.policy_shift_probability = min(self.policy_shift_probability, 1.0)

        # Randomly decide to form a trade alliance or negotiate each step.
        if random.random() < 0.05:  # 5% chance each step to form alliance or negotiate.
            gov_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code != self.country_code]
            if gov_agents:
                partner = random.choice(gov_agents)
                self.negotiate_trade_agreement(partner)

        # Respond to random economic shocks, e.g., 1% chance.
        if random.random() < 0.01:
            shock = random.choice(['recession', 'boom'])
            self.respond_to_shock(shock)

    def negotiate_trade_agreement(self, partner_agent: 'GovernmentAgent'):
        """
        Negotiates a trade agreement with another government agent, potentially reducing tariffs if both partners agree.
        """
        # Example simple negotiation logic: if both countries have relatively low tariff rates, reduce further and form alliance.
        if self.policies['Tariff_Rate'] < 10 and partner_agent.policies['Tariff_Rate'] < 10:
            old_tariff_self = self.policies['Tariff_Rate']
            old_tariff_partner = partner_agent.policies['Tariff_Rate']
            self.policies['Tariff_Rate'] = max(self.policies['Tariff_Rate'] - 1, 0)
            partner_agent.policies['Tariff_Rate'] = max(partner_agent.policies['Tariff_Rate'] - 1, 0)
            self.form_trade_alliance(partner_agent.country_code)
            partner_agent.form_trade_alliance(self.country_code)
            logging.info(f'Government {self.country_code} and {partner_agent.country_code}: Negotiated a trade agreement. Tariff rates reduced from {old_tariff_self}% to {self.policies["Tariff_Rate"]}% and from {old_tariff_partner}% to {partner_agent.policies["Tariff_Rate"]}%.')

    def respond_to_shock(self, shock_type: str):
        """
        Responds to global economic shocks like recessions or booms by adjusting domestic policies.
        """
        if shock_type == 'recession':
            # Implement expansionary policy to stimulate economy, e.g., reduce tariffs to boost trade or invest in domestic sectors.
            old_tariff = self.policies.get('Tariff_Rate', 0)
            self.policies['Tariff_Rate'] = max(old_tariff - 2, 0)
            logging.info(f'Government {self.country_code}: Responding to recession by decreasing tariff rate from {old_tariff}% to {self.policies["Tariff_Rate"]}%.')
        elif shock_type == 'boom':
            # Implement contractionary policy to cool down economy, e.g., increase tariffs to protect domestic industries from overheating.
            old_tariff = self.policies.get('Tariff_Rate', 0)
            self.policies['Tariff_Rate'] = min(old_tariff + 2, 30)
            logging.info(f'Government {self.country_code}: Responding to boom by increasing tariff rate from {old_tariff}% to {self.policies["Tariff_Rate"]}%.')

    def form_trade_alliance(self, partner_country_code: str):
        """
        Forms a trade alliance with another country or modifies policy if the alliance already exists.
        """
        if partner_country_code not in self.trade_alliances:
            self.trade_alliances.add(partner_country_code)
            logging.info(f'Government {self.country_code}: Formed a new trade alliance with {partner_country_code}.')
        else:
            # The alliance already exists, possibly strengthen it or apply advanced logic if needed.
            logging.info(f'Government {self.country_code}: Already in trade alliance with {partner_country_code}, strengthening alliance.')

class CompanyAgent(Agent):
    """
    Company agents represent businesses in different industry sectors, with advanced strategies such as investing in industries, adjusting production, and supply chain management.
    """
    def __init__(self, unique_id, model, name, industry_sector, country_code, financial_data):
        super().__init__(unique_id, model)
        self.name = name
        self.industry_sector = industry_sector
        self.country_code = country_code
        self.financial_data = financial_data
        self.production_capacity = financial_data.get('Production_Capacity', 100)
        self.supply_chain = []  # List of supplier company agents or countries where inputs are sourced from
        self.financial_health = financial_data.get('Financial_Health', 100)
        self.demand_preferences = {'domestic': 0.5, 'imported': 0.5}  # Simplified preference for domestic vs. imported goods if relevant

    def make_sourcing_decision(self):
        """
        Adjust sourcing decisions based on government policies, such as tariffs, and market conditions.
        This simulates supply chain shifts in response to changing trade policies or other economic conditions.
        """
        government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == self.country_code]
        if government_agents:
            tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)
            if tariff_rate > 10:
                # If tariffs are high, the company might increase local sourcing to avoid import costs.
                self.demand_preferences['domestic'] += 0.1
                self.demand_preferences['imported'] = max(self.demand_preferences['imported'] - 0.1, 0)
                logging.info(f'Company {self.name} in {self.country_code}: Adjusting sourcing towards domestic suppliers due to high tariffs.')
            else:
                # If tariffs are low, the company might maintain or even increase foreign sourcing for cheaper imports.
                self.demand_preferences['imported'] += 0.05
                self.demand_preferences['domestic'] = max(self.demand_preferences['domestic'] - 0.05, 0)
                logging.info(f'Company {self.name} in {self.country_code}: Adjusting sourcing towards imported goods due to low tariffs.')

    def adjust_production(self):
        """
        Adjust production capacity based on demand, supply chain status, and financial health.
        """
        # Placeholder logic: production capacity changes in response to simulated demand.
        demand = self.model.compute_demand(self.industry_sector)
        old_capacity = self.production_capacity
        if demand > self.production_capacity:
            self.production_capacity += 10  # Increase capacity if demand is high.
            logging.info(f'Company {self.name} in {self.country_code}: Increased production capacity from {old_capacity} to {self.production_capacity} due to rising demand.')
        elif demand < self.production_capacity * 0.8:
            self.production_capacity = max(self.production_capacity - 10, 50)  # Decrease capacity if demand is low.
            logging.info(f'Company {self.name} in {self.country_code}: Decreased production capacity from {old_capacity} to {self.production_capacity} due to falling demand.')

    def invest_in_industry(self, investment_amount: float):
        """
        Invest in industry to improve the company's financial health or production capacity.
        This simulates decisions like expanding factories, training employees, or R&D spending.
        """
        old_financial_health = self.financial_health
        old_capacity = self.production_capacity
        self.financial_health += investment_amount * 0.1  # Example investment effect on financial health.
        self.production_capacity += investment_amount * 0.05  # Example effect on production capacity.
        logging.info(f'Company {self.name} in {self.country_code}: Invested {investment_amount}, financial health improved from {old_financial_health} to {self.financial_health} and capacity from {old_capacity} to {self.production_capacity}.')

    def step(self):
        """
        Defines the agent's behavior at each step, including sourcing decisions, production adjustments, and potential investments.
        """
        self.make_sourcing_decision()
        self.adjust_production()
        # Example: Random investment decisions to simulate growth or adaptation strategies.
        if random.random() < 0.02:  # 2% chance each step to invest.
            investment = random.uniform(1000, 5000)
            self.invest_in_industry(investment)

class ConsumerAgent(Agent):
    """
    Consumer agents represent individual or aggregate consumer behavior with advanced preferences and responses to policies.
    """
    def __init__(self, unique_id, model, income_level, country_code):
        super().__init__(unique_id, model)
        self.income_level = income_level  # 'Low', 'Medium', 'High' income category for simplified model.
        self.country_code = country_code
        self.demand_preferences = self.generate_preferences()  # Preferences for goods based on income level and policy.

    def generate_preferences(self):
        """
        Generates demand preferences based on income level. Higher income consumers prefer more luxury goods.
        """
        if self.income_level == 'Low':
            return {'essential_goods': 0.7, 'luxury_goods': 0.3}
        elif self.income_level == 'Medium':
            return {'essential_goods': 0.5, 'luxury_goods': 0.5}
        else:  # High income consumers prefer more luxury goods.
            return {'essential_goods': 0.3, 'luxury_goods': 0.7}

    def adjust_consumption(self):
        """
        Adjusts consumption behavior based on changing prices, income, and government policies like tariffs.
        If tariffs are high on imported goods, consumers might buy more domestic goods.
        """
        government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == self.country_code]
        if government_agents:
            tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)
            if tariff_rate > 10:
                # If tariffs on imported goods are high, reduce luxury goods consumption (assuming many are imported)
                old_luxury_pref = self.demand_preferences['luxury_goods']
                self.demand_preferences['luxury_goods'] *= 0.9  # reduce consumption of expensive imported luxuries.
                logging.info(f'Consumer in {self.country_code}: Decreasing consumption of imported luxury goods from preference {old_luxury_pref} to {self.demand_preferences["luxury_goods"]} due to high tariffs.')
            else:
                # If tariffs are low, possibly maintain or increase luxury goods consumption.
                old_luxury_pref = self.demand_preferences['luxury_goods']
                self.demand_preferences['luxury_goods'] *= 1.05
                logging.info(f'Consumer in {self.country_code}: Slightly increasing consumption of luxury goods from preference {old_luxury_pref} to {self.demand_preferences["luxury_goods"]} due to low tariffs.')

    def step(self):
        """
        Defines the agent's behavior at each step, adjusting consumption preferences if needed.
        """
        self.adjust_consumption()

class IntermediaryAgent(Agent):
    """
    Intermediary agents represent logistics providers, financial institutions, or other services influencing trade.
    They can provide services such as improving supply chain efficiency or offering financing solutions.
    """
    def __init__(self, unique_id, model, service_type):
        super().__init__(unique_id, model)
        self.service_type = service_type  # 'Logistics', 'Finance', etc.

    def provide_service(self):
        """
        Provides services to other agents based on demand, possibly affecting overall trade efficiency and costs.
        """
        # Placeholder service provision logic. For example, logistics agents could reduce shipping times, finance agents could reduce financial constraints.
        pass

    def step(self):
        """
        Defines the agent's behavior at each step. This can include adjusting fees or expanding services as needed.
        """
        self.provide_service()

class ConsumerAgent(Agent):
    """
    Consumer agents represent individual or aggregate consumer behavior with advanced preferences and responses to policies.
    """
    def __init__(self, unique_id, model, income_level, country_code):
        super().__init__(unique_id, model)
        self.income_level = income_level  # 'Low', 'Medium', 'High' income category for simplified model.
        self.country_code = country_code
        self.demand_preferences = self.generate_preferences()  # Preferences for goods based on income level and policy.

    def generate_preferences(self):
        """
        Generates demand preferences based on income level. Higher income consumers prefer more luxury goods.
        """
        if self.income_level == 'Low':
            return {'essential_goods': 0.7, 'luxury_goods': 0.3}
        elif self.income_level == 'Medium':
            return {'essential_goods': 0.5, 'luxury_goods': 0.5}
        else:  # High income consumers prefer more luxury goods.
            return {'essential_goods': 0.3, 'luxury_goods': 0.7}

    def adjust_consumption(self):
        """
        Adjusts consumption behavior based on changing prices, income, and government policies like tariffs.
        If tariffs are high on imported goods, consumers might buy more domestic goods.
        """
        government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == self.country_code]
        if government_agents:
            tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)
            if tariff_rate > 10:
                # If tariffs on imported goods are high, reduce luxury goods consumption (assuming many are imported)
                old_luxury_pref = self.demand_preferences['luxury_goods']
                self.demand_preferences['luxury_goods'] *= 0.9  # reduce consumption of expensive imported luxuries.
                logging.info(f'Consumer in {self.country_code}: Decreasing consumption of imported luxury goods from preference {old_luxury_pref} to {self.demand_preferences["luxury_goods"]} due to high tariffs.')
            else:
                # If tariffs are low, possibly maintain or increase luxury goods consumption.
                old_luxury_pref = self.demand_preferences['luxury_goods']
                self.demand_preferences['luxury_goods'] *= 1.05
                logging.info(f'Consumer in {self.country_code}: Slightly increasing consumption of luxury goods from preference {old_luxury_pref} to {self.demand_preferences["luxury_goods"]} due to low tariffs.')

    def step(self):
        """
        Defines the agent's behavior at each step, adjusting consumption preferences if needed.
        """
        self.adjust_consumption()

class TradeModel(Model):
    """
    The overall model that includes all agents and manages their interactions. Now enhanced to incorporate
    advanced behaviors, data-driven initialization, and complex interactions between agents.
    """
    def __init__(self, processed_data: Dict[str, pd.DataFrame], political_data: Dict[str, Dict[str, pd.DataFrame]], news_data: Dict[str, pd.DataFrame], social_media_data: Dict[str, pd.DataFrame], legislation_data: Dict[str, pd.DataFrame]):
        super().__init__()
        self.schedule = RandomActivation(self)
        self.processed_data = processed_data  # Processed data from the DataProcessing module
        self.political_data = political_data  # Political data fetched separately
        self.news_data = news_data  # News data fetched separately
        self.social_media_data = social_media_data  # Social media data fetched separately
        self.legislation_data = legislation_data  # Legislation data fetched separately
        self.datacollector = DataCollector(model_reporters={"Trade_Volume": self.compute_trade_volume})

        # Assuming processed_data includes data from the World Bank to obtain a list of countries
        country_codes = self.processed_data.get('world_bank', pd.DataFrame()).get('Country', []).unique()
        if not isinstance(country_codes, np.ndarray):
            country_codes = []
        self.country_codes = country_codes
        self.create_agents()
        self.running = True
        self.trade_volume_history = []

        # Placeholder for decision logics
        self.decision_logics = {}  # To be filled after GPT-4 integration

        # Placeholder for future events
        self.future_events = []  # List to store future events

    def create_agents(self):
        """
        Creates agents for the model, including government, company, consumer, and intermediary agents,
        leveraging processed data to initialize their states.
        """
        # Create government agents for all countries present in processed data or default set if data is unavailable.
        for i, country_code in enumerate(self.country_codes):
            economic_indicators = self.get_economic_indicators(country_code)
            political_info = self.political_data.get(country_code, {})
            political_parties = political_info.get('Political_Parties')
            upcoming_elections = political_info.get('Upcoming_Elections')
            current_policies = political_info.get('Current_Policies')
            news = self.news_data.get(country_code)
            tweets = self.social_media_data.get(country_code)
            legislation = self.legislation_data.get(country_code)

            government_agent = GovernmentAgent(
                unique_id=i,
                model=self,
                country_code=country_code,
                economic_indicators=economic_indicators,
                political_parties=political_parties,
                upcoming_elections=upcoming_elections,
                current_policies=current_policies
            )
            # Attach news and social media data to the agent
            government_agent.news_data = news
            government_agent.tweets_data = tweets
            # Attach legislation data if needed
            government_agent.legislation_data = legislation
            self.schedule.add(government_agent)

        # Create company agents using hypothetical data or processed data. This can be data-driven if real data is available.
        top_companies = self.get_top_companies()
        for i, company in enumerate(top_companies, start=len(self.country_codes)):
            company_agent = CompanyAgent(i, self, company['name'], company['sector'], company['country_code'], company['financial_data'])
            self.schedule.add(company_agent)

        # Create consumer agents representing populations. This can be scaled by population data, if available.
        num_consumers = 100  # Adjust based on complexity and performance considerations
        for i in range(len(self.country_codes) + len(top_companies), len(self.country_codes) + len(top_companies) + num_consumers):
            income_level = random.choice(['Low', 'Medium', 'High'])
            country_code = random.choice(self.country_codes) if len(self.country_codes) > 0 else 'USA'
            consumer_agent = ConsumerAgent(i, self, income_level, country_code)
            self.schedule.add(consumer_agent)

        # Create intermediary agents like logistics providers, banks. Scale the number and roles as needed.
        service_types = ['Logistics', 'Finance']
        num_intermediaries = 50  # Can be adjusted based on performance
        start_id = len(self.country_codes) + len(top_companies) + num_consumers
        for i in range(start_id, start_id + num_intermediaries):
            service_type = random.choice(service_types)
            intermediary_agent = IntermediaryAgent(i, self, service_type)
            self.schedule.add(intermediary_agent)

    # Helper functions for economic indicators and top companies

    def get_economic_indicators(self, country_code: str) -> Dict[str, Any]:
        """
        Retrieves economic indicators for a given country from processed data.
        This can include GDP, trade balance, trade percentage, etc., using the processed World Bank data.
        """
        df = self.processed_data.get('world_bank', pd.DataFrame())
        if df.empty:
            # Return default economic indicators if no data is available
            return {'GDP': 0, 'Trade_Percentage_GDP': 0, 'GDP_Growth': 0}
        country_data = df[df['Country'] == country_code]
        if country_data.empty:
            # Return default if no data for the given country is found
            return {'GDP': 0, 'Trade_Percentage_GDP': 0, 'GDP_Growth': 0}
        # Use the latest data row for economic indicators
        latest_year = country_data['Year'].max()
        latest_data = country_data[country_data['Year'] == latest_year].iloc[0].to_dict()
        # Placeholder: Calculate GDP_Growth based on previous year data if available
        previous_year = latest_year - 1
        previous_data = country_data[country_data['Year'] == previous_year]
        if not previous_data.empty:
            previous_gdp = previous_data.iloc[0].get('GDP', 0)
            current_gdp = latest_data.get('GDP', 0)
            gdp_growth = ((current_gdp - previous_gdp) / previous_gdp) * 100 if previous_gdp != 0 else 0
        else:
            gdp_growth = 0
        latest_data['GDP_Growth'] = gdp_growth
        return latest_data

    def get_top_companies(self) -> List[Dict[str, Any]]:
        """
        Retrieves top companies data, either from real data if available or uses placeholder data for the simulation.
        In a real scenario, this can be data-driven from processed data sources or an API.
        """
        # Placeholder list of companies
        companies = [
            {
                'name': 'CompanyA',
                'sector': 'Manufacturing',
                'country_code': 'USA',
                'financial_data': {'Production_Capacity': 500, 'Financial_Health': 90}
            },
            {
                'name': 'CompanyB',
                'sector': 'Technology',
                'country_code': 'CHN',
                'financial_data': {'Production_Capacity': 300, 'Financial_Health': 85}
            },
            {
                'name': 'CompanyC',
                'sector': 'Automotive',
                'country_code': 'DEU',
                'financial_data': {'Production_Capacity': 400, 'Financial_Health': 88}
            },
            {
                'name': 'CompanyD',
                'sector': 'Electronics',
                'country_code': 'JPN',
                'financial_data': {'Production_Capacity': 350, 'Financial_Health': 92}
            },
            # Additional companies can be added with more data if needed
        ]
        return companies

    def compute_trade_volume(self) -> float:
        """
        Computes the total trade volume in the model based on agent interactions.
        This can be extended to actually compute interactions between companies and government policies.
        For demonstration, we'll simulate it with random values.
        """
        # Placeholder for actual computation based on agent interactions
        # Suppose trade volume is influenced by the average tariffs and the number of trade alliances
        government_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgent)]
        if not government_agents:
            # If no government agents, return a default simulated value
            return random.uniform(1e6, 1e9)
        # Example: Compute some measure of trade volume from agents
        average_tariff = np.mean([agent.policies['Tariff_Rate'] for agent in government_agents])
        total_alliances = sum([len(agent.trade_alliances) for agent in government_agents])
        # We'll simulate a relationship, e.g., higher alliances and lower tariffs = higher trade volume
        trade_volume = (1e7 * len(government_agents)) * (1 + total_alliances / 100) * (1 - average_tariff / 100)
        # Add randomness
        trade_volume *= random.uniform(0.8, 1.2)
        return trade_volume

    def compute_demand(self, industry_sector: str) -> float:
        """
        Computes demand for a given industry sector. In a real scenario, this can be data-driven from processed data.
        For demonstration, we simulate a random demand influenced by sector performance.
        """
        # Placeholder for demand computation logic - can be extended with actual data
        base_demand = random.uniform(100, 1000)
        # Adjust demand based on sector. Suppose each sector gets some multiplier
        sector_multipliers = {
            'Manufacturing': 1.1,
            'Technology': 1.3,
            'Automotive': 1.2,
            'Electronics': 1.15
        }
        multiplier = sector_multipliers.get(industry_sector, 1.0)
        # Additional factors could include overall economic indicators
        # For simplicity, just multiplying base demand with sector multiplier
        demand = base_demand * multiplier
        return demand

    def step(self):
        """
        Advances the model by one step, allowing agents to act and computing trade volume and other metrics.
        """
        self.schedule.step()
        # Collect metrics
        self.datacollector.collect(self)
        trade_volume = self.compute_trade_volume()
        self.trade_volume_history.append(trade_volume)

    def reset(self):
        """
        Resets the model for a new simulation run, clearing any previous state and data.
        """
        self.schedule = RandomActivation(self)
        self.datacollector = DataCollector(model_reporters={"Trade_Volume": self.compute_trade_volume})
        self.create_agents()
        self.trade_volume_history = []

# =================================
# 4. GPT-4 Integration Module
# =================================

def gather_context_for_party(processed_data, country_code, party_name):
    """
    Gathers relevant data for a specific party within a country to create context for GPT-4.

    Args:
        processed_data (dict): The dictionary containing all processed data.
        country_code (str): The ISO alpha-3 country code.
        party_name (str): The name of the political party.

    Returns:
        str: A JSON-formatted string containing all relevant data.
    """
    country_political_data = processed_data['political_data'].get(country_code, {})
    political_parties = country_political_data.get('Political_Parties', pd.DataFrame())
    upcoming_elections = country_political_data.get('Upcoming_Elections', pd.DataFrame())
    current_policies = country_political_data.get('Current_Policies', pd.DataFrame())

    news_df = processed_data['news_data'].get(country_code, pd.DataFrame())
    social_media_df = processed_data['social_media_data'].get(country_code, pd.DataFrame())
    legislation_df = processed_data['legislation_data'].get(country_code, pd.DataFrame())

    # Extract party ideology
    party_info = political_parties[political_parties['Party'] == party_name]
    ideology = party_info['Ideology'].iloc[0] if not party_info.empty else 'Other'

    # Compile recent news and social media sentiments
    recent_news = news_df['Sentiment'].value_counts().to_dict() if not news_df.empty else {}
    recent_social_media = social_media_df['Sentiment'].value_counts().to_dict() if not social_media_df.empty else {}

    # Compile current policies
    policies = current_policies.to_dict(orient='records') if not current_policies.empty else []

    # Compile legislation
    legislation = legislation_df.to_dict(orient='records') if not legislation_df.empty else []

    # Compile upcoming elections
    elections = upcoming_elections.to_dict(orient='records') if not upcoming_elections.empty else []

    context = {
        'Country': country_code,
        'Party': party_name,
        'Ideology': ideology,
        'Recent_News_Sentiments': recent_news,
        'Recent_Social_Media_Sentiments': recent_social_media,
        'Current_Policies': policies,
        'Legislation': legislation,
        'Upcoming_Elections': elections
    }

    return json.dumps(context, indent=2)

def generate_decision_logic(context_json):
    """
    Generates decision logic for a political party using GPT-4 based on the provided context.

    Args:
        context_json (str): JSON-formatted string containing all relevant data for the party.

    Returns:
        str: The generated decision logic in CSV or code format.
    """
    prompt = f"""
    You are an expert political strategist. Based on the following context, generate a set of decision logics that the political party might undertake in the next 6 months. The output should be in CSV format with the following columns: Decision_ID, Description, Trigger_Event, Expected_Outcome, Implementation_Date.

    Context:
    {context_json}

    Ensure that the decisions are unique to the party's ideology and current political landscape. The decision logics should account for various scenarios and potential events that could occur during the 6-month forecasting period.
    """

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that generates decision logics for political parties."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,  # Adjust based on needs and token limits
            temperature=0.7,
            n=1,
            stop=None
        )
        decision_logic = response.choices[0].message['content']
        return decision_logic.strip()
    except Exception as e:
        logging.error(f"Error generating decision logic: {e}")
        return ""

def parse_decision_logic(decision_logic_str):
    """
    Parses the decision logic string into a Pandas DataFrame.

    Args:
        decision_logic_str (str): The decision logic in CSV format.

    Returns:
        pd.DataFrame: The parsed decision logic.
    """
    try:
        # Use StringIO to read the CSV string
        decision_df = pd.read_csv(io.StringIO(decision_logic_str))
        return decision_df
    except Exception as e:
        logging.error(f"Error parsing decision logic CSV: {e}")
        return pd.DataFrame()

def save_decision_logic(decision_df, country_code, party_name, output_dir='decision_logics'):
    """
    Saves the decision logic DataFrame as a CSV file.

    Args:
        decision_df (pd.DataFrame): The decision logic DataFrame.
        country_code (str): The ISO alpha-3 country code.
        party_name (str): The name of the political party.
        output_dir (str): Directory to save the CSV files.
    """
    if decision_df.empty:
        logging.warning(f"No decision logic to save for {party_name} in {country_code}.")
        return

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    filename = f"{country_code}_{party_name.replace(' ', '_')}_decision_logic.csv"
    filepath = os.path.join(output_dir, filename)
    decision_df.to_csv(filepath, index=False)
    logging.info(f"Decision logic saved to {filepath}.")

def load_decision_logics(decision_logic_dir='decision_logics'):
    """
    Loads all decision logic CSV files from the specified directory.

    Args:
        decision_logic_dir (str): Directory containing the decision logic CSV files.

    Returns:
        dict: A dictionary with keys as (country_code, party_name) and values as DataFrames.
    """
    decision_logics = {}
    for filename in os.listdir(decision_logic_dir):
        if filename.endswith('_decision_logic.csv'):
            parts = filename.split('_')
            country_code = parts[0]
            party_name = '_'.join(parts[1:-2])  # Adjust based on filename structure
            filepath = os.path.join(decision_logic_dir, filename)
            df = pd.read_csv(filepath)
            decision_logics[(country_code, party_name)] = df
            logging.info(f"Loaded decision logic for {party_name} in {country_code} from {filepath}.")
    return decision_logics

# =================================
# 5. Synthesis of Future News and Social Media Posts
# =================================

    def generate_future_news(context_json, forecast_period='6 months'):
        """
        Generates plausible future news articles that could impact the simulation.

        Args:
            context_json (str): JSON-formatted string containing all relevant data for the country/party.
            forecast_period (str): The forecasting period (e.g., '6 months').

        Returns:
            list: A list of generated news articles as strings.
        """
        prompt = f"""
        You are a journalist tasked with writing plausible future news articles that could impact the political and economic landscape of the following context within the next {forecast_period}.

        Context:
        {context_json}

        Generate 5 news articles with headlines and brief descriptions.
        """

        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a creative and insightful assistant that generates realistic future news articles based on provided context."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,  # Adjust based on needs
                temperature=0.7,
                n=1,
                stop=None
            )
            articles = response.choices[0].message['content']
            # Split articles assuming they are separated by newlines or numbering
            # This might require more sophisticated parsing based on GPT-4's output format
            article_list = [article.strip() for article in articles.split('\n') if article.strip()]
            return article_list
        except Exception as e:
            logging.error(f"Error generating future news: {e}")
            return []

    def generate_future_social_media(context_json, forecast_period='6 months'):
        """
        Generates plausible future social media posts that could impact the simulation.

        Args:
            context_json (str): JSON-formatted string containing all relevant data for the country/party.
            forecast_period (str): The forecasting period (e.g., '6 months').

        Returns:
            list: A list of generated social media posts as strings.
        """
        prompt = f"""
        You are a social media manager creating plausible future social media posts that reflect public sentiment and events within the next {forecast_period}.

        Context:
        {context_json}

        Generate 10 social media posts (e.g., tweets) that could influence or reflect the public's view on trade policies.
        """

        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a creative assistant that generates realistic future social media posts based on provided context."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,  # Adjust based on needs
                temperature=0.7,
                n=1,
                stop=None
            )
            posts = response.choices[0].message['content']
            # Split posts assuming they are separated by newlines or numbering
            # This might require more sophisticated parsing based on GPT-4's output format
            post_list = [post.strip() for post in posts.split('\n') if post.strip()]
            return post_list
        except Exception as e:
            logging.error(f"Error generating future social media posts: {e}")
            return []

    def integrate_future_events(model, country_code, party_name, future_news, future_posts):
        """
        Integrates future news and social media posts into the simulation model.

        Args:
            model (TradeModel): The simulation model instance.
            country_code (str): The ISO alpha-3 country code.
            party_name (str): The name of the political party.
            future_news (list): List of future news articles.
            future_posts (list): List of future social media posts.
        """
        # Example: Schedule future events based on the forecast period
        for news in future_news:
            # Assign an implementation date within the next 6 months
            implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))
            # Add to model's event queue or relevant data structures
            model.add_future_news(country_code, party_name, news, implementation_date)

        for post in future_posts:
            implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))
            model.add_future_social_media(country_code, party_name, post, implementation_date)

    # =================================
    # 6. Simulation Model Enhancements
    # =================================

    def generate_and_assign_decision_logics(processed_data, model, decision_logic_dir='decision_logics'):
        """
        Generates decision logics for all parties in all countries and assigns them to the simulation model.

        Args:
            processed_data (dict): The dictionary containing all processed data.
            model (TradeModel): The simulation model instance.
            decision_logic_dir (str): Directory to save the decision logic CSV files.
        """
        decision_logics = {}
        for country_code, political_info in processed_data['political_data'].items():
            parties_df = political_info.get('Political_Parties', pd.DataFrame())
            for _, party_row in parties_df.iterrows():
                party_name = party_row['Party']
                context_json = gather_context_for_party(processed_data, country_code, party_name)
                decision_logic_csv = generate_decision_logic(context_json)
                decision_logic_df = parse_decision_logic(decision_logic_csv)
                save_decision_logic(decision_logic_df, country_code, party_name, decision_logic_dir)
                decision_logics[(country_code, party_name)] = decision_logic_df

                # Optionally, generate future events
                future_news = generate_future_news(context_json)
                future_posts = generate_future_social_media(context_json)
                integrate_future_events(model, country_code, party_name, future_news, future_posts)

        return decision_logics

    # Enhance the TradeModel to handle future events
    class TradeModelEnhanced(TradeModel):
        """
        Enhanced TradeModel that can handle future news and social media events.
        """
        def __init__(self, processed_data: Dict[str, pd.DataFrame], political_data: Dict[str, Dict[str, pd.DataFrame]], news_data: Dict[str, pd.DataFrame], social_media_data: Dict[str, pd.DataFrame], legislation_data: Dict[str, pd.DataFrame]):
            super().__init__(processed_data, political_data, news_data, social_media_data, legislation_data)
            self.future_news = []  # List of tuples: (country_code, party_name, news, implementation_date)
            self.future_social_media = []  # List of tuples: (country_code, party_name, post, implementation_date)

        def add_future_news(self, country_code, party_name, news, implementation_date):
            """
            Adds a future news event to the model.

            Args:
                country_code (str): ISO alpha-3 country code.
                party_name (str): Name of the political party.
                news (str): News article content.
                implementation_date (pd.Timestamp): Date when the news becomes active.
            """
            self.future_news.append((country_code, party_name, news, implementation_date))
            logging.info(f"Scheduled future news for {party_name} in {country_code} on {implementation_date.date()}: {news[:60]}...")

        def add_future_social_media(self, country_code, party_name, post, implementation_date):
            """
            Adds a future social media post event to the model.

            Args:
                country_code (str): ISO alpha-3 country code.
                party_name (str): Name of the political party.
                post (str): Social media post content.
                implementation_date (pd.Timestamp): Date when the post becomes active.
            """
            self.future_social_media.append((country_code, party_name, post, implementation_date))
            logging.info(f"Scheduled future social media post for {party_name} in {country_code} on {implementation_date.date()}: {post[:60]}...")

        def process_future_events(self):
            """
            Processes future events that are due in the current simulation step.
            """
            current_date = pd.Timestamp.today() + pd.Timedelta(days=self.schedule.steps)
            # Process news
            due_news = [event for event in self.future_news if event[3] <= current_date]
            for event in due_news:
                country_code, party_name, news, _ = event
                # Integrate the news into the corresponding GovernmentAgent
                gov_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == country_code]
                for gov in gov_agents:
                    if gov.governing_party == party_name:
                        if gov.news_data is not None:
                            new_row = {'description': news}
                            gov.news_data = gov.news_data.append(new_row, ignore_index=True)
                            # Reprocess the news data to update sentiments and entities
                            gov.news_data = self.model.processed_data.get(f'{country_code}_news')
                        else:
                            # Create a new DataFrame if none exists
                            gov.news_data = pd.DataFrame([{'description': news}])
                            gov.news_data = self.model.processed_data.get(f'{country_code}_news')
                # Remove the processed event
                self.future_news.remove(event)
                logging.info(f"Processed future news for {party_name} in {country_code}: {news[:60]}...")

            # Process social media
            due_posts = [event for event in self.future_social_media if event[3] <= current_date]
            for event in due_posts:
                country_code, party_name, post, _ = event
                # Integrate the post into the corresponding GovernmentAgent
                gov_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == country_code]
                for gov in gov_agents:
                    if gov.governing_party == party_name:
                        if gov.tweets_data is not None:
                            new_row = {'text': post}
                            gov.tweets_data = gov.tweets_data.append(new_row, ignore_index=True)
                            # Reprocess the tweets data to update sentiments and entities
                            gov.tweets_data = self.model.processed_data.get(f'{country_code}_tweets')
                        else:
                            # Create a new DataFrame if none exists
                            gov.tweets_data = pd.DataFrame([{'text': post}])
                            gov.tweets_data = self.model.processed_data.get(f'{country_code}_tweets')
                # Remove the processed event
                self.future_social_media.remove(event)
                logging.info(f"Processed future social media post for {party_name} in {country_code}: {post[:60]}...")

        def step(self):
            """
            Advances the model by one step, allowing agents to act and processing due future events.
            """
            super().step()
            self.process_future_events()
            # Additional metrics or logging can be added here

    # =================================
    # 7. Additional Enhancements and Best Practices
    # =================================

    # The following section includes additional functions, error handling, and utilities to enhance the simulation's robustness and scalability.

    def setup_logging(log_file='simulation.log', log_level=logging.INFO):
        """
        Sets up the logging configuration.

        Args:
            log_file (str): The file to which logs will be written.
            log_level (int): The logging level.
        """
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s %(levelname)s:%(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )

    def load_environment_variables(env_file='.env'):
        """
        Loads environment variables from a .env file.

        Args:
            env_file (str): Path to the .env file.
        """
        from dotenv import load_dotenv
        load_dotenv(env_file)

    def initialize_simulation():
        """
        Initializes the entire simulation workflow:
        - Loads environment variables.
        - Sets up logging.
        - Acquires data.
        - Processes data.
        - Initializes the simulation model.
        - Generates and assigns decision logics.
        - Runs the simulation.
        """
        # Load environment variables
        load_environment_variables()

        # Set up logging
        setup_logging()

        # Data Acquisition
        data_acquisition = DataAcquisition()
        raw_data = data_acquisition.fetch_all_data()

        # Data Processing
        data_processing = DataProcessing(raw_data)
        data_processing.process_all_data()
        processed_data = data_processing.processed_data

        # Load decision logics (after generation)
        # For the first run, decision logics may not exist. They should be generated and saved.

        # Initialize the enhanced Trade Model
        model = TradeModelEnhanced(
            processed_data=processed_data,
            political_data=raw_data.get('political_data', {}),
            news_data=raw_data.get('news_data', {}),
            social_media_data=raw_data.get('social_media_data', {}),
            legislation_data=raw_data.get('legislation_data', {})
        )

        # Generate and assign decision logics
        decision_logics = generate_and_assign_decision_logics(processed_data, model)
        model.decision_logics = decision_logics

        # Save the updated model state or any other necessary components
        # This can include persisting the model to a file or database

        return model

    # =================================
    # 8. Example Unit Test for Decision Logic Generation
    # =================================

    import unittest

    class TestDecisionLogicGeneration(unittest.TestCase):
        def setUp(self):
            # Sample context
            self.sample_context = json.dumps({
                'Country': 'USA',
                'Party': 'Democratic Party',
                'Ideology': 'Liberal',
                'Recent_News_Sentiments': {'Positive': 10, 'Negative': 5},
                'Recent_Social_Media_Sentiments': {'Positive': 8, 'Negative': 7},
                'Current_Policies': [{'Policy': 'Tariff Rate Adjustment', 'Status': 'Increasing'}],
                'Legislation': [{'Bill_Name': 'Trade Cooperation Act', 'Status': 'active', 'Date_Introduced': '2023-03-22'}],
                'Upcoming_Elections': [{'Date_Upcoming_Election': '2024-11-15'}]
            })

        def test_generate_decision_logic(self):
            decision_logic = generate_decision_logic(self.sample_context)
            self.assertTrue(len(decision_logic) > 0, "Decision logic should not be empty.")
            decision_df = parse_decision_logic(decision_logic)
            self.assertFalse(decision_df.empty, "Parsed decision logic DataFrame should not be empty.")
            self.assertListEqual(list(decision_df.columns), ['Decision_ID', 'Description', 'Trigger_Event', 'Expected_Outcome', 'Implementation_Date'], "DataFrame columns do not match expected structure.")

        def test_integrate_future_events(self):
            # Initialize a mock model
            mock_model = TradeModelEnhanced({}, {}, {}, {}, {})
            future_news = ["Headline: USA increases tariffs on steel imports.", "Headline: New trade agreement signed with EU."]
            future_posts = ["@user1 The new tariffs on steel are hurting our economy!", "@user2 Excited about the new trade deal with the EU!"]
            integrate_future_events(mock_model, 'USA', 'Democratic Party', future_news, future_posts)
            self.assertEqual(len(mock_model.future_news), 2, "There should be two future news events scheduled.")
            self.assertEqual(len(mock_model.future_social_media), 2, "There should be two future social media posts scheduled.")

    if __name__ == '__main__':
        unittest.main()

import os
import sys
import json
import logging
import pandas as pd
import numpy as np
import random
import io
import re
from typing import Dict, Any, List, Tuple, Optional
from mesa import Agent, Model
from mesa.time import RandomActivation
from mesa.datacollection import DataCollector
from dotenv import load_dotenv
import openai
import unittest
from unittest.mock import patch, MagicMock
from datetime import datetime, timedelta

# =================================
# 1. Setup and Configuration
# =================================

# Ensure OpenAI API key is set
def load_openai_api_key(env_file='.env'):
    """
    Loads the OpenAI API key from the specified .env file.
    """
    load_dotenv(env_file)
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logging.error("OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.")
        sys.exit(1)
    openai.api_key = api_key

def setup_logging(log_file='simulation.log', log_level=logging.INFO):
    """
    Sets up the logging configuration.

    Args:
        log_file (str): The file to which logs will be written.
        log_level (int): The logging level.
    """
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s %(levelname)s:%(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )

# =================================
# 2. Data Acquisition and Processing Modules
# =================================

class DataAcquisition:
    """
    Handles data acquisition from various sources.
    Placeholder for actual implementation.
    """
    def fetch_all_data(self) -> Dict[str, Any]:
        """
        Fetches all necessary data from data sources.

        Returns:
            Dict[str, Any]: A dictionary containing all fetched data.
        """
        # Placeholder implementation
        # In a real scenario, implement data fetching from APIs, databases, or files.
        return {
            'world_bank': pd.DataFrame({
                'Country': ['USA', 'CAN', 'CHN', 'DEU'],
                'Year': [2023, 2023, 2023, 2023],
                'GDP': [21000000000000, 1700000000000, 14000000000000, 4000000000000],
                'Trade_Percentage_GDP': [25, 30, 35, 28],
                'GDP_Growth': [2.3, 1.8, 5.5, 1.5]
            }),
            'political_data': {
                'USA': {
                    'Political_Parties': pd.DataFrame({
                        'Party': ['Democratic Party', 'Republican Party'],
                        'Ideology': ['Liberal', 'Conservative']
                    }),
                    'Upcoming_Elections': pd.DataFrame({
                        'Date_Upcoming_Election': ['2024-11-05']
                    }),
                    'Current_Policies': pd.DataFrame({
                        'Policy': ['Tariff Rate Adjustment', 'Climate Change Initiative'],
                        'Status': ['Increasing', 'Active']
                    })
                },
                'CAN': {
                    'Political_Parties': pd.DataFrame({
                        'Party': ['Liberal Party', 'Conservative Party'],
                        'Ideology': ['Liberal', 'Conservative']
                    }),
                    'Upcoming_Elections': pd.DataFrame({
                        'Date_Upcoming_Election': ['2025-10-20']
                    }),
                    'Current_Policies': pd.DataFrame({
                        'Policy': ['Healthcare Reform', 'Climate Action Plan'],
                        'Status': ['Active', 'Active']
                    })
                },
                # Add more countries as needed
            },
            'news_data': {
                'USA': pd.DataFrame({
                    'description': [
                        'The economy is booming with low unemployment rates.',
                        'Public sentiment is turning negative due to rising inflation.',
                        'The Democratic Party launches a new green energy initiative.'
                    ],
                    'Sentiment': ['Positive', 'Negative', 'Positive'],
                    'Entities': [['economy', 'unemployment'], ['public sentiment', 'inflation'], ['Democratic Party', 'green energy']]
                }),
                'CAN': pd.DataFrame({
                    'description': [
                        'Canada\'s healthcare reforms have received widespread support.',
                        'Climate action plans are being accelerated by the Liberal Party.'
                    ],
                    'Sentiment': ['Positive', 'Positive'],
                    'Entities': [['healthcare reforms'], ['Climate Action Plan', 'Liberal Party']]
                }),
                # Add more countries as needed
            },
            'social_media_data': {
                'USA': pd.DataFrame({
                    'text': [
                        '@user1 Loving the new trade policies!',
                        '@user2 Concerned about the recent tariff increases.',
                        '@user3 Supportive of the green energy initiatives!'
                    ],
                    'Sentiment': ['Positive', 'Negative', 'Positive'],
                    'Entities': [['trade policies'], ['tariff increases'], ['green energy initiatives']]
                }),
                'CAN': pd.DataFrame({
                    'text': [
                        '@canadian1 Excited about the healthcare reforms!',
                        '@canadian2 Worried about the climate action delays.'
                    ],
                    'Sentiment': ['Positive', 'Negative'],
                    'Entities': [['healthcare reforms'], ['climate action delays']]
                }),
                # Add more countries as needed
            },
            'legislation_data': {
                'USA': pd.DataFrame({
                    'Bill_Name': ['Trade Cooperation Act', 'Green Energy Promotion Act'],
                    'Status': ['active', 'active'],
                    'Date_Introduced': ['2023-03-22', '2022-07-15']
                }),
                'CAN': pd.DataFrame({
                    'Bill_Name': ['Healthcare Improvement Act', 'Climate Action Enhancement Act'],
                    'Status': ['active', 'active'],
                    'Date_Introduced': ['2023-01-10', '2022-05-30']
                }),
                # Add more countries as needed
            },
            'Trade_Alliances': {
                'USA': ['EU', 'CAN'],
                'CAN': ['USA', 'MEX'],
                'CHN': ['RUS', 'IND'],
                'DEU': ['FRA', 'ITA'],
                # Add more countries as needed
            }
        }

class DataProcessing:
    """
    Processes raw data into structured formats for the simulation.
    """
    def __init__(self, raw_data: Dict[str, Any]):
        self.raw_data = raw_data
        self.processed_data = {}

    def process_all_data(self):
        """
        Processes all raw data into structured formats.
        """
        # Process World Bank data (already in DataFrame)
        self.processed_data['world_bank'] = self.raw_data.get('world_bank', pd.DataFrame())

        # Process political data
        self.processed_data['political_data'] = self.raw_data.get('political_data', {})

        # Process news data
        self.processed_data['news_data'] = self.raw_data.get('news_data', {})

        # Process social media data
        self.processed_data['social_media_data'] = self.raw_data.get('social_media_data', {})

        # Process legislation data
        self.processed_data['legislation_data'] = self.raw_data.get('legislation_data', {})

        # Process trade alliances
        self.processed_data['Trade_Alliances'] = self.raw_data.get('Trade_Alliances', {})

        # Add more processing steps as needed

# =================================
# 3. Agent-Based Modeling Module
# =================================

class GovernmentAgentEnhanced(Agent):
    """
    Enhanced Government agents with data-driven dispositions and a disposition-to-scenario-action matrix.
    """
    def __init__(self, unique_id, model, country_code, economic_indicators, political_parties: pd.DataFrame,
                 upcoming_elections: pd.DataFrame, current_policies: pd.DataFrame,
                 disposition_matrix: pd.DataFrame, modifier_matrix: pd.DataFrame):
        super().__init__(unique_id, model)
        self.country_code = country_code
        self.economic_indicators = economic_indicators  # e.g., GDP, trade balance, etc.
        self.policies = {'Tariff_Rate': 0, 'Subsidy_Rate': 0, 'Defense_Spending': 0}  # Initialize with default rates
        self.trade_alliances = set(model.processed_data['Trade_Alliances'].get(country_code, []))  # Countries this government has alliances with

        # Attributes
        self.political_parties = political_parties  # DataFrame with Party and Ideology
        self.upcoming_elections = upcoming_elections  # DataFrame with election details
        self.current_policies = current_policies  # DataFrame with current policy statuses

        self.governing_party = self.determine_governing_party()
        self.policy_shift_probability = self.determine_policy_shift_probability()

        # Attach news and social media data if available
        self.news_data = self.model.processed_data.get(f'{self.country_code}_news')
        self.tweets_data = self.model.processed_data.get(f'{self.country_code}_tweets')
        self.legislation_data = self.model.processed_data.get(f'{self.country_code}_legislation_data')

        # Disposition Matrix
        self.disposition_matrix = disposition_matrix  # DataFrame with Decision_ID, Action, Scenario, Probability(%)

        # Modifier Matrix
        self.modifier_matrix = modifier_matrix  # DataFrame with Relation_Type, Relation_Name, Scenario, Modifier_Value

    def determine_governing_party(self) -> str:
        """
        Determines the current governing party based on existing data.
        """
        if not self.political_parties.empty:
            return self.political_parties.iloc[0]['Party']
        return "Unknown"

    def determine_policy_shift_probability(self) -> float:
        """
        Determines the base probability of a policy shift based on factors like upcoming elections and economic indicators.
        """
        base_probability = 0.05  # Base 5% chance of policy shift each step
        # Increase probability if an upcoming election is near
        if not self.upcoming_elections.empty:
            election_date = pd.to_datetime(self.upcoming_elections.iloc[0]['Date_Upcoming_Election'])
            days_until_election = (election_date - pd.Timestamp.today()).days
            if days_until_election <= 180:  # 6 months
                base_probability += 0.10  # Additional 10% chance
        # Modify based on economic indicators
        gdp_growth = self.economic_indicators.get('GDP_Growth', 0)
        if gdp_growth < 0:
            base_probability += 0.05  # Additional 5% chance to shift policies during economic downturn
        return min(base_probability, 1.0)

    def calculate_public_sentiment(self) -> str:
        """
        Calculates the overall public sentiment based on news and social media data.
        Returns 'Positive', 'Negative', or 'Neutral'.
        """
        sentiments = []
        # Analyze news sentiment
        if self.news_data is not None and not self.news_data.empty:
            sentiments += self.news_data['Sentiment'].tolist()
        # Analyze social media sentiment
        if self.tweets_data is not None and not self.tweets_data.empty:
            sentiments += self.tweets_data['Sentiment'].tolist()
        if not sentiments:
            return 'Neutral'
        # Calculate the most common sentiment
        sentiment_counts = pd.Series(sentiments).value_counts()
        dominant_sentiment = sentiment_counts.idxmax()
        return dominant_sentiment

    def implement_policies_based_on_dispositions(self, current_scenarios: List[str]):
        """
        Implements policies based on the disposition-to-scenario-action probability matrix and applies modifiers.

        Args:
            current_scenarios (List[str]): List of current active scenarios in the simulation.
        """
        for scenario in current_scenarios:
            # Filter the disposition matrix for the current scenario
            scenario_decisions = self.disposition_matrix[self.disposition_matrix['Scenario'] == scenario]
            for _, decision in scenario_decisions.iterrows():
                base_probability = decision['Probability(%)'] / 100.0
                action = decision['Action']
                # Calculate modifier based on relationships
                modifier = self.calculate_modifier(action, scenario)
                adjusted_probability = base_probability + modifier
                adjusted_probability = max(min(adjusted_probability, 1.0), 0.0)  # Clamp between 0 and 1
                if random.random() < adjusted_probability:
                    self.execute_action(action)

    def calculate_modifier(self, action: str, scenario: str) -> float:
        """
        Calculates the modifier based on relationships (partners, alliances, industries) for the given action and scenario.

        Args:
            action (str): The action being considered.
            scenario (str): The current scenario.

        Returns:
            float: The modifier value to adjust the probability.
        """
        modifier = 0.0
        # Example: If the action involves forming alliances, and the country has strong alliances with certain partners
        if 'Form Trade Alliances' in action or 'Finalize Trade Agreements' in action:
            for alliance in self.trade_alliances:
                # Find modifiers for alliances
                mod_rows = self.modifier_matrix[
                    (self.modifier_matrix['Relation_Type'] == 'Alliance') &
                    (self.modifier_matrix['Relation_Name'] == alliance) &
                    (self.modifier_matrix['Scenario'] == scenario)
                ]
                if not mod_rows.empty:
                    modifier += mod_rows.iloc[0]['Modifier_Value']

        # Example: If the action involves specific industries
        if 'Tariffs' in action or 'Subsidies' in action:
            for industry in ['Automotive', 'Technology']:
                mod_rows = self.modifier_matrix[
                    (self.modifier_matrix['Relation_Type'] == 'Industry') &
                    (self.modifier_matrix['Relation_Name'] == industry) &
                    (self.modifier_matrix['Scenario'] == scenario)
                ]
                if not mod_rows.empty:
                    modifier += mod_rows.iloc[0]['Modifier_Value']

        return modifier

    def execute_action(self, action: str):
        """
        Executes the specified action by updating policies or performing actions.

        Args:
            action (str): The action to be executed.
        """
        action = action.lower()
        if 'increase tariffs' in action:
            old_tariff = self.policies.get('Tariff_Rate', 0)
            self.policies['Tariff_Rate'] = min(old_tariff + 1, 30)  # Example increment
            logging.info(f'Government {self.country_code}: Increased tariff rate from {old_tariff}% to {self.policies["Tariff_Rate"]}% as per action "{action}".')
        elif 'decrease tariffs' in action:
            old_tariff = self.policies.get('Tariff_Rate', 0)
            self.policies['Tariff_Rate'] = max(old_tariff - 1, 0)  # Example decrement
            logging.info(f'Government {self.country_code}: Decreased tariff rate from {old_tariff}% to {self.policies["Tariff_Rate"]}% as per action "{action}".')
        elif 'form trade alliances' in action:
            # Example: Randomly choose a partner country from defined list
            potential_partners = [code for code in self.model.country_codes if code != self.country_code and code not in self.trade_alliances]
            if potential_partners:
                partner = random.choice(potential_partners)
                self.form_trade_alliance(partner)
        elif 'finalize trade agreements' in action:
            # Implement trade agreements logic
            logging.info(f'Government {self.country_code}: Finalizing trade agreements as per action "{action}".')
        elif 'implement subsidies' in action:
            old_subsidy = self.policies.get('Subsidy_Rate', 0)
            self.policies['Subsidy_Rate'] = old_subsidy + 1  # Example increment
            logging.info(f'Government {self.country_code}: Implemented subsidies increasing from {old_subsidy}% to {self.policies["Subsidy_Rate"]}% as per action "{action}".')
        elif 'reduce subsidies' in action:
            old_subsidy = self.policies.get('Subsidy_Rate', 0)
            self.policies['Subsidy_Rate'] = max(old_subsidy - 1, 0)  # Example decrement
            logging.info(f'Government {self.country_code}: Reduced subsidies from {old_subsidy}% to {self.policies["Subsidy_Rate"]}% as per action "{action}".')
        elif 'implement climate change policies' in action:
            logging.info(f'Government {self.country_code}: Implementing climate change policies as per action "{action}".')
        elif 'enhance disaster response mechanisms' in action:
            logging.info(f'Government {self.country_code}: Enhancing disaster response mechanisms as per action "{action}".')
        elif 'promote renewable energy initiatives' in action:
            logging.info(f'Government {self.country_code}: Promoting renewable energy initiatives as per action "{action}".')
        elif 'launch election campaign' in action:
            logging.info(f'Government {self.country_code}: Launching election campaign as per action "{action}".')
        elif 'initiate policy reforms' in action:
            logging.info(f'Government {self.country_code}: Initiating policy reforms as per action "{action}".')
        elif 'handle political scandals' in action:
            logging.info(f'Government {self.country_code}: Handling political scandals as per action "{action}".')
        elif 'resign leadership' in action:
            logging.info(f'Government {self.country_code}: Resigning leadership as per action "{action}".')
        elif 'appoint new leaders' in action:
            logging.info(f'Government {self.country_code}: Appointing new leaders as per action "{action}".')
        elif 'address public protests' in action:
            logging.info(f'Government {self.country_code}: Addressing public protests as per action "{action}".')
        elif 'launch public awareness campaigns' in action:
            logging.info(f'Government {self.country_code}: Launching public awareness campaigns as per action "{action}".')
        elif 'implement social welfare programs' in action:
            logging.info(f'Government {self.country_code}: Implementing social welfare programs as per action "{action}".')
        elif 'increase defense spending' in action:
            old_defense = self.policies.get('Defense_Spending', 0)
            self.policies['Defense_Spending'] = old_defense + 1  # Example increment
            logging.info(f'Government {self.country_code}: Increased defense spending from {old_defense} to {self.policies["Defense_Spending"]} as per action "{action}".')
        elif 'decrease defense spending' in action:
            old_defense = self.policies.get('Defense_Spending', 0)
            self.policies['Defense_Spending'] = max(old_defense - 1, 0)  # Example decrement
            logging.info(f'Government {self.country_code}: Decreased defense spending from {old_defense} to {self.policies["Defense_Spending"]} as per action "{action}".')
        elif 'strengthen security measures' in action:
            logging.info(f'Government {self.country_code}: Strengthening security measures as per action "{action}".')
        # Add more actions as needed

    class CompanyAgent(Agent):
        """
        Company agents represent businesses in different industry sectors, with advanced strategies such as investing in industries, adjusting production, and supply chain management.
        """
        def __init__(self, unique_id, model, name, industry_sector, country_code, financial_data):
            super().__init__(unique_id, model)
            self.name = name
            self.industry_sector = industry_sector
            self.country_code = country_code
            self.financial_data = financial_data
            self.production_capacity = financial_data.get('Production_Capacity', 100)
            self.supply_chain = []  # List of supplier company agents or countries where inputs are sourced from
            self.financial_health = financial_data.get('Financial_Health', 100)
            self.demand_preferences = {'domestic': 0.5, 'imported': 0.5}  # Simplified preference for domestic vs. imported goods if relevant

        def make_sourcing_decision(self):
            """
            Adjust sourcing decisions based on government policies, such as tariffs, and market conditions.
            This simulates supply chain shifts in response to changing trade policies or other economic conditions.
            """
            government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == self.country_code]
            if government_agents:
                tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)
                if tariff_rate > 10:
                    # If tariffs are high, the company might increase local sourcing to avoid import costs.
                    self.demand_preferences['domestic'] += 0.1
                    self.demand_preferences['imported'] = max(self.demand_preferences['imported'] - 0.1, 0)
                    logging.info(f'Company {self.name} in {self.country_code}: Adjusting sourcing towards domestic suppliers due to high tariffs.')
                else:
                    # If tariffs are low, the company might maintain or even increase foreign sourcing for cheaper imports.
                    self.demand_preferences['imported'] += 0.05
                    self.demand_preferences['domestic'] = max(self.demand_preferences['domestic'] - 0.05, 0)
                    logging.info(f'Company {self.name} in {self.country_code}: Adjusting sourcing towards imported goods due to low tariffs.')

        def adjust_production(self):
            """
            Adjusts production capacity based on demand, supply chain status, and financial health.
            """
            demand = self.model.compute_demand(self.industry_sector)
            old_capacity = self.production_capacity
            if demand > self.production_capacity:
                self.production_capacity += 10  # Increase capacity if demand is high.
                logging.info(f'Company {self.name} in {self.country_code}: Increased production capacity from {old_capacity} to {self.production_capacity} due to rising demand.')
            elif demand < self.production_capacity * 0.8:
                self.production_capacity = max(self.production_capacity - 10, 50)  # Decrease capacity if demand is low.
                logging.info(f'Company {self.name} in {self.country_code}: Decreased production capacity from {old_capacity} to {self.production_capacity} due to falling demand.')

        def invest_in_industry(self, investment_amount: float):
            """
            Invest in industry to improve the company's financial health or production capacity.
            This simulates decisions like expanding factories, training employees, or R&D spending.
            """
            old_financial_health = self.financial_health
            old_capacity = self.production_capacity
            self.financial_health += investment_amount * 0.1  # Example investment effect on financial health.
            self.production_capacity += investment_amount * 0.05  # Example effect on production capacity.
            logging.info(f'Company {self.name} in {self.country_code}: Invested {investment_amount}, financial health improved from {old_financial_health} to {self.financial_health} and capacity from {old_capacity} to {self.production_capacity}.')

        def step(self):
            """
            Defines the agent's behavior at each step, including sourcing decisions, production adjustments, and potential investments.
            """
            self.make_sourcing_decision()
            self.adjust_production()
            # Example: Random investment decisions to simulate growth or adaptation strategies.
            if random.random() < 0.02:  # 2% chance each step to invest.
                investment = random.uniform(1000, 5000)
                self.invest_in_industry(investment)

class ConsumerAgent(Agent):
    """
    Consumer agents represent individual or aggregate consumer behavior with advanced preferences and responses to policies.
    """
    def __init__(self, unique_id, model, income_level, country_code):
        super().__init__(unique_id, model)
        self.income_level = income_level  # 'Low', 'Medium', 'High' income category for simplified model.
        self.country_code = country_code
        self.demand_preferences = self.generate_preferences()  # Preferences for goods based on income level and policy.

    def generate_preferences(self):
        """
        Generates demand preferences based on income level. Higher income consumers prefer more luxury goods.
        """
        if self.income_level == 'Low':
            return {'essential_goods': 0.7, 'luxury_goods': 0.3}
        elif self.income_level == 'Medium':
            return {'essential_goods': 0.5, 'luxury_goods': 0.5}
        else:  # High income consumers prefer more luxury goods.
            return {'essential_goods': 0.3, 'luxury_goods': 0.7}

    def adjust_consumption(self):
        """
        Adjusts consumption behavior based on changing prices, income, and government policies like tariffs.
        If tariffs are high on imported goods, consumers might buy more domestic goods.
        """
        government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == self.country_code]
        if government_agents:
            tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)
            if tariff_rate > 10:
                # If tariffs on imported goods are high, reduce luxury goods consumption (assuming many are imported)
                old_luxury_pref = self.demand_preferences['luxury_goods']
                self.demand_preferences['luxury_goods'] *= 0.9  # reduce consumption of expensive imported luxuries.
                logging.info(f'Consumer in {self.country_code}: Decreasing consumption of imported luxury goods from preference {old_luxury_pref} to {self.demand_preferences["luxury_goods"]} due to high tariffs.')
            else:
                # If tariffs are low, possibly maintain or increase luxury goods consumption.
                old_luxury_pref = self.demand_preferences['luxury_goods']
                self.demand_preferences['luxury_goods'] *= 1.05
                logging.info(f'Consumer in {self.country_code}: Slightly increasing consumption of luxury goods from preference {old_luxury_pref} to {self.demand_preferences["luxury_goods"]} due to low tariffs.')

    def step(self):
        """
        Defines the agent's behavior at each step, adjusting consumption preferences if needed.
        """
        self.adjust_consumption()

class IntermediaryAgent(Agent):
    """
    Intermediary agents represent logistics providers, financial institutions, or other services influencing trade.
    They can provide services such as improving supply chain efficiency or offering financing solutions.
    """
    def __init__(self, unique_id, model, service_type):
        super().__init__(unique_id, model)
        self.service_type = service_type  # 'Logistics', 'Finance', etc.

    def provide_service(self):
        """
        Provides services to other agents based on demand, possibly affecting overall trade efficiency and costs.
        """
        # Placeholder service provision logic. For example, logistics agents could reduce shipping times, finance agents could reduce financial constraints.
        pass

    def step(self):
        """
        Defines the agent's behavior at each step. This can include adjusting fees or expanding services as needed.
        """
        self.provide_service()

class TradeModelEnhanced(Model):
    """
    Enhanced TradeModel that incorporates data-driven dispositions and modifier matrices.
    """
    def __init__(self, processed_data: Dict[str, pd.DataFrame], political_data: Dict[str, Dict[str, pd.DataFrame]], news_data: Dict[str, pd.DataFrame], social_media_data: Dict[str, pd.DataFrame], legislation_data: Dict[str, pd.DataFrame]):
        super().__init__()
        self.schedule = RandomActivation(self)
        self.processed_data = processed_data  # Processed data from DataProcessing module
        self.political_data = political_data  # Political data fetched separately
        self.news_data = news_data  # News data fetched separately
        self.social_media_data = social_media_data  # Social media data fetched separately
        self.legislation_data = legislation_data  # Legislation data fetched separately
        self.datacollector = DataCollector(model_reporters={"Trade_Volume": self.compute_trade_volume})

        # Assuming processed_data includes data from the World Bank to obtain a list of countries
        country_codes = self.processed_data.get('world_bank', pd.DataFrame()).get('Country', []).unique()
        if not isinstance(country_codes, np.ndarray):
            country_codes = []
        self.country_codes = country_codes
        self.create_agents()
        self.running = True
        self.trade_volume_history = []

        # Placeholder for scenario-action tables
        self.scenario_action_tables = {}  # To store Disposition and Modifier matrices per country and party

        # Placeholder for future events
        self.future_news = []  # List of tuples: (country_code, party_name, news, implementation_date)
        self.future_social_media = []  # List of tuples: (country_code, party_name, post, implementation_date)

        # Parameters for simulation branching
        self.max_branching_depth = 3  # Adjust based on desired depth
        self.current_branching_depth = 0  # Initialize current depth

    def create_agents(self):
        """
        Creates agents for the model, including government, company, consumer, and intermediary agents,
        leveraging processed data to initialize their states.
        """
        # Create government agents for all countries present in processed data or default set if data is unavailable.
        for i, country_code in enumerate(self.country_codes):
            economic_indicators = self.get_economic_indicators(country_code)
            political_info = self.political_data.get(country_code, {})
            political_parties = political_info.get('Political_Parties')
            upcoming_elections = political_info.get('Upcoming_Elections')
            current_policies = political_info.get('Current_Policies')
            news = self.news_data.get(country_code)
            tweets = self.social_media_data.get(country_code)
            legislation = self.legislation_data.get(country_code)

            if political_parties is None or political_parties.empty:
                logging.warning(f"No political parties data for {country_code}. Skipping government agent creation.")
                continue

            governing_party = political_parties.iloc[0]['Party']
            context_json = gather_context_for_party(self.processed_data, country_code, governing_party)

            # Generate scenario-action matrices using GPT-4
            disposition_df, modifier_df = generate_disposition_and_modifier_matrices(context_json, self.get_scenarios())

            if disposition_df.empty or modifier_df.empty:
                logging.warning(f"Disposition or Modifier matrix for {governing_party} in {country_code} is empty. Skipping agent creation.")
                continue

            government_agent = GovernmentAgentEnhanced(
                unique_id=i,
                model=self,
                country_code=country_code,
                economic_indicators=economic_indicators,
                political_parties=political_parties,
                upcoming_elections=upcoming_elections,
                current_policies=current_policies,
                disposition_matrix=disposition_df,
                modifier_matrix=modifier_df
            )
            # Attach news and social media data to the agent
            government_agent.news_data = news
            government_agent.tweets_data = tweets
            # Attach legislation data if needed
            government_agent.legislation_data = legislation
            self.schedule.add(government_agent)

        # Create company agents using hypothetical data or processed data. This can be data-driven if real data is available.
        top_companies = self.get_top_companies()
        for i, company in enumerate(top_companies, start=len(self.country_codes)):
            company_agent = CompanyAgent(i, self, company['name'], company['sector'], company['country_code'], company['financial_data'])
            self.schedule.add(company_agent)

        # Create consumer agents representing populations. This can be scaled by population data, if available.
        num_consumers = 100  # Adjust based on complexity and performance considerations
        for i in range(len(self.country_codes) + len(top_companies), len(self.country_codes) + len(top_companies) + num_consumers):
            income_level = random.choice(['Low', 'Medium', 'High'])
            country_code = random.choice(self.country_codes) if len(self.country_codes) > 0 else 'USA'
            consumer_agent = ConsumerAgent(i, self, income_level, country_code)
            self.schedule.add(consumer_agent)

        # Create intermediary agents like logistics providers, banks. Scale the number and roles as needed.
        service_types = ['Logistics', 'Finance']
        num_intermediaries = 50  # Can be adjusted based on performance
        start_id = len(self.country_codes) + len(top_companies) + num_consumers
        for i in range(start_id, start_id + num_intermediaries):
            service_type = random.choice(service_types)
            intermediary_agent = IntermediaryAgent(i, self, service_type)
            self.schedule.add(intermediary_agent)

    def get_economic_indicators(self, country_code: str) -> Dict[str, Any]:
        """
        Retrieves economic indicators for a given country from processed data.
        This can include GDP, trade balance, trade percentage, etc., using the processed World Bank data.
        """
        df = self.processed_data.get('world_bank', pd.DataFrame())
        if df.empty:
            # Return default economic indicators if no data is available
            return {'GDP': 0, 'Trade_Percentage_GDP': 0, 'GDP_Growth': 0}
        country_data = df[df['Country'] == country_code]
        if country_data.empty:
            # Return default if no data for the given country is found
            return {'GDP': 0, 'Trade_Percentage_GDP': 0, 'GDP_Growth': 0}
        # Use the latest data row for economic indicators
        latest_year = country_data['Year'].max()
        latest_data = country_data[country_data['Year'] == latest_year].iloc[0].to_dict()
        # Calculate GDP_Growth based on previous year data if available
        previous_year = latest_year - 1
        previous_data = country_data[country_data['Year'] == previous_year]
        if not previous_data.empty:
            previous_gdp = previous_data.iloc[0].get('GDP', 0)
            current_gdp = latest_data.get('GDP', 0)
            gdp_growth = ((current_gdp - previous_gdp) / previous_gdp) * 100 if previous_gdp != 0 else 0
        else:
            gdp_growth = 0
        latest_data['GDP_Growth'] = gdp_growth
        return latest_data

    def get_top_companies(self) -> List[Dict[str, Any]]:
        """
        Retrieves top companies data, either from real data if available or uses placeholder data for the simulation.
        In a real scenario, this can be data-driven from processed data sources or an API.
        """
        # Placeholder list of companies
        companies = [
            {
                'name': 'CompanyA',
                'sector': 'Manufacturing',
                'country_code': 'USA',
                'financial_data': {'Production_Capacity': 500, 'Financial_Health': 90}
            },
            {
                'name': 'CompanyB',
                'sector': 'Technology',
                'country_code': 'CHN',
                'financial_data': {'Production_Capacity': 300, 'Financial_Health': 85}
            },
            {
                'name': 'CompanyC',
                'sector': 'Automotive',
                'country_code': 'DEU',
                'financial_data': {'Production_Capacity': 400, 'Financial_Health': 88}
            },
            {
                'name': 'CompanyD',
                'sector': 'Electronics',
                'country_code': 'JPN',
                'financial_data': {'Production_Capacity': 350, 'Financial_Health': 92}
            },
            # Additional companies can be added with more data if needed
        ]
        return companies

    def compute_trade_volume(self) -> float:
        """
        Computes the total trade volume in the model based on agent interactions.
        This can be extended to actually compute interactions between companies and government policies.
        For demonstration, we'll simulate it with random values.
        """
        # Placeholder for actual computation based on agent interactions
        # Suppose trade volume is influenced by the average tariffs and the number of trade alliances
        government_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgentEnhanced)]
        if not government_agents:
            # If no government agents, return a default simulated value
            return random.uniform(1e6, 1e9)
        # Example: Compute some measure of trade volume from agents
        average_tariff = np.mean([agent.policies['Tariff_Rate'] for agent in government_agents])
        total_alliances = sum([len(agent.trade_alliances) for agent in government_agents])
        # We'll simulate a relationship, e.g., higher alliances and lower tariffs = higher trade volume
        trade_volume = (1e7 * len(government_agents)) * (1 + total_alliances / 100) * (1 - average_tariff / 100)
        # Add randomness
        trade_volume *= random.uniform(0.8, 1.2)
        return trade_volume

    def compute_demand(self, industry_sector: str) -> float:
        """
        Computes demand for a given industry sector. In a real scenario, this can be data-driven from processed data.
        For demonstration, we simulate a random demand influenced by sector performance.
        """
        # Placeholder for demand computation logic - can be extended with actual data
        base_demand = random.uniform(100, 1000)
        # Adjust demand based on sector. Suppose each sector gets some multiplier
        sector_multipliers = {
            'Manufacturing': 1.1,
            'Technology': 1.3,
            'Automotive': 1.2,
            'Electronics': 1.15
        }
        multiplier = sector_multipliers.get(industry_sector, 1.0)
        # Additional factors could include overall economic indicators
        # For simplicity, just multiplying base demand with sector multiplier
        demand = base_demand * multiplier
        return demand

    def get_scenarios(self) -> List[str]:
        """
        Retrieves the list of defined scenarios from the processed data or uses a default set.
        """
        # Placeholder for actual scenario retrieval logic
        return [
            'Recession',
            'Economic Boom',
            'Inflation Surge',
            'Trade Deficit Increase',
            'Upcoming Elections',
            'Political Scandals',
            'Leadership Changes',
            'Public Protests',
            'Shifts in Public Sentiment',
            'Natural Disasters',
            'Climate Change Policies'
        ]

    def step(self):
        """
        Advances the model by one step, allowing agents to act and processing due future events.
        """
        self.schedule.step()
        # Collect metrics
        self.datacollector.collect(self)
        trade_volume = self.compute_trade_volume()
        self.trade_volume_history.append(trade_volume)
        # Process future events
        self.process_future_events()
        # Additional metrics or logging can be added here

    def reset(self):
        """
        Resets the model for a new simulation run, clearing any previous state and data.
        """
        self.schedule = RandomActivation(self)
        self.datacollector = DataCollector(model_reporters={"Trade_Volume": self.compute_trade_volume})
        self.create_agents()
        self.trade_volume_history = []

    def add_future_news(self, country_code, party_name, news, implementation_date):
        """
        Adds a future news event to the model.

        Args:
            country_code (str): ISO alpha-3 country code.
            party_name (str): Name of the political party.
            news (str): News article content.
            implementation_date (pd.Timestamp): Date when the news becomes active.
        """
        self.future_news.append((country_code, party_name, news, implementation_date))
        logging.info(f"Scheduled future news for {party_name} in {country_code} on {implementation_date.date()}: {news[:60]}...")

    def add_future_social_media(self, country_code, party_name, post, implementation_date):
        """
        Adds a future social media post event to the model.

        Args:
            country_code (str): ISO alpha-3 country code.
            party_name (str): Name of the political party.
            post (str): Social media post content.
            implementation_date (pd.Timestamp): Date when the post becomes active.
        """
        self.future_social_media.append((country_code, party_name, post, implementation_date))
        logging.info(f"Scheduled future social media post for {party_name} in {country_code} on {implementation_date.date()}: {post[:60]}...")

    def process_future_events(self):
        """
        Processes future events that are due in the current simulation step.
        """
        current_date = pd.Timestamp.today() + pd.Timedelta(days=self.schedule.steps)
        # Process news
        due_news = [event for event in self.future_news if event[3] <= current_date]
        for event in due_news:
            country_code, party_name, news, _ = event
            # Integrate the news into the corresponding GovernmentAgent
            gov_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == country_code]
            for gov in gov_agents:
                if gov.governing_party == party_name:
                    if gov.news_data is not None:
                        new_row = {'description': news, 'Sentiment': 'Neutral', 'Entities': []}  # Sentiment and Entities can be processed further
                        gov.news_data = gov.news_data.append(new_row, ignore_index=True)
                    else:
                        # Create a new DataFrame if none exists
                        gov.news_data = pd.DataFrame([{'description': news, 'Sentiment': 'Neutral', 'Entities': []}])
            # Remove the processed event
            self.future_news.remove(event)
            logging.info(f"Processed future news for {party_name} in {country_code}: {news[:60]}...")

        # Process social media
        due_posts = [event for event in self.future_social_media if event[3] <= current_date]
        for event in due_posts:
            country_code, party_name, post, _ = event
            # Integrate the post into the corresponding GovernmentAgent
            gov_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == country_code]
            for gov in gov_agents:
                if gov.governing_party == party_name:
                    if gov.tweets_data is not None:
                        new_row = {'text': post, 'Sentiment': 'Neutral', 'Entities': []}  # Sentiment and Entities can be processed further
                        gov.tweets_data = gov.tweets_data.append(new_row, ignore_index=True)
                    else:
                        # Create a new DataFrame if none exists
                        gov.tweets_data = pd.DataFrame([{'text': post, 'Sentiment': 'Neutral', 'Entities': []}])
            # Remove the processed event
            self.future_social_media.remove(event)
            logging.info(f"Processed future social media post for {party_name} in {country_code}: {post[:60]}...")

    def run_batch(self, batch_size: int, branching_depth: int = 1):
        """
        Runs a batch of simulation steps and generates future events based on outcomes.

        Args:
            batch_size (int): Number of simulation steps to run in this batch.
            branching_depth (int): Current depth of branching.
        """
        for _ in range(batch_size):
            self.step()

        # Analyze outcomes
        trade_volumes = self.trade_volume_history[-batch_size:]
        mean_volume = np.mean(trade_volumes)
        median_volume = np.median(trade_volumes)
        std_volume = np.std(trade_volumes)

        # Identify 1-sigma away outcomes
        lower_bound = mean_volume - std_volume
        upper_bound = mean_volume + std_volume
        notable_outliers = [vol for vol in trade_volumes if vol < (mean_volume - 2 * std_volume) or vol > (mean_volume + 2 * std_volume)]

        # Generate events based on mean, median, and outliers
        events = []
        if mean_volume:
            events.append(('Mean Outcome', mean_volume))
        if median_volume:
            events.append(('Median Outcome', median_volume))
        if std_volume:
            events.append(('1 Sigma Below', lower_bound))
            events.append(('1 Sigma Above', upper_bound))
        if notable_outliers:
            events.append(('Notable Outliers', notable_outliers))

        for event_type, outcome in events:
            context_json = self.construct_event_context(event_type, outcome)
            future_news = generate_future_news(context_json)
            future_posts = generate_future_social_media(context_json)
            # Branching based on outcome
            if branching_depth < self.max_branching_depth:
                for news in future_news:
                    # Assign an implementation date within the next 6 months
                    implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))
                    # Add to model's event queue
                    self.add_future_news('USA', 'Democratic Party', news, implementation_date)
                for post in future_posts:
                    implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))
                    self.add_future_social_media('USA', 'Democratic Party', post, implementation_date)
                logging.info(f"Generated and integrated future events for event type '{event_type}' at branching depth {branching_depth}.")
            else:
                logging.info(f"Maximum branching depth reached. No further events generated for event type '{event_type}'.")

    def construct_event_context(self, event_type: str, outcome: Any) -> str:
        """
        Constructs a context JSON string based on the event type and outcome.

        Args:
            event_type (str): The type of event (e.g., 'Mean Outcome').
            outcome (Any): The outcome associated with the event.

        Returns:
            str: A JSON-formatted string containing the context.
        """
        context = {
            'Event_Type': event_type,
            'Outcome': outcome,
            'Country': 'USA',  # Assuming USA for simplicity; adjust as needed
            'Party': 'Democratic Party'  # Assuming Democratic Party; adjust as needed
        }
        return json.dumps(context, indent=2)

    # =================================
    # 4. GPT-4 Integration Module
    # =================================

    def gather_context_for_party(processed_data, country_code, party_name):
        """
        Gathers relevant data for a specific party within a country to create context for GPT-4.

        Args:
            processed_data (dict): The dictionary containing all processed data.
            country_code (str): The ISO alpha-3 country code.
            party_name (str): The name of the political party.

        Returns:
            str: A JSON-formatted string containing all relevant data.
        """
        country_political_data = processed_data['political_data'].get(country_code, {})
        political_parties = country_political_data.get('Political_Parties', pd.DataFrame())
        upcoming_elections = country_political_data.get('Upcoming_Elections', pd.DataFrame())
        current_policies = country_political_data.get('Current_Policies', pd.DataFrame())

        news_df = processed_data['news_data'].get(country_code, pd.DataFrame())
        social_media_df = processed_data['social_media_data'].get(country_code, pd.DataFrame())
        legislation_df = processed_data['legislation_data'].get(country_code, pd.DataFrame())

        # Extract party ideology
        party_info = political_parties[political_parties['Party'] == party_name]
        ideology = party_info['Ideology'].iloc[0] if not party_info.empty else 'Other'

        # Compile recent news and social media sentiments
        recent_news = news_df['Sentiment'].value_counts().to_dict() if not news_df.empty else {}
        recent_social_media = social_media_df['Sentiment'].value_counts().to_dict() if not social_media_df.empty else {}

        # Compile current policies
        policies = current_policies.to_dict(orient='records') if not current_policies.empty else []

        # Compile legislation
        legislation = legislation_df.to_dict(orient='records') if not legislation_df.empty else []

        # Compile upcoming elections
        elections = upcoming_elections.to_dict(orient='records') if not upcoming_elections.empty else []

        context = {
            'Country': country_code,
            'Party': party_name,
            'Ideology': ideology,
            'Recent_News_Sentiments': recent_news,
            'Recent_Social_Media_Sentiments': recent_social_media,
            'Current_Policies': policies,
            'Legislation': legislation,
            'Upcoming_Elections': elections
        }

        return json.dumps(context, indent=2)

    def generate_disposition_and_modifier_matrices(context_json: str, scenarios: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Generates both the disposition matrix and the modifier matrix using GPT-4.

        Args:
            context_json (str): JSON-formatted string containing all relevant data for the party/government.
            scenarios (List[str]): List of defined scenarios.

        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: Disposition Matrix and Modifier Matrix DataFrames.
        """
        prompt = f"""
        You are an expert political strategist tasked with generating two matrices for a political party based on the provided context. The matrices should adhere strictly to the defined lists below.

        ### Comprehensive Lists

        **1. Scenarios**

        - **Economic Scenarios:**
          - Recession
          - Economic Boom
          - Inflation Surge
          - Trade Deficit Increase

        - **Political Scenarios:**
          - Upcoming Elections
          - Political Scandals
          - Leadership Changes

        - **Social Scenarios:**
          - Public Protests
          - Shifts in Public Sentiment

        - **Environmental Scenarios:**
          - Natural Disasters
          - Climate Change Policies

        **2. Actions**

        - **Economic Actions:**
          - Increase Tariffs
          - Decrease Tariffs
          - Implement Subsidies
          - Reduce Subsidies
          - Increase Taxes
          - Decrease Taxes
          - Stimulate Economic Growth
          - Implement Austerity Measures

        - **Political Actions:**
          - Launch Election Campaign
          - Initiate Policy Reforms
          - Handle Political Scandals
          - Resign Leadership
          - Appoint New Leaders

        - **Social Actions:**
          - Address Public Protests
          - Launch Public Awareness Campaigns
          - Implement Social Welfare Programs

        - **Environmental Actions:**
          - Implement Climate Change Policies
          - Enhance Disaster Response Mechanisms
          - Promote Renewable Energy Initiatives

        - **Trade Actions:**
          - Form Trade Alliances
          - Finalize Trade Agreements
          - Impose Trade Sanctions
          - Lift Trade Sanctions

        - **Defense and Security Actions:**
          - Increase Defense Spending
          - Decrease Defense Spending
          - Strengthen Security Measures

        **3. Parties/Countries/Industries/Companies**

        - **Countries (ISO Alpha-3 Codes):**
          - USA (United States of America)
          - CAN (Canada)
          - CHN (China)
          - DEU (Germany)
          - FRA (France)
          - JPN (Japan)
          - GBR (United Kingdom)
          - ITA (Italy)
          - RUS (Russia)
          - IND (India)
          - BRA (Brazil)
          - AUS (Australia)
          - KOR (South Korea)
          - ESP (Spain)
          - MEX (Mexico)

        - **Political Parties:**
          - **USA:**
            - Democratic Party (Liberal)
            - Republican Party (Conservative)
          - **CAN:**
            - Liberal Party (Liberal)
            - Conservative Party (Conservative)
          - **GER:**
            - Christian Democratic Union (Conservative)
            - Social Democratic Party (Liberal)
          - *(Add more as needed)*

        - **Industries:**
          - Automotive
          - Technology
          - Energy
          - Healthcare
          - Manufacturing
          - Agriculture
          - Finance
          - Telecommunications
          - Pharmaceuticals
          - Construction

        - **Companies:**
          - **Automotive:**
            - General Motors (USA)
            - Toyota (JPN)
            - Volkswagen (DEU)
          - **Technology:**
            - Apple (USA)
            - Samsung (KOR)
            - Huawei (CHN)
          - **Energy:**
            - ExxonMobil (USA)
            - Shell (NLD)
            - BP (GBR)
          - **Healthcare:**
            - Pfizer (USA)
            - Roche (CHE)
            - Johnson & Johnson (USA)
          - *(Add more as needed)*

        ### Context

        {context_json}

        ### Scenarios

        - {"\n- ".join(scenarios)}

        ### Instructions

        Based on the above context and scenarios, generate two CSV-formatted tables strictly using the terms defined in the Comprehensive Lists.

        1. **Disposition Matrix** with columns: Decision_ID, Action, Scenario, Probability(%).
        2. **Modifier Matrix** with columns: Relation_Type, Relation_Name, Scenario, Modifier_Value.

        **Ensure that:**

        - Each decision is unique and reflects the party's ideology, current policies, economic indicators, public sentiment, existing trade alliances, and relationships with partners and industries.
        - Modifier values are normalized (e.g., 0.1 for +10%, -0.05 for -5%) and reflect how specific relationships influence the probability of actions under each scenario.
        - **Only** use the scenarios, actions, parties, countries, industries, and companies defined in the Comprehensive Lists.

        ### Output
        ```
        ### Disposition Matrix
        Decision_ID,Action,Scenario,Probability(%)
        1,Increase Tariffs,Recession,70
        2,Form Trade Alliances,Economic Boom,50
        3,Finalize Trade Agreements,Trade Deficit Increase,60
        4,Decrease Tariffs,Economic Boom,30
        5,Implement Subsidies,Recession,40
        6,Implement Climate Change Policies,Climate Change Policies,80
        7,Launch Election Campaign,Upcoming Elections,75
        8,Handle Political Scandal,Political Scandals,65
        9,Address Public Protests,Public Protests,55
        10,Enhance Disaster Response,Natural Disasters,60

        ### Modifier Matrix
        Relation_Type,Relation_Name,Scenario,Modifier_Value
        Alliance,EU,Recession,0.05
        Alliance,CAN,Economic Boom,0.10
        Industry,Automotive,Recession,0.15
        Industry,Technology,Climate Change Policies,-0.10
        ```
        """

        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that generates disposition matrices for political parties."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=3000,  # Adjust based on token usage
                temperature=0.7,
                n=1,
                stop=["```
```"]
            )
            full_output = response.choices[0].message['content'].strip()

            # Use regular expressions to extract the CSV sections
            disposition_pattern = r"### Disposition Matrix\n(.*?)\n\n### Modifier Matrix"
            modifier_pattern = r"### Modifier Matrix\n(.*)"
            closing_pattern = r"```
```"

            disposition_match = re.search(disposition_pattern, full_output, re.DOTALL)
            modifier_match = re.search(modifier_pattern, full_output, re.DOTALL)

            if disposition_match and modifier_match:
                disposition_csv = disposition_match.group(1).strip()
                modifier_csv = modifier_match.group(1).strip()

                # Parse Disposition Matrix
                try:
                    disposition_df = pd.read_csv(io.StringIO(disposition_csv))
                except Exception as e:
                    logging.error(f"Error parsing Disposition Matrix CSV: {e}")
                    disposition_df = pd.DataFrame()

                # Parse Modifier Matrix
                try:
                    modifier_df = pd.read_csv(io.StringIO(modifier_csv))
                except Exception as e:
                    logging.error(f"Error parsing Modifier Matrix CSV: {e}")
                    modifier_df = pd.DataFrame()

                # Validate DataFrames
                expected_disposition_columns = ['Decision_ID', 'Action', 'Scenario', 'Probability(%)']
                expected_modifier_columns = ['Relation_Type', 'Relation_Name', 'Scenario', 'Modifier_Value']

                if not all(col in disposition_df.columns for col in expected_disposition_columns):
                    logging.error("Disposition CSV does not contain the expected columns.")
                    disposition_df = pd.DataFrame()

                if not all(col in modifier_df.columns for col in expected_modifier_columns):
                    logging.error("Modifier CSV does not contain the expected columns.")
                    modifier_df = pd.DataFrame()

                return disposition_df, modifier_df
            else:
                logging.error("Expected sections '### Disposition Matrix' and '### Modifier Matrix' not found in the response.")
                return pd.DataFrame(), pd.DataFrame()
        except Exception as e:
            logging.error(f"Error generating disposition and modifier matrices: {e}")
            return pd.DataFrame(), pd.DataFrame()

    # =================================
    # 5. Synthesis of Future News and Social Media Posts
    # =================================

    def generate_future_news(context_json, forecast_period='6 months'):
        """
        Generates plausible future news articles that could impact the simulation.

        Args:
            context_json (str): JSON-formatted string containing all relevant data for the country/party.
            forecast_period (str): The forecasting period (e.g., '6 months').

        Returns:
            list: A list of generated news articles as strings.
        """
        prompt = f"""
        You are a journalist tasked with writing plausible future news articles that could impact the political and economic landscape of the following context within the next {forecast_period}.

        Context:
        {context_json}

        Generate 5 news articles with headlines and brief descriptions.
        """

        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a creative and insightful assistant that generates realistic future news articles based on provided context."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,  # Adjust based on needs
                temperature=0.7,
                n=1,
                stop=None
            )
            articles = response.choices[0].message['content']
            # Split articles assuming they are separated by newlines or numbering
            # This might require more sophisticated parsing based on GPT-4's output format
            article_list = [article.strip() for article in articles.split('\n') if article.strip()]
            return article_list
        except Exception as e:
            logging.error(f"Error generating future news: {e}")
            return []

    def generate_future_social_media(context_json, forecast_period='6 months'):
        """
        Generates plausible future social media posts that could impact the simulation.

        Args:
            context_json (str): JSON-formatted string containing all relevant data for the country/party.
            forecast_period (str): The forecasting period (e.g., '6 months').

        Returns:
            list: A list of generated social media posts as strings.
        """
        prompt = f"""
        You are a social media manager creating plausible future social media posts that reflect public sentiment and events within the next {forecast_period}.

        Context:
        {context_json}

        Generate 10 social media posts (e.g., tweets) that could influence or reflect the public's view on trade policies.
        """

        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a creative assistant that generates realistic future social media posts based on provided context."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,  # Adjust based on needs
                temperature=0.7,
                n=1,
                stop=None
            )
            posts = response.choices[0].message['content']
            # Split posts assuming they are separated by newlines or numbering
            # This might require more sophisticated parsing based on GPT-4's output format
            post_list = [post.strip() for post in posts.split('\n') if post.strip()]
            return post_list
        except Exception as e:
            logging.error(f"Error generating future social media posts: {e}")
            return []

    def integrate_future_events(model, country_code, party_name, future_news, future_posts):
        """
        Integrates future news and social media posts into the simulation model.

        Args:
            model (TradeModelEnhanced): The simulation model instance.
            country_code (str): The ISO alpha-3 country code.
            party_name (str): The name of the political party.
            future_news (list): List of future news articles.
            future_posts (list): List of future social media posts.
        """
        # Example: Schedule future events based on the forecast period
        for news in future_news:
            # Assign an implementation date within the next 6 months
            implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))
            # Add to model's event queue or relevant data structures
            model.add_future_news(country_code, party_name, news, implementation_date)

        for post in future_posts:
            implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))
            model.add_future_social_media(country_code, party_name, post, implementation_date)

# =================================
# 6. Simulation Model Enhancements
# =================================

def generate_and_assign_scenario_action_tables(processed_data, model):
    """
    Generates scenario-action probability and modifier tables for all parties in all countries and assigns them to the simulation model.

    Args:
        processed_data (dict): The dictionary containing all processed data.
        model (TradeModelEnhanced): The simulation model instance.
    """
    for country_code, political_info in processed_data['political_data'].items():
        parties_df = political_info.get('Political_Parties', pd.DataFrame())
        for _, party_row in parties_df.iterrows():
            party_name = party_row['Party']
            context_json = gather_context_for_party(processed_data, country_code, party_name)
            disposition_df, modifier_df = generate_disposition_and_modifier_matrices(context_json, model.get_scenarios())
            if disposition_df.empty or modifier_df.empty:
                logging.warning(f"Failed to generate scenario-action tables for {party_name} in {country_code}.")
                continue
            # Assign tables to the corresponding GovernmentAgent
            gov_agents = [agent for agent in model.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == country_code and agent.governing_party == party_name]
            for gov in gov_agents:
                gov.disposition_matrix = disposition_df
                gov.modifier_matrix = modifier_df
                logging.info(f"Assigned scenario-action tables to {party_name} in {country_code}.")

    # Generate future events based on initial tables
    for country_code, political_info in processed_data['political_data'].items():
        parties_df = political_info.get('Political_Parties', pd.DataFrame())
        for _, party_row in parties_df.iterrows():
            party_name = party_row['Party']
            context_json = gather_context_for_party(processed_data, country_code, party_name)
            future_news = generate_future_news(context_json)
            future_posts = generate_future_social_media(context_json)
            integrate_future_events(model, country_code, party_name, future_news, future_posts)

# =================================
# 7. Additional Enhancements and Best Practices
# =================================

def initialize_simulation():
    """
    Initializes the entire simulation workflow:
    - Loads environment variables.
    - Sets up logging.
    - Acquires data.
    - Processes data.
    - Initializes the simulation model.
    - Generates and assigns scenario-action tables.
    - Runs the simulation.
    """
    # Load environment variables
    load_openai_api_key()

    # Set up logging
    setup_logging()

    # Data Acquisition
    data_acquisition = DataAcquisition()
    raw_data = data_acquisition.fetch_all_data()

    # Data Processing
    data_processing = DataProcessing(raw_data)
    data_processing.process_all_data()
    processed_data = data_processing.processed_data

    # Initialize the enhanced Trade Model
    model = TradeModelEnhanced(
        processed_data=processed_data,
        political_data=raw_data.get('political_data', {}),
        news_data=raw_data.get('news_data', {}),
        social_media_data=raw_data.get('social_media_data', {}),
        legislation_data=raw_data.get('legislation_data', {})
    )

    # Generate and assign scenario-action tables
    generate_and_assign_scenario_action_tables(processed_data, model)

    # Initialize branching parameters
    model.current_branching_depth = 0

    return model

# =================================
# 8. Comprehensive Unit Testing
# =================================

class TestTradeModelEnhanced(unittest.TestCase):
    def setUp(self):
        # Sample processed data
        self.processed_data = {
            'world_bank': pd.DataFrame({
                'Country': ['USA'],
                'Year': [2023],
                'GDP': [21000000000000],
                'Trade_Percentage_GDP': [25],
                'GDP_Growth': [2.3]
            }),
            'political_data': {
                'USA': {
                    'Political_Parties': pd.DataFrame({
                        'Party': ['Democratic Party', 'Republican Party'],
                        'Ideology': ['Liberal', 'Conservative']
                    }),
                    'Upcoming_Elections': pd.DataFrame({
                        'Date_Upcoming_Election': ['2024-11-05']
                    }),
                    'Current_Policies': pd.DataFrame({
                        'Policy': ['Tariff Rate Adjustment', 'Climate Change Initiative'],
                        'Status': ['Increasing', 'Active']
                    })
                }
            },
            'news_data': {
                'USA': pd.DataFrame({
                    'description': [
                        'The economy is booming with low unemployment rates.',
                        'Public sentiment is turning negative due to rising inflation.',
                        'The Democratic Party launches a new green energy initiative.'
                    ],
                    'Sentiment': ['Positive', 'Negative', 'Positive'],
                    'Entities': [['economy', 'unemployment'], ['public sentiment', 'inflation'], ['Democratic Party', 'green energy']]
                })
            },
            'social_media_data': {
                'USA': pd.DataFrame({
                    'text': [
                        '@user1 Loving the new trade policies!',
                        '@user2 Concerned about the recent tariff increases.',
                        '@user3 Supportive of the green energy initiatives!'
                    ],
                    'Sentiment': ['Positive', 'Negative', 'Positive'],
                    'Entities': [['trade policies'], ['tariff increases'], ['green energy initiatives']]
                })
            },
            'legislation_data': {
                'USA': pd.DataFrame({
                    'Bill_Name': ['Trade Cooperation Act', 'Green Energy Promotion Act'],
                    'Status': ['active', 'active'],
                    'Date_Introduced': ['2023-03-22', '2022-07-15']
                })
            },
            'Trade_Alliances': {
                'USA': ['EU', 'CAN']
            }
        }

        # Define scenarios
        self.scenarios = [
            'Recession',
            'Economic Boom',
            'Inflation Surge',
            'Trade Deficit Increase',
            'Upcoming Elections',
            'Political Scandals',
            'Leadership Changes',
            'Public Protests',
            'Shifts in Public Sentiment',
            'Natural Disasters',
            'Climate Change Policies'
        ]

        # Initialize model
        self.model = TradeModelEnhanced(
            processed_data=self.processed_data,
            political_data=self.processed_data.get('political_data', {}),
            news_data=self.processed_data.get('news_data', {}),
            social_media_data=self.processed_data.get('social_media_data', {}),
            legislation_data=self.processed_data.get('legislation_data', {})
        )

    @patch('openai.ChatCompletion.create')
    def test_generate_disposition_and_modifier_matrices(self, mock_create):
        # Mock GPT-4 response with both matrices
        mock_csv = """### Disposition Matrix
Decision_ID,Action,Scenario,Probability(%)
1,Increase Tariffs,Recession,70
2,Form Trade Alliances,Economic Boom,50
3,Finalize Trade Agreements,Trade Deficit Increase,60
4,Decrease Tariffs,Economic Boom,30
5,Implement Subsidies,Recession,40
6,Implement Climate Change Policies,Climate Change Policies,80
7,Launch Election Campaign,Upcoming Elections,75
8,Handle Political Scandal,Political Scandals,65
9,Address Public Protests,Public Protests,55
10,Enhance Disaster Response,Natural Disasters,60

### Modifier Matrix
Relation_Type,Relation_Name,Scenario,Modifier_Value
Alliance,EU,Recession,0.05
Alliance,CAN,Economic Boom,0.10
Industry,Automotive,Recession,0.15
Industry,Technology,Climate Change Policies,-0.10
"""
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message = {'content': mock_csv}
        mock_create.return_value = mock_response

        # Compile context
        context_json = gather_context_for_party(self.processed_data, 'USA', 'Democratic Party')

        # Generate matrices
        disposition_df, modifier_df = generate_disposition_and_modifier_matrices(context_json, self.scenarios)

        # Assertions
        self.assertFalse(disposition_df.empty, "Disposition DataFrame should not be empty.")
        self.assertFalse(modifier_df.empty, "Modifier DataFrame should not be empty.")
        self.assertListEqual(list(disposition_df.columns), ['Decision_ID', 'Action', 'Scenario', 'Probability(%)'], "Disposition DataFrame columns mismatch.")
        self.assertListEqual(list(modifier_df.columns), ['Relation_Type', 'Relation_Name', 'Scenario', 'Modifier_Value'], "Modifier DataFrame columns mismatch.")
        self.assertEqual(len(disposition_df), 10, "Disposition DataFrame should have 10 rows.")
        self.assertEqual(len(modifier_df), 4, "Modifier DataFrame should have 4 rows.")
        self.assertEqual(disposition_df.iloc[0]['Action'], 'Increase Tariffs', "First action should be 'Increase Tariffs'.")
        self.assertEqual(modifier_df.iloc[0]['Modifier_Value'], 0.05, "First modifier value should be 0.05.")

    @patch('openai.ChatCompletion.create')
    def test_agent_decision_making(self, mock_create):
        # Mock GPT-4 response with both matrices
        mock_csv = """### Disposition Matrix
Decision_ID,Action,Scenario,Probability(%)
1,Increase Tariffs,Recession,70
2,Form Trade Alliances,Economic Boom,50
3,Finalize Trade Agreements,Trade Deficit Increase,60
4,Decrease Tariffs,Economic Boom,30
5,Implement Subsidies,Recession,40
6,Implement Climate Change Policies,Climate Change Policies,80
7,Launch Election Campaign,Upcoming Elections,75
8,Handle Political Scandal,Political Scandals,65
9,Address Public Protests,Public Protests,55
10,Enhance Disaster Response,Natural Disasters,60

### Modifier Matrix
Relation_Type,Relation_Name,Scenario,Modifier_Value
Alliance,EU,Recession,0.05
Alliance,CAN,Economic Boom,0.10
Industry,Automotive,Recession,0.15
Industry,Technology,Climate Change Policies,-0.10
"""
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message = {'content': mock_csv}
        mock_create.return_value = mock_response

        # Compile context
        context_json = gather_context_for_party(self.processed_data, 'USA', 'Democratic Party')

        # Generate matrices
        disposition_df, modifier_df = generate_disposition_and_modifier_matrices(context_json, self.scenarios)

        # Initialize agent
        agent = GovernmentAgentEnhanced(
            unique_id=123456,
            model=self.model,
            country_code='USA',
            economic_indicators=self.processed_data['world_bank'][self.processed_data['world_bank']['Country'] == 'USA'].iloc[0].to_dict(),
            political_parties=self.processed_data['political_data']['USA']['Political_Parties'],
            upcoming_elections=self.processed_data['political_data']['USA']['Upcoming_Elections'],
            current_policies=self.processed_data['political_data']['USA']['Current_Policies'],
            disposition_matrix=disposition_df,
            modifier_matrix=modifier_df
        )

        # Mock current scenarios
        current_scenarios = ['Recession', 'Economic Boom']

        # Mock execute_action method
        with patch.object(agent, 'execute_action') as mock_execute_action:
            # Seed random number generator for reproducibility
            random.seed(0)
            agent.implement_policies_based_on_dispositions(current_scenarios)
            # Check if execute_action was called
            self.assertTrue(mock_execute_action.called, "execute_action should be called based on disposition probabilities and modifiers.")
            # Further assertions can be made based on the actions expected

    def tearDown(self):
        # Clean up any temporary files or resources if needed
        pass

# =================================
# 9. Main Execution
# =================================

if __name__ == '__main__':
    # Initialize simulation
    model = initialize_simulation()

    # Define simulation parameters
    total_batches = 5  # Number of batches to run
    batch_size = 50     # Number of simulation steps per batch
    branching_depth = 1  # Initial branching depth

    for batch in range(total_batches):
        logging.info(f"Starting simulation batch {batch + 1}/{total_batches} at branching depth {branching_depth}.")
        model.run_batch(batch_size, branching_depth)
        branching_depth += 1  # Increment depth for next batch
        if branching_depth > model.max_branching_depth:
            logging.info(f"Reached maximum branching depth of {model.max_branching_depth}. Stopping further branching.")
            break

    # Optionally, run unit tests
    unittest.main(argv=['first-arg-is-ignored'], exit=False)

"""https://colab.research.google.com/drive/1YY-3cd7-uK42XZ1bHws936RDFHUIT2Xu?usp=sharing

https://chatgpt.com/share/6736d800-5424-8004-90d7-b516affd8735

https://chatgpt.com/c/6735612d-3208-8004-a824-c0a96a53b270
"""
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Technical Whitepaper: A Simulation of Global Trade Dynamics using Agent-Based Modeling and GPT-4 Integration\n",
        "\n",
        "## 1. Introduction and Abstract\n",
        "\n",
        "In an era where global trade policies and their implications are becoming increasingly complex, a detailed and dynamic simulation tool can provide invaluable insights for policymakers, business leaders, and researchers. This whitepaper presents an innovative simulation framework designed to model global trade dynamics, political decision-making, and economic outcomes using a combination of agent-based modeling (ABM) and GPT-4 large language model (LLM) integration.\n",
        "\n",
        "Our simulation uses real-world economic, political, and social data from a variety of authoritative sources to create agents representing governments, companies, consumers, and intermediaries. These agents act according to data-driven \"disposition matrices\" (scenario-action probability tables) and \"modifier matrices\" (relation modifiers) that are partially generated by GPT-4 based on relevant context. Through iterative simulation steps, the model explores a wide range of probable future scenarios over a six-month horizon, updating policies and outcomes in response to changing conditions such as trade volumes, political climates, economic indicators, public sentiment, and upcoming elections.\n",
        "\n",
        "This whitepaper comprehensively describes the data sources used (including UN Comtrade, World Bank, IMF, WTO, GTA, USITC, Trading Economics, U.S. Census Bureau, political data from Wikipedia, and more), the variables ingested from these sources, the agent-based modeling framework, and the integration of GPT-4 to drive agent behaviors and scenario generation. It details how scenario branching is performed to capture different future possibilities and how we measure outcomes for scenario selection, iteration, and further simulations.\n",
        "\n",
        "## 2. Background and Motivation\n",
        "\n",
        "### 2.1 Need for Advanced Trade Simulations\n",
        "\n",
        "Global trade dynamics and policies are influenced by a wide range of economic, political, and social factors. Traditional modeling approaches often rely on static or simplified assumptions that do not adequately capture the complexity and interplay of factors affecting international trade. Our simulation model addresses these complexities by combining detailed data-driven approaches with advanced agent-based modeling, allowing for realistic and adaptive policymaking simulations.\n",
        "\n",
        "### 2.2 Integrating Agent-Based Modeling and GPT-4\n",
        "\n",
        "Agent-based modeling (ABM) provides a framework where individual agents, representing countries, companies, and consumers, interact with each other and evolve based on defined rules and behaviors. GPT-4, a large language model, can enhance ABM by dynamically generating decision rules (actions and outcomes) based on scenarios extracted from real-world data. GPT-4 can synthesize complex datasets into coherent strategies and policies that government agents might realistically undertake, providing a more human-like decision-making process within the simulation.\n",
        "\n",
        "### 2.3 Goals of the Simulation\n",
        "\n",
        "The primary goals of this simulation framework are to:\n",
        "\n",
        "- Predict how changes in tariffs, trade agreements, or political climates affect global trade volumes and economic outcomes.\n",
        "- Evaluate the influence of political scenarios, like upcoming elections or political scandals, on policy decisions and their consequent economic impacts.\n",
        "- Explore how public sentiment shifts, driven by real-time news and social media data, can influence government policies.\n",
        "- Provide a range of plausible scenarios and outcomes by branching through various future events and measures of trade volume, ensuring robust scenario analysis for policymakers and stakeholders.\n",
        "\n",
        "## 3. Detailed Architecture and Data Usage\n",
        "\n",
        "### 3.1 Data Sources and Variables Acquired\n",
        "\n",
        "Our simulation model relies on an extensive range of data sources to ensure accuracy and relevance. The data acquisition process is designed to pull economic, political, social, and legislative data from primary and backup sources. Below is an overview of each data source, the variables we pull from them, and their application in the simulation:\n",
        "\n",
        "**1. UN Comtrade Data**\n",
        "   - **Source**: United Nations Comtrade Database\n",
        "   - **API Endpoints**:\n",
        "     - `comtrade_data_endpoint`: For final trade data.\n",
        "     - `comtrade_tariffline_endpoint`: For tariff line data.\n",
        "     - `comtrade_bulk_endpoint`: For bulk data.\n",
        "     - `comtrade_suv_endpoint`: For Standard Unit Values (SUV) data.\n",
        "     - `comtrade_ais_endpoint`: For experimental AIS (ship tracking) data.\n",
        "     - `comtrade_metadata_endpoint`: For metadata and publication notes.\n",
        "     - `comtrade_reference_endpoint`: For reference data like commodity codes and country references.\n",
        "   - **Variables**:\n",
        "     - Trade values (imports, exports) for different commodities (cmdCode) and countries (reporterCode, partnerCode).\n",
        "     - Tariff line information by commodity and country.\n",
        "     - Commodity classifications and reference data (HS codes).\n",
        "     - Standard Unit Values (SUV) by period and commodity codes.\n",
        "     - AIS data for maritime tracking (under development).\n",
        "   - **Use**: This data helps determine current trade volumes, tariff lines, and commodity classifications, enriching the economic context for government and company agents.\n",
        "\n",
        "**2. World Bank Data**\n",
        "   - **Source**: World Bank Open Data API\n",
        "   - **Variables**:\n",
        "     - GDP (`NY.GDP.MKTP.CD`)\n",
        "     - Trade as a percentage of GDP (`NE.TRD.GNFS.ZS`)\n",
        "     - Additional economic indicators (like unemployment, inflation, etc. if needed)\n",
        "   - **Use**: Economic indicators (such as GDP, trade percentages) used to initialize government agents' economic conditions and to monitor changes over time.\n",
        "\n",
        "**3. USITC HTS Data**\n",
        "   - **Source**: United States International Trade Commission (USITC) HTS API\n",
        "   - **Variables**:\n",
        "     - Tariff classifications and rates by commodity codes\n",
        "   - **Use**: Detailed tariff rate data informs how government agent policies on tariffs could shift and affect industries.\n",
        "\n",
        "**4. Global Trade Alert (GTA) Data**\n",
        "   - **Source**: Global Trade Alert API\n",
        "   - **Variables**:\n",
        "     - Active trade measures and interventions globally.\n",
        "     - Timelines of policy changes (like tariff increases, trade agreements).\n",
        "   - **Use**: Provides additional context on current global trade measures and interventions, influencing scenario creation and government agent policies.\n",
        "\n",
        "**5. International Monetary Fund (IMF) Data**\n",
        "   - **Source**: IMF API\n",
        "   - **Variables**:\n",
        "     - Various macroeconomic indicators (e.g., GDP growth rates, inflation rates, current account balances) not covered by the World Bank data.\n",
        "   - **Use**: Additional macroeconomic data to ensure the simulation's economic context is robust and updated.\n",
        "\n",
        "**6. World Trade Organization (WTO) Data**\n",
        "   - **Source**: WTO data APIs or datasets\n",
        "   - **Variables**:\n",
        "     - Trade data and tariffs under various trade agreements\n",
        "     - Information on international trade policies and conflicts\n",
        "   - **Use**: Backup or complementary data to ensure completeness when other sources are missing or incomplete.\n",
        "\n",
        "**7. Trading Economics Data**\n",
        "   - **Source**: Trading Economics API\n",
        "   - **Variables**:\n",
        "     - Economic indicators (like interest rates, exchange rates, commodity prices) not covered elsewhere.\n",
        "   - **Use**: Enhances economic context (e.g., the cost of commodities, interest rates affecting company investments, and consumer spending).\n",
        "\n",
        "**8. US Census Data**\n",
        "   - **Source**: U.S. Census Bureau International Trade API\n",
        "   - **Variables**:\n",
        "     - Detailed U.S. export and import data by commodity codes and country\n",
        "   - **Use**: Provides detailed trade flow data, essential for accurate trade volume calculations and scenario generation.\n",
        "\n",
        "**9. Consolidated Screening List Data**\n",
        "   - **Source**: The Consolidated Screening List (CSL) from the International Trade Administration\n",
        "   - **Variables**:\n",
        "     - Entities and individuals restricted or sanctioned in international trade\n",
        "   - **Use**: Helps model the effects of sanctions and restricted parties on trade decisions of government agents and companies.\n",
        "\n",
        "**10. Political Data**\n",
        "   - **Source**: Data fetched from official government sources, Wikipedia's API, or specialized political data APIs.\n",
        "   - **Variables**:\n",
        "     - Lists of political parties and their ideologies\n",
        "     - Upcoming elections data and timetables\n",
        "     - Current government policies\n",
        "   - **Use**: Helps determine the political scenario for government agents, influence upcoming elections on policy changes, and incorporate political party ideologies into decision-making.\n",
        "\n",
        "**11. News Data**\n",
        "   - **Source**: NewsAPI or other news aggregator APIs\n",
        "   - **Variables**:\n",
        "     - Headlines and summaries of articles related to trade policies and economic conditions\n",
        "     - Public sentiment derived from news article sentiment analysis\n",
        "   - **Use**: News data helps gauge current public sentiment and ongoing events that can influence government policy decisions and scenario building.\n",
        "\n",
        "**12. Social Media Data**\n",
        "   - **Source**: Twitter API, potential other social media APIs\n",
        "   - **Variables**:\n",
        "     - Public sentiment derived from social media posts\n",
        "     - Key topics and public opinions on trade, economy, and government policies\n",
        "   - **Use**: Social media sentiment helps assess real-time public reaction to policies, influencing government agent actions and scenario branches.\n",
        "\n",
        "**13. Legislative Data**\n",
        "   - **Source**: Official government legislative tracking, third-party legislative data providers\n",
        "   - **Variables**:\n",
        "     - Current trade and economic legislation, bill names, statuses, introduction dates\n",
        "   - **Use**: Current legislative environment informs how future policies might evolve and how government agents act.\n",
        "\n",
        "**14. Developer Trade Gov Data**\n",
        "   - **Source**: Data from the `api.trade.gov` for trade data and analytics\n",
        "   - **Variables**:\n",
        "     - Data on trade barriers, market insights, compliance, etc.\n",
        "   - **Use**: Provides extra context on trade restrictions and compliance issues impacting trade flows and government decisions.\n",
        "\n",
        "### 3.2 Data Processing and Integration\n",
        "\n",
        "A Data Acquisition module systematically fetches data from these sources, handles rate limits, concurrency, and data validation. The Data Processing module subsequently cleans, normalizes, and stores this data in a structured format suitable for the simulation. Key steps include:\n",
        "\n",
        "- **Data Cleaning**: Removing duplicates, handling missing values, and ensuring data types are consistent.\n",
        "- **Data Validation**: Checking data ranges and consistency across sources.\n",
        "- **Integration**: Merging data from various sources into unified structures (e.g., a combined dataframe for economic indicators per country).\n",
        "- **Normalization**: Transforming different metrics and units into a unified format (e.g., converting currency units or normalizing sentiment scales).\n",
        "\n",
        "This processed data is then ready to initialize agents in the simulation, as well as to update the scenario and decision logic as the simulation progresses.\n",
        "\n",
        "## 4. Core Innovation: LLM-Based Agent Behavior and Scenario-Driven Decision Logic\n",
        "\n",
        "### 4.1 Government Agent Dispositions and Decision Logic\n",
        "\n",
        "Each Government Agent in the simulation is assigned dispositions and decision-making rules that determine their policy actions. These rules are captured in two key matrices:\n",
        "\n",
        "1. **Disposition Matrix (Scenario-Action Probability Table)**:\n",
        "   - **Description**: This matrix outlines how likely a government is to perform a particular action under different scenarios.\n",
        "   - **Columns**:\n",
        "     - *Decision_ID*: Unique identifier for each decision.\n",
        "     - *Action*: The policy action to be considered (e.g., \"Increase Tariffs\", \"Form Trade Alliances\").\n",
        "     - *Scenario*: The scenario in which this action might be taken (e.g., \"Recession\", \"Upcoming Elections\").\n",
        "     - *Probability(%)*: The probability (in percentage) that the government will take this action if the given scenario applies.\n",
        "   - **Example**:\n",
        "     ```\n",
        "     ### Disposition Matrix\n",
        "     Decision_ID,Action,Scenario,Probability(%)\n",
        "     1,Increase Tariffs,Recession,70\n",
        "     2,Form Trade Alliances,Economic Boom,50\n",
        "     3,Finalize Trade Agreements,Trade Deficit Increase,60\n",
        "     4,Decrease Tariffs,Economic Boom,30\n",
        "     5,Implement Subsidies,Recession,40\n",
        "     6,Implement Climate Change Policies,Climate Change Policies,80\n",
        "     7,Launch Election Campaign,Upcoming Elections,75\n",
        "     8,Handle Political Scandal,Political Scandals,65\n",
        "     9,Address Public Protests,Public Protests,55\n",
        "     10,Enhance Disaster Response,Natural Disasters,60\n",
        "     ```\n",
        "\n",
        "2. **Modifier Matrix (Relation Modifier Table)**:\n",
        "   - **Description**: This matrix adjusts the probabilities of actions based on relationships, alliances, and other contextual factors.\n",
        "   - **Columns**:\n",
        "     - *Relation_Type*: The type of relation (e.g., \"Alliance\", \"Industry\").\n",
        "     - *Relation_Name*: The name of the related entity or industry.\n",
        "     - *Scenario*: The scenario to which the relation applies.\n",
        "     - *Modifier_Value*: The numeric modifier (positive or negative) to apply to the base probability. For example, `0.10` represents a 10% increase and `-0.05` represents a 5% decrease.\n",
        "   - **Example**:\n",
        "     ```\n",
        "     ### Modifier Matrix\n",
        "     Relation_Type,Relation_Name,Scenario,Modifier_Value\n",
        "     Alliance,EU,Recession,0.05\n",
        "     Alliance,CAN,Economic Boom,0.10\n",
        "     Industry,Automotive,Recession,0.15\n",
        "     Industry,Technology,Climate Change Policies,-0.10\n",
        "     ```\n",
        "\n",
        "### 4.2 GPT-4 Integration for Disposition Matrices\n",
        "\n",
        "GPT-4 is used to generate these scenario-action probability and modifier tables. Specifically:\n",
        "\n",
        "- **Context Extraction**: For each government agent (country and political party), a detailed context JSON string is created. This context includes:\n",
        "  - Economic indicators (like GDP and trade percentages) sourced from the World Bank and other economic data providers.\n",
        "  - Political climate information (party ideologies, upcoming elections, current policies).\n",
        "  - Public sentiment derived from news and social media data.\n",
        "  - Legislative context and trade alliance information.\n",
        "- **GPT-4 Prompting**: Using this context, GPT-4 is given a predefined template prompt to produce:\n",
        "  1. A **Disposition Matrix** with scenario-action probabilities, considering the party's ideology, current policies, and economic/political context.\n",
        "  2. A **Modifier Matrix** indicating how specific relationships (alliances, industries) influence the probability of actions under various scenarios.\n",
        "\n",
        "**Example Prompt Template**:\n",
        "```\n",
        "You are an expert political strategist tasked with generating two matrices for a political party based on the provided context. The matrices should adhere strictly to the defined lists below.\n",
        "\n",
        "### Comprehensive Lists\n",
        "\n",
        "**1. Scenarios**\n",
        "\n",
        "- **Economic Scenarios:**\n",
        "  - Recession\n",
        "  - Economic Boom\n",
        "  - Inflation Surge\n",
        "  - Trade Deficit Increase\n",
        "\n",
        "- **Political Scenarios:**\n",
        "  - Upcoming Elections\n",
        "  - Political Scandals\n",
        "  - Leadership Changes\n",
        "\n",
        "- **Social Scenarios:**\n",
        "  - Public Protests\n",
        "  - Shifts in Public Sentiment\n",
        "\n",
        "- **Environmental Scenarios:**\n",
        "  - Natural Disasters\n",
        "  - Climate Change Policies\n",
        "\n",
        "**2. Actions**\n",
        "\n",
        "- **Economic Actions:**\n",
        "  - Increase Tariffs\n",
        "  - Decrease Tariffs\n",
        "  - Implement Subsidies\n",
        "  - Reduce Subsidies\n",
        "  - Increase Taxes\n",
        "  - Decrease Taxes\n",
        "  - Stimulate Economic Growth\n",
        "  - Implement Austerity Measures\n",
        "\n",
        "- **Political Actions:**\n",
        "  - Launch Election Campaign\n",
        "  - Initiate Policy Reforms\n",
        "  - Handle Political Scandals\n",
        "  - Resign Leadership\n",
        "  - Appoint New Leaders\n",
        "\n",
        "- **Social Actions:**\n",
        "  - Address Public Protests\n",
        "  - Launch Public Awareness Campaigns\n",
        "  - Implement Social Welfare Programs\n",
        "\n",
        "- **Environmental Actions:**\n",
        "  - Implement Climate Change Policies\n",
        "  - Enhance Disaster Response Mechanisms\n",
        "  - Promote Renewable Energy Initiatives\n",
        "\n",
        "- **Trade Actions:**\n",
        "  - Form Trade Alliances\n",
        "  - Finalize Trade Agreements\n",
        "  - Impose Trade Sanctions\n",
        "  - Lift Trade Sanctions\n",
        "\n",
        "- **Defense and Security Actions:**\n",
        "  - Increase Defense Spending\n",
        "  - Decrease Defense Spending\n",
        "  - Strengthen Security Measures\n",
        "\n",
        "**3. Parties/Countries/Industries/Companies**\n",
        "\n",
        "- **Countries (ISO Alpha-3 Codes):**\n",
        "  - USA (United States of America)\n",
        "  - CAN (Canada)\n",
        "  - CHN (China)\n",
        "  - DEU (Germany)\n",
        "  - FRA (France)\n",
        "  - JPN (Japan)\n",
        "  - GBR (United Kingdom)\n",
        "  - ITA (Italy)\n",
        "  - RUS (Russia)\n",
        "  - IND (India)\n",
        "  - BRA (Brazil)\n",
        "  - AUS (Australia)\n",
        "  - KOR (South Korea)\n",
        "  - ESP (Spain)\n",
        "  - MEX (Mexico)\n",
        "\n",
        "- **Political Parties:**\n",
        "  - **USA:**\n",
        "    - Democratic Party (Liberal)\n",
        "    - Republican Party (Conservative)\n",
        "  - **CAN:**\n",
        "    - Liberal Party (Liberal)\n",
        "    - Conservative Party (Conservative)\n",
        "  - **GER:**\n",
        "    - Christian Democratic Union (Conservative)\n",
        "    - Social Democratic Party (Liberal)\n",
        "\n",
        "- **Industries:**\n",
        "  - Automotive\n",
        "  - Technology\n",
        "  - Energy\n",
        "  - Healthcare\n",
        "  - Manufacturing\n",
        "  - Agriculture\n",
        "  - Finance\n",
        "  - Telecommunications\n",
        "  - Pharmaceuticals\n",
        "  - Construction\n",
        "\n",
        "- **Companies:**\n",
        "  - **Automotive:**\n",
        "    - General Motors (USA)\n",
        "    - Toyota (JPN)\n",
        "    - Volkswagen (DEU)\n",
        "  - **Technology:**\n",
        "    - Apple (USA)\n",
        "    - Samsung (KOR)\n",
        "    - Huawei (CHN)\n",
        "  - **Energy:**\n",
        "    - ExxonMobil (USA)\n",
        "    - Shell (NLD)\n",
        "    - BP (GBR)\n",
        "  - **Healthcare:**\n",
        "    - Pfizer (USA)\n",
        "    - Roche (CHE)\n",
        "    - Johnson & Johnson (USA)\n",
        "\n",
        "### Context\n",
        "\n",
        "{context_json}\n",
        "\n",
        "### Scenarios\n",
        "\n",
        "- {\"\\n- \".join(scenarios)}\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Based on the above context and scenarios, generate two CSV-formatted tables strictly using the terms defined in the Comprehensive Lists.\n",
        "\n",
        "1. **Disposition Matrix** with columns: Decision_ID, Action, Scenario, Probability(%).\n",
        "2. **Modifier Matrix** with columns: Relation_Type, Relation_Name, Scenario, Modifier_Value.\n",
        "\n",
        "**Ensure that:**\n",
        "\n",
        "- Each decision is unique and reflects the party's ideology, current policies, economic indicators, public sentiment, existing trade alliances, and relationships with partners and industries.\n",
        "- Modifier values are normalized (e.g., 0.1 for +10%, -0.05 for -5%) and reflect how specific relationships influence the probability of actions under each scenario.\n",
        "- **Only** use the scenarios, actions, parties, countries, industries, and companies defined in the Comprehensive Lists.\n",
        "\n",
        "### Output\n",
        "```\n",
        "### Disposition Matrix\n",
        "Decision_ID,Action,Scenario,Probability(%)\n",
        "1,Increase Tariffs,Recession,70\n",
        "2,Form Trade Alliances,Economic Boom,50\n",
        "3,Finalize Trade Agreements,Trade Deficit Increase,60\n",
        "4,Decrease Tariffs,Economic Boom,30\n",
        "5,Implement Subsidies,Recession,40\n",
        "6,Implement Climate Change Policies,Climate Change Policies,80\n",
        "7,Launch Election Campaign,Upcoming Elections,75\n",
        "8,Handle Political Scandal,Political Scandals,65\n",
        "9,Address Public Protests,Public Protests,55\n",
        "10,Enhance Disaster Response,Natural Disasters,60\n",
        "\n",
        "### Modifier Matrix\n",
        "Relation_Type,Relation_Name,Scenario,Modifier_Value\n",
        "Alliance,EU,Recession,0.05\n",
        "Alliance,CAN,Economic Boom,0.10\n",
        "Industry,Automotive,Recession,0.15\n",
        "Industry,Technology,Climate Change Policies,-0.10\n",
        "```\n",
        "```\n",
        "\n",
        "### 4.3 Example of GPT-4 Output\n",
        "\n",
        "Given the prompt and a context for the USA's Democratic Party, GPT-4 produces a disposition matrix and a modifier matrix. An example output (abbreviated) might look like:\n",
        "\n",
        "```\n",
        "### Disposition Matrix\n",
        "Decision_ID,Action,Scenario,Probability(%)\n",
        "1,Increase Tariffs,Recession,70\n",
        "2,Decrease Tariffs,Economic Boom,30\n",
        "3,Implement Subsidies,Recession,50\n",
        "4,Form Trade Alliances,Upcoming Elections,60\n",
        "5,Implement Climate Change Policies,Climate Change Policies,80\n",
        "...\n",
        "```\n",
        "\n",
        "and\n",
        "\n",
        "```\n",
        "### Modifier Matrix\n",
        "Relation_Type,Relation_Name,Scenario,Modifier_Value\n",
        "Alliance,CAN,Economic Boom,0.10\n",
        "Industry,Technology,Climate Change Policies,-0.10\n",
        "...\n",
        "```\n",
        "\n",
        "### 4.4 Application of Disposition and Modifier Matrices\n",
        "\n",
        "1. **Scenario and Probability Calculation**:\n",
        "   - The simulation identifies current active scenarios based on economic and political data.\n",
        "   - For each government agent, the relevant subset of their disposition matrix is selected.\n",
        "   - The base probability of each action is derived for each scenario.\n",
        "\n",
        "2. **Relation Modifiers**:\n",
        "   - The modifier matrix is applied to adjust these probabilities based on alliances (like with Canada), industries impacted by policies (like automotive or technology), and other relationships.\n",
        "   - For example, if an agent has a strong alliance with Canada and an \"Economic Boom\" scenario, a `0.10` (10% increase) modifier might increase the base probability of certain actions like forming trade alliances.\n",
        "\n",
        "3. **Action Decision**:\n",
        "   - The final adjusted probabilities are used to determine which actions the agent will implement stochastically. For instance, if an \"Increase Tariffs\" action has a 70% probability under \"Recession\" scenario and alliance modifiers add 5%, the final probability might be 75%.\n",
        "\n",
        "### 4.5 Example: Generating and Using the Tables\n",
        "\n",
        "For the Democratic Party of the USA (given the context of a mild recession, upcoming elections, and alliances):\n",
        "- **GPT-4 Output**:\n",
        "  - A disposition matrix listing probabilities for various actions under relevant scenarios.\n",
        "  - A modifier matrix showing how alliances and industries adjust these probabilities.\n",
        "- **Simulation Step**:\n",
        "  - If the scenario is \"Recession\" and the \"Increase Tariffs\" action has a 70% base probability:\n",
        "    - If the alliance with the EU in a recession scenario adds a 5% modifier, the final probability is `70% + 5% = 75%`.\n",
        "  - The government agent may then decide to \"Increase Tariffs\" with a 75% chance in that step.\n",
        "\n",
        "## 5. Agent-Based Modeling with Data-Driven and Scenario-Driven Logic\n",
        "\n",
        "### 5.1 Agent Types and Behaviors\n",
        "\n",
        "1. **Government Agents**:\n",
        "   - Represent countries and their policies.\n",
        "   - Have disposal and modifier matrices that guide actions under different scenarios.\n",
        "   - Adjust policies like tariffs, subsidies, trade alliances, and spending based on economic indicators, political pressures, and public sentiment.\n",
        "\n",
        "2. **Company Agents**:\n",
        "   - Represent businesses in different sectors (e.g., manufacturing, technology).\n",
        "   - Decisions influenced by government policies (like tariffs), production capacity, financial health, and market demand.\n",
        "   - They adjust sourcing (domestic vs. imported) and production according to changing trade policies and economic conditions.\n",
        "   - Might invest in industries to increase competitiveness or adapt to new policies.\n",
        "\n",
        "3. **Consumer Agents**:\n",
        "   - Represent individual or aggregate consumer behavior.\n",
        "   - Adjust consumption preferences based on income level, tariffs on imported goods, and other economic conditions.\n",
        "   - For instance, if tariffs on imported luxury goods rise, consumers might shift towards domestic alternatives.\n",
        "\n",
        "4. **Intermediary Agents**:\n",
        "   - Represent entities like logistics providers and financial institutions that facilitate trade.\n",
        "   - Their services and costs can influence how companies and governments operate within global supply chains.\n",
        "\n",
        "### 5.2 Detailed Agent Logic\n",
        "\n",
        "**GovernmentAgentEnhanced**:\n",
        "- **Economic_indicators**: Values like GDP, trade deficit, GDP growth used in scenario determination.\n",
        "- **Political_data**: Includes party ideology, upcoming election dates, public sentiment, and how these may influence decision probabilities.\n",
        "- **Disposition_matrix & modifier_matrix**: Determine action probabilities under given scenarios, adjusting for relationships like alliances or industry support.\n",
        "\n",
        "**CompanyAgent**:\n",
        "- **Production_capacity & financial_health**: Track health and capabilities. If demand is high, a company increases production. If demand falls, production might decrease.\n",
        "- **Supply_chain**: Decides whether to source domestically or internationally, factoring in tariffs and trade policies.\n",
        "- **Invest_in_industry**: At random intervals, invests in improving financial health or production capacity.\n",
        "\n",
        "**ConsumerAgent**:\n",
        "- **Income_level**: Affects the ratio of spending on essential vs. luxury goods.\n",
        "- **Demand_preferences**: Adjusted by policy changes (like tariffs on imports) and economic conditions. For instance, higher tariffs on imports might lead to increased demand for domestic goods.\n",
        "\n",
        "### 5.3 The Simulation Model Architecture\n",
        "\n",
        "**TradeModelEnhanced**:\n",
        "- Brings all agents together in a simulation environment.\n",
        "- Gathers processed data for each country, initializes agents with relevant data.\n",
        "- Each step:\n",
        "  - Government agents decide policies.\n",
        "  - Company agents adjust production, sourcing, and investments.\n",
        "  - Consumer agents adjust consumption preferences.\n",
        "- Each batch of steps:\n",
        "  - The model collects data (like trade volume).\n",
        "  - Identifies scenarios (like average or outlier outcomes).\n",
        "- Introduces new events (like future news or policies) to simulate evolving conditions.\n",
        "\n",
        "### 5.4 Example Agent Interactions\n",
        "\n",
        "A simplified example:\n",
        "- A GovernmentAgent faces a \"Recession\" scenario.\n",
        "  - The disposition matrix gives a 70% base probability for \"Increase Tariffs\".\n",
        "  - The modifier matrix (like alliance with EU giving a 5% increase) raises it to 75%.\n",
        "  - The government decides to increase tariffs.\n",
        "- CompanyAgent sees an increase in tariffs on imports:\n",
        "  - Company might shift sourcing from imported to domestic resources to avoid higher import costs.\n",
        "- ConsumerAgent experiences higher prices on imported luxury goods:\n",
        "  - If tariffs significantly raise these prices, consumers shift consumption, influencing future production decisions of CompanyAgents.\n",
        "\n",
        "## 6. Simulation Process and Scenario Analysis\n",
        "\n",
        "### 6.1 The Step-by-Step Simulation\n",
        "\n",
        "1. **Initialization**:\n",
        "   - Agents are created with data from sources (economic indicators, political climates).\n",
        "   - Government agent's disposition and modifier matrices are generated by GPT-4 using the initial data context.\n",
        "   - Company and consumer agents set up with initial parameters (production capacity, demand preferences).\n",
        "\n",
        "2. **Single Simulation Step**:\n",
        "   - Government agents:\n",
        "     - Assess current scenario.\n",
        "     - Determine actions (like adjusting tariffs) using their disposition and modifier matrices.\n",
        "     - Implement chosen actions.\n",
        "   - Company agents:\n",
        "     - Observe changes in policies (like tariff changes).\n",
        "     - Adjust sourcing and production capacity accordingly.\n",
        "     - Possibly invest in capacity to improve financial health.\n",
        "   - Consumer agents:\n",
        "     - Adjust consumption preferences based on new government policies.\n",
        "   - Intermediary agents:\n",
        "     - Provide services (like adjusting shipping rates or financial products) based on the scenario.\n",
        "\n",
        "3. **Data Collection**:\n",
        "   - After each step, the model collects metrics such as trade volume, average tariff rates, and others for analysis and scenario determination.\n",
        "\n",
        "4. **Batch Simulation**:\n",
        "   - The model runs for several steps (e.g., 50 steps to simulate 6 months).\n",
        "   - At the end of the batch, the model analyzes outcomes:\n",
        "     - **Mean and Median outcomes**: The average/median scenario.\n",
        "     - **1 Sigma Scenarios**: Scenarios around one standard deviation from the mean.\n",
        "     - **High-Impact Outliers**: Scenarios beyond two standard deviations.\n",
        "\n",
        "5. **Scenario Branching**:\n",
        "   - Based on identified outcomes, the model creates branches for plausible future scenarios:\n",
        "     - For the mean scenario, 1-sigma scenarios, and outlier scenarios.\n",
        "   - For each scenario branch:\n",
        "     - The model uses GPT-4 to generate updated disposition and modifier matrices given the new scenario context.\n",
        "     - Agents continue the simulation for the next batch of steps.\n",
        "\n",
        "### 6.2 Detailed Scenario Branching Logic\n",
        "\n",
        "**Example**:\n",
        "- After the first 6-month simulation:\n",
        "  - The average trade volume scenario (`Mean Outcome`) is \\$1 trillion.\n",
        "  - A 1-sigma scenario is \\$1.1 trillion.\n",
        "  - Another 1-sigma scenario is \\$0.9 trillion.\n",
        "  - An outlier scenario might be \\$1.2 trillion.\n",
        "\n",
        "**Branching**:\n",
        "- The simulation creates branches:\n",
        "  1. **Mean scenario** (Trade volume \\$1 trillion)\n",
        "  2. **1 Sigma Above** scenario (Trade volume \\$1.1 trillion)\n",
        "  3. **1 Sigma Below** scenario (Trade volume \\$0.9 trillion)\n",
        "  4. **High-Impact Outlier** scenario (Trade volume \\$1.2 trillion)\n",
        "\n",
        "For each branch:\n",
        "- GPT-4 is provided an updated context (like which scenario is unfolding) and asked to generate new disposition and modifier matrices.\n",
        "- The model reinitializes with these updated matrices and runs another batch of simulation steps.\n",
        "\n",
        "### 6.3 Iteration and Depth of Simulation\n",
        "\n",
        "The simulation is designed to run multiple batches to cover a certain horizon (e.g., 6 months per batch for a total horizon of 2.5 years or 5 batches):\n",
        "- **Total Batches**: 5.\n",
        "- **Steps per Batch**: Each batch may run for 50 steps, simulating daily or weekly increments within a 6-month period.\n",
        "- **Branching Depth**: Up to a maximum depth of 3 scenario branches to avoid combinatorial explosion.\n",
        "\n",
        "At each branching step, the simulation explores different plausible futures, updating conditions and decisions. This branching allows the model to explore a variety of future states and their likelihood.\n",
        "\n",
        "### 6.4 Integration of Future Events using GPT-4\n",
        "\n",
        "At the end of each batch:\n",
        "- The simulation identifies key outcomes and possible future events (like new trade deals, changes in public sentiment, or political shifts).\n",
        "- GPT-4 generates plausible future news articles and social media posts reflecting these outcomes and sentiments.\n",
        "- These future events are scheduled into the model to update agent decision-making in subsequent steps, reflecting dynamic changes in the environment over time.\n",
        "\n",
        "### 6.5 Example: Full Prompt to GPT-4 with Example Output\n",
        "\n",
        "**Initial Prompt**:\n",
        "```\n",
        "You are an expert political strategist tasked with generating two matrices for a political party based on the provided context...\n",
        "...\n",
        "```\n",
        "\n",
        "**Initial Context**:\n",
        "```\n",
        "{\n",
        "  'Country': 'USA',\n",
        "  'Party': 'Democratic Party',\n",
        "  'Ideology': 'Liberal',\n",
        "  'Recent_News_Sentiments': {'Positive': 10, 'Negative': 5},\n",
        "  'Recent_Social_Media_Sentiments': {'Positive': 8, 'Negative': 7},\n",
        "  'Current_Policies': [\n",
        "    {'Policy': 'Tariff Rate Adjustment', 'Status': 'Increasing'},\n",
        "    {'Policy': 'Climate Change Initiative', 'Status': 'Active'}\n",
        "  ],\n",
        "  'Legislation': [\n",
        "    {'Bill_Name': 'Trade Cooperation Act', 'Status': 'active', 'Date_Introduced': '2023-03-22'},\n",
        "    {'Bill_Name': 'Green Energy Promotion Act', 'Status': 'active', 'Date_Introduced': '2022-07-15'}\n",
        "  ],\n",
        "  'Upcoming_Elections': [\n",
        "    {'Date_Upcoming_Election': '2024-11-05'}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "**GPT-4 Output**:\n",
        "```\n",
        "### Disposition Matrix\n",
        "Decision_ID,Action,Scenario,Probability(%)\n",
        "1,Implement Climate Change Policies,Climate Change Policies,80\n",
        "2,Increase Tariffs,Recession,70\n",
        "3,Form Trade Alliances,Economic Boom,50\n",
        "4,Initiate Policy Reforms,Upcoming Elections,65\n",
        "5,Address Public Protests,Public Protests,55\n",
        "\n",
        "### Modifier Matrix\n",
        "Relation_Type,Relation_Name,Scenario,Modifier_Value\n",
        "Alliance,CAN,Economic Boom,0.10\n",
        "Industry,Technology,Climate Change Policies,-0.05\n",
        "```\n",
        "\n",
        "## 7. Conclusion and Future Work\n",
        "\n",
        "This agent-based modeling system, enhanced with GPT-4 integration for generating scenario-driven decision matrices, represents a significant advancement in simulating and predicting the outcomes of complex global trade scenarios. By leveraging detailed data inputs (from economic to political data) and exploring various plausible futures through scenario branching, this tool provides valuable insights for policymakers, businesses, and researchers alike.\n",
        "\n",
        "**Key Contributions**:\n",
        "- A comprehensive system that uses real-world data, expert political strategies simulated through GPT-4, and ABM to explore future trade scenarios.\n",
        "- Iterative scenario branching to capture a wide range of potential futures and measure their economic and political implications over a 6-month horizon.\n",
        "\n",
        "**Future Enhancements** may include:\n",
        "- Incorporation of more diverse and real-time data sources, such as global commodity prices or real-time news sentiment.\n",
        "- Refinement of scenario selection and branching based on more sophisticated statistical methods and domain expert feedback.\n",
        "- Expanding the horizon beyond 6 months with additional policy decisions and global events.\n",
        "- Increasing the fidelity of agent behaviors, especially for companies and consumers, using more nuanced economic modeling and microeconomic data.\n",
        "- Enhanced detail on how legislative changes, trade alliances, and global political shifts affect government policies and outcomes.\n",
        "\n",
        "By continually refining the model, incorporating feedback from domain experts, and updating data inputs, we aim to maintain the simulation's accuracy, relevance, and utility in shaping informed policy and strategic business decisions in an ever-evolving global trade landscape."
      ],
      "metadata": {
        "id": "KO32L7eXh2hk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5DGmjUhhtFU"
      },
      "outputs": [],
      "source": [
        "# trade_wars_simulation.py\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wbgapi as wb\n",
        "import time\n",
        "import logging\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from mesa import Agent, Model\n",
        "from mesa.time import RandomActivation\n",
        "from mesa.datacollection import DataCollector\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "from flask import Flask, request, jsonify\n",
        "import threading\n",
        "import unittest\n",
        "import pycountry\n",
        "import random\n",
        "from scipy import stats\n",
        "from dotenv import load_dotenv\n",
        "import requests_cache\n",
        "from typing import Optional, Dict, Any, List, Union, Tuple\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from flask_cors import CORS\n",
        "import zipfile\n",
        "import spacy\n",
        "from transformers import pipeline, Pipeline\n",
        "from apscheduler.schedulers.background import BackgroundScheduler\n",
        "import json\n",
        "import io\n",
        "import openai\n",
        "import sys\n",
        "import re\n",
        "from unittest.mock import patch, MagicMock\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# =================================\n",
        "# 1. Setup and Configuration\n",
        "# =================================\n",
        "\n",
        "# Ensure OpenAI API key is set\n",
        "def load_openai_api_key(env_file='.env'):\n",
        "    \"\"\"\n",
        "    Loads the OpenAI API key from the specified .env file.\n",
        "    \"\"\"\n",
        "    load_dotenv(env_file)\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        logging.error(\"OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
        "        sys.exit(1)\n",
        "    openai.api_key = api_key\n",
        "\n",
        "def setup_logging(log_file='simulation.log', log_level=logging.INFO):\n",
        "    \"\"\"\n",
        "    Sets up the logging configuration.\n",
        "\n",
        "    Args:\n",
        "        log_file (str): The file to which logs will be written.\n",
        "        log_level (int): The logging level.\n",
        "    \"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=log_level,\n",
        "        format='%(asctime)s %(levelname)s:%(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file),\n",
        "            logging.StreamHandler(sys.stdout)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# Define custom exceptions for clarity in error handling\n",
        "class RateLimitError(Exception):\n",
        "    \"\"\"Exception raised when hitting rate limit of API.\"\"\"\n",
        "    pass\n",
        "\n",
        "# Ensure OpenAI API key is set\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "if not openai.api_key:\n",
        "    raise ValueError(\"OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
        "\n",
        "# =================================\n",
        "# 1. Data Acquisition Module\n",
        "# =================================\n",
        "\n",
        "class DataAcquisition:\n",
        "    \"\"\"\n",
        "    Fetches data from various external sources, prioritizing primary providers and using backups as needed.\n",
        "    Enhanced to handle UN Comtrade API functionalities, including advanced parameterization, error handling, data validation,\n",
        "    concurrency for improved performance, and integration with additional trade-related APIs and political data.\n",
        "    \"\"\"\n",
        "    def __init__(self, proxy_url: Optional[str] = None):\n",
        "        # Initialize API keys from environment variables or configuration\n",
        "        self.gta_api_key = os.getenv('GTA_API_KEY')\n",
        "        self.usitc_api_key = os.getenv('USITC_API_KEY')\n",
        "        self.imf_api_key = os.getenv('IMF_API_KEY')\n",
        "        self.wto_api_key = os.getenv('WTO_API_KEY')\n",
        "        self.trading_economics_api_key = os.getenv('TRADING_ECONOMICS_API_KEY')\n",
        "        self.census_api_key = os.getenv('CENSUS_API_KEY')\n",
        "        self.comtrade_api_key = os.getenv('COMTRADE_API_KEY')\n",
        "        self.consolidated_screening_list_api_key = os.getenv('CONSOLIDATED_SCREENING_LIST_API_KEY')\n",
        "        self.newsapi_key = os.getenv('NEWSAPI_KEY')\n",
        "        self.twitter_api_key = os.getenv('TWITTER_API_KEY')\n",
        "        self.twitter_api_secret = os.getenv('TWITTER_API_SECRET')\n",
        "        self.twitter_access_token = os.getenv('TWITTER_ACCESS_TOKEN')\n",
        "        self.twitter_access_secret = os.getenv('TWITTER_ACCESS_SECRET')\n",
        "\n",
        "        # Proxy configuration if needed\n",
        "        self.proxy_url = proxy_url\n",
        "\n",
        "        # Setup session with proxies if provided\n",
        "        self.session = requests.Session()\n",
        "        if proxy_url:\n",
        "            self.session.proxies.update({\n",
        "                'http': proxy_url,\n",
        "                'https': proxy_url\n",
        "            })\n",
        "\n",
        "        # API endpoints\n",
        "        self.comtrade_data_endpoint = 'https://comtradeapi.un.org/data/v1/get'\n",
        "        self.comtrade_tariffline_endpoint = 'https://comtradeapi.un.org/data/v1/tariffline'\n",
        "        self.comtrade_bulk_endpoint = 'https://comtradeapi.un.org/bulk/v1/get'\n",
        "        self.comtrade_async_endpoint = 'https://comtradeapi.un.org/async/v1/get'\n",
        "        self.comtrade_suv_endpoint = 'https://comtradeapi.un.org/data/v1/suv'  # Corrected endpoint\n",
        "        self.comtrade_ais_endpoint = 'https://comtradeapi.un.org/experimental/v1/getAIS'\n",
        "        self.comtrade_metadata_endpoint = 'https://comtradeapi.un.org/public/v1/getMetadata'\n",
        "        self.comtrade_reference_endpoint = 'https://comtradeapi.un.org/public/v1/getReference'\n",
        "        self.comtrade_liveupdate_endpoint = 'https://comtradeapi.un.org/data/v1/getLiveUpdate'\n",
        "        self.gta_endpoint = 'https://api.globaltradealert.org/api/v1/data/'\n",
        "        self.usitc_hts_endpoint = 'https://api.usitc.gov/v1/hts'\n",
        "        self.imf_endpoint = 'https://api.imf.org/v1/data'\n",
        "        self.wto_endpoint = 'https://data.wto.org/api/'\n",
        "        self.trading_economics_endpoint = 'https://api.tradingeconomics.com/'\n",
        "        self.developer_trade_gov_endpoint = 'https://api.trade.gov/'\n",
        "        self.census_export_endpoint = 'https://api.census.gov/data/timeseries/intltrade/exports/hs'\n",
        "        self.census_import_endpoint = 'https://api.census.gov/data/timeseries/intltrade/imports/hs'\n",
        "        self.consolidated_screening_list_endpoint = 'https://api.wto.org/tfad/transparency/procedures_contacts_single_window'\n",
        "\n",
        "        # Headers for some APIs\n",
        "        self.gta_headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': f'APIKey {self.gta_api_key}'\n",
        "        }\n",
        "        self.usitc_headers = {\n",
        "            'Authorization': f'Bearer {self.usitc_api_key}'\n",
        "        }\n",
        "        self.imf_headers = {\n",
        "            'Authorization': f'Bearer {self.imf_api_key}'\n",
        "        }\n",
        "        self.wto_headers = {\n",
        "            'Authorization': f'Bearer {self.wto_api_key}'\n",
        "        }\n",
        "        self.trading_economics_headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': f'Bearer {self.trading_economics_api_key}'\n",
        "        }\n",
        "        self.developer_trade_gov_headers = {\n",
        "            'Authorization': f'Bearer {os.getenv(\"DEVELOPER_TRADE_GOV_API_KEY\")}'\n",
        "        }\n",
        "        self.census_headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        self.consolidated_screening_list_headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': f'Bearer {self.consolidated_screening_list_api_key}'\n",
        "        }\n",
        "\n",
        "        # Additional configuration parameters\n",
        "        self.default_max_records = 5000  # Default max records for partial data retrieval\n",
        "        self.default_wait_time = 1.5  # Wait time between requests to avoid rate limit issues\n",
        "\n",
        "        # Set up a session with retries for robust requests to handle network or rate limit issues\n",
        "        retries = requests.adapters.Retry(\n",
        "            total=5,\n",
        "            backoff_factor=1,\n",
        "            status_forcelist=[429, 500, 502, 503, 504],\n",
        "            allowed_methods=[\"GET\", \"POST\"]\n",
        "        )\n",
        "        adapter = requests.adapters.HTTPAdapter(max_retries=retries)\n",
        "        self.session.mount('https://', adapter)\n",
        "        self.session.mount('http://', adapter)\n",
        "\n",
        "        # Set up caching\n",
        "        requests_cache.install_cache('trade_cache', backend='sqlite', expire_after=86400)  # Cache expires after 1 day\n",
        "\n",
        "        # Initialize placeholders for data retrieval tasks\n",
        "        self.fetched_data = {}\n",
        "\n",
        "        # Define primary and backup data sources\n",
        "        self.primary_data_sources = {\n",
        "            'world_bank': self.fetch_world_bank_data,\n",
        "            'un_comtrade': self.fetch_un_comtrade_data,\n",
        "            'usitc_hts': self.fetch_usitc_hts_data,\n",
        "            'gta': self.fetch_gta_data,\n",
        "            'imf': self.fetch_imf_data,\n",
        "            'wto_api': self.fetch_wto_data,\n",
        "            'trading_economics': self.fetch_trading_economics_data,\n",
        "            'census_exports': self.fetch_census_export_data,\n",
        "            'census_imports': self.fetch_census_import_data,\n",
        "            'consolidated_screening_list': self.fetch_consolidated_screening_list_data,\n",
        "            'developer_trade_gov': self.fetch_developer_trade_gov_data,\n",
        "            # Political and media data\n",
        "            'political_data': self.fetch_all_political_data,\n",
        "            'news_data': self.fetch_all_news_data,\n",
        "            'social_media_data': self.fetch_all_social_media_data,\n",
        "            'legislation_data': self.fetch_all_legislation_data\n",
        "        }\n",
        "\n",
        "        self.backup_data_sources = {\n",
        "            'world_bank': self.fetch_world_bank_data_backup,\n",
        "            'un_comtrade': self.fetch_un_comtrade_data_backup,\n",
        "            # Define backups for other sources similarly\n",
        "        }\n",
        "\n",
        "    def schedule_data_refresh(self, interval_minutes: int = 60):\n",
        "        \"\"\"\n",
        "        Schedules periodic data refreshes to keep simulation data up-to-date.\n",
        "        \"\"\"\n",
        "        scheduler = BackgroundScheduler()\n",
        "        scheduler.add_job(self.fetch_all_data, 'interval', minutes=interval_minutes)\n",
        "        scheduler.start()\n",
        "        logging.info(f'Data refresh scheduled every {interval_minutes} minutes.')\n",
        "\n",
        "    # Placeholder backup methods\n",
        "    def fetch_world_bank_data_backup(self):\n",
        "        logging.info('Fetching World Bank data from backup source...')\n",
        "        return self.fetch_world_bank_data()\n",
        "\n",
        "    def fetch_un_comtrade_data_backup(self):\n",
        "        logging.info('Fetching UN Comtrade data from backup source...')\n",
        "        return self.fetch_un_comtrade_data()\n",
        "\n",
        "    def _handle_response_errors(self, response: requests.Response):\n",
        "        \"\"\"\n",
        "        Handles any errors in the response from external APIs and raises appropriate exceptions.\n",
        "        \"\"\"\n",
        "        if response.status_code == 400:\n",
        "            raise ValueError(\"Bad Request: The server could not understand the request.\")\n",
        "        elif response.status_code == 401:\n",
        "            raise PermissionError(\"Unauthorized: Invalid or missing API key.\")\n",
        "        elif response.status_code == 403:\n",
        "            raise PermissionError(\"Forbidden: Access to the requested resource is denied.\")\n",
        "        elif response.status_code == 404:\n",
        "            raise LookupError(\"Not Found: The requested resource could not be found.\")\n",
        "        elif response.status_code == 429:\n",
        "            raise RateLimitError(\"Too Many Requests: Rate limit exceeded.\")\n",
        "        elif response.status_code >= 500:\n",
        "            raise ConnectionError(f\"Server Error: {response.status_code} - {response.reason}\")\n",
        "        # Raise for any other unsuccessful status\n",
        "        response.raise_for_status()\n",
        "\n",
        "    def _fetch_data_concurrently(self, tasks: List[Dict[str, Any]]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Executes multiple API requests concurrently using ThreadPoolExecutor for performance optimization.\n",
        "        This method is used for fetching large datasets in parallel where tasks are splitted for each period or parameter subset.\n",
        "        \"\"\"\n",
        "        data_frames = []\n",
        "        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
        "            futures = {executor.submit(self._fetch_data_task, **task_params): task_params for task_params in tasks}\n",
        "            for future in as_completed(futures):\n",
        "                task_params = futures[future]\n",
        "                try:\n",
        "                    result_df = future.result()\n",
        "                    if result_df is not None and not result_df.empty:\n",
        "                        data_frames.append(result_df)\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error fetching data for task {task_params}: {e}\")\n",
        "        if data_frames:\n",
        "            return pd.concat(data_frames, ignore_index=True)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def _fetch_data_task(self, endpoint: str, params: Dict[str, Any], method: str = 'GET', headers: Dict[str, str] = None, body: Dict[str, Any] = None, max_records: int = None) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches data for a single task by making an API request to the specified endpoint.\n",
        "        \"\"\"\n",
        "        if not headers:\n",
        "            headers = {}\n",
        "        if self.comtrade_api_key:\n",
        "            headers.update({\"Authorization\": f\"Bearer {self.comtrade_api_key}\"})\n",
        "\n",
        "        # If max_records is not specified, use default\n",
        "        if max_records is None:\n",
        "            max_records = self.default_max_records\n",
        "\n",
        "        # Rate limiting attempt\n",
        "        time.sleep(self.default_wait_time)  # wait to respect rate limits\n",
        "        if method.upper() == 'GET':\n",
        "            response = self.session.get(endpoint, params=params, headers=headers)\n",
        "        else:\n",
        "            response = self.session.post(endpoint, json=body, headers=headers, params=params)\n",
        "        self._handle_response_errors(response)\n",
        "\n",
        "        try:\n",
        "            data = response.json()\n",
        "            if not data or 'data' not in data:\n",
        "                logging.warning(f\"No data found for parameters: {params}\")\n",
        "                return None\n",
        "            df = pd.json_normalize(data['data'])\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing response data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_comtrade_data(self,\n",
        "                            type_code: str = 'C',\n",
        "                            freq_code: str = 'A',\n",
        "                            cl_code: str = 'HS',\n",
        "                            reporter_code: Optional[str] = None,\n",
        "                            period: Optional[str] = None,\n",
        "                            partner_code: Optional[str] = None,\n",
        "                            partner2_code: Optional[str] = None,\n",
        "                            cmd_code: Optional[str] = None,\n",
        "                            flow_code: Optional[str] = None,\n",
        "                            customs_code: Optional[str] = None,\n",
        "                            mot_code: Optional[str] = None,\n",
        "                            max_records: Optional[int] = None,\n",
        "                            aggregate_by: Optional[str] = None,\n",
        "                            breakdown_mode: Optional[str] = None,\n",
        "                            include_desc: bool = True) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches final trade data from the UN Comtrade API using the standard data endpoint.\n",
        "        This method supports advanced parameterization for thorough data retrieval.\n",
        "        \"\"\"\n",
        "        logging.info('Fetching data from UN Comtrade API final data endpoint...')\n",
        "        endpoint = f'{self.comtrade_data_endpoint}/{type_code}/{freq_code}/{cl_code}'\n",
        "\n",
        "        # Build parameter dictionary for the API call\n",
        "        params = {}\n",
        "        if reporter_code:\n",
        "            params['reporterCode'] = reporter_code\n",
        "        if period:\n",
        "            params['period'] = period\n",
        "        if partner_code:\n",
        "            params['partnerCode'] = partner_code\n",
        "        if partner2_code:\n",
        "            params['partner2Code'] = partner2_code\n",
        "        if cmd_code:\n",
        "            params['cmdCode'] = cmd_code\n",
        "        if flow_code:\n",
        "            params['flowCode'] = flow_code\n",
        "        if customs_code:\n",
        "            params['customsCode'] = customs_code\n",
        "        if mot_code:\n",
        "            params['motCode'] = mot_code\n",
        "        if aggregate_by:\n",
        "            params['aggregateBy'] = aggregate_by\n",
        "        if breakdown_mode:\n",
        "            params['breakdownMode'] = breakdown_mode\n",
        "        params['includeDesc'] = str(include_desc).lower()\n",
        "\n",
        "        df = self._fetch_data_task(endpoint, params, method='GET', max_records=max_records)\n",
        "        if df is not None:\n",
        "            logging.info('UN Comtrade final data fetched successfully.')\n",
        "        return df\n",
        "\n",
        "    def fetch_comtrade_tariffline_data(self,\n",
        "                                       type_code: str = 'C',\n",
        "                                       freq_code: str = 'A',\n",
        "                                       cl_code: str = 'HS',\n",
        "                                       reporter_code: Optional[str] = None,\n",
        "                                       period: Optional[str] = None,\n",
        "                                       partner_code: Optional[str] = None,\n",
        "                                       partner2_code: Optional[str] = None,\n",
        "                                       cmd_code: Optional[str] = None,\n",
        "                                       flow_code: Optional[str] = None,\n",
        "                                       customs_code: Optional[str] = None,\n",
        "                                       mot_code: Optional[str] = None,\n",
        "                                       max_records: Optional[int] = None,\n",
        "                                       include_desc: bool = True) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches tariff line data from the UN Comtrade API's Tariffline data endpoint.\n",
        "        \"\"\"\n",
        "        logging.info('Fetching tariff line data from UN Comtrade API...')\n",
        "        endpoint = f'{self.comtrade_tariffline_endpoint}/{type_code}/{freq_code}/{cl_code}'\n",
        "\n",
        "        # Build parameter dictionary similarly as above\n",
        "        params = {}\n",
        "        if reporter_code:\n",
        "            params['reporterCode'] = reporter_code\n",
        "        if period:\n",
        "            params['period'] = period\n",
        "        if partner_code:\n",
        "            params['partnerCode'] = partner_code\n",
        "        if partner2_code:\n",
        "            params['partner2Code'] = partner2_code\n",
        "        if cmd_code:\n",
        "            params['cmdCode'] = cmd_code\n",
        "        if flow_code:\n",
        "            params['flowCode'] = flow_code\n",
        "        if customs_code:\n",
        "            params['customsCode'] = customs_code\n",
        "        if mot_code:\n",
        "            params['motCode'] = mot_code\n",
        "        params['includeDesc'] = str(include_desc).lower()\n",
        "\n",
        "        df = self._fetch_data_task(endpoint, params, method='GET', max_records=max_records)\n",
        "        if df is not None:\n",
        "            logging.info('UN Comtrade tariff line data fetched successfully.')\n",
        "        return df\n",
        "\n",
        "    def fetch_comtrade_bulk_data(self,\n",
        "                                 type_code: str,\n",
        "                                 freq_code: str,\n",
        "                                 cl_code: str,\n",
        "                                 reporter_code: Optional[str] = None,\n",
        "                                 period: Optional[str] = None,\n",
        "                                 decompress: bool = True,\n",
        "                                 published_date_from: Optional[str] = None,\n",
        "                                 published_date_to: Optional[str] = None,\n",
        "                                 directory: Optional[str] = None) -> None:\n",
        "        \"\"\"\n",
        "        Downloads bulk data files from the UN Comtrade API's bulk data endpoint.\n",
        "        This method handles large datasets by downloading the data in compressed files.\n",
        "        \"\"\"\n",
        "        logging.info('Fetching bulk data from UN Comtrade API...')\n",
        "        if not self.comtrade_api_key:\n",
        "            logging.error(\"A valid COMTRADE_API_KEY is required to download bulk data.\")\n",
        "            return\n",
        "        endpoint = f'{self.comtrade_bulk_endpoint}/{type_code}/{freq_code}/{cl_code}'\n",
        "\n",
        "        # Build parameters dictionary\n",
        "        params = {}\n",
        "        if reporter_code:\n",
        "            params['reporterCode'] = reporter_code\n",
        "        if period:\n",
        "            params['period'] = period\n",
        "        if published_date_from:\n",
        "            params['publishedDateFrom'] = published_date_from\n",
        "        if published_date_to:\n",
        "            params['publishedDateTo'] = published_date_to\n",
        "\n",
        "        # Obtain bulk data availability\n",
        "        data_availability = self._fetch_data_task(endpoint, params, method='GET')\n",
        "        if data_availability is None or data_availability.empty:\n",
        "            logging.warning(\"No bulk data available for the specified parameters.\")\n",
        "            return\n",
        "\n",
        "        # Download each file from the bulk data availability info\n",
        "        if directory is None:\n",
        "            directory = os.getcwd()\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "        for idx, row in data_availability.iterrows():\n",
        "            file_url = row.get('downloadUrl')\n",
        "            if not file_url:\n",
        "                logging.warning(f\"No file URL found for record {idx}.\")\n",
        "                continue\n",
        "            logging.info(f\"Downloading file from {file_url}...\")\n",
        "            response = self.session.get(file_url)\n",
        "            self._handle_response_errors(response)\n",
        "            content_disposition = response.headers.get('Content-Disposition')\n",
        "            if content_disposition:\n",
        "                filename = content_disposition.split('filename=')[1].strip('\"')\n",
        "            else:\n",
        "                filename = f\"bulk_data_{idx}.zip\"\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            if decompress and filename.endswith('.zip'):\n",
        "                # Decompress the file\n",
        "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(directory)\n",
        "                os.remove(file_path)\n",
        "            logging.info(f\"File {filename} downloaded and processed.\")\n",
        "\n",
        "    def fetch_comtrade_metadata(self,\n",
        "                                type_code: str,\n",
        "                                freq_code: str,\n",
        "                                cl_code: str,\n",
        "                                period: str,\n",
        "                                reporter_code: Optional[str] = None,\n",
        "                                show_history: bool = False) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches metadata and publication notes from the UN Comtrade API.\n",
        "        \"\"\"\n",
        "        logging.info('Fetching metadata from UN Comtrade API...')\n",
        "        endpoint = self.comtrade_metadata_endpoint\n",
        "\n",
        "        params = {\n",
        "            'typeCode': type_code,\n",
        "            'freqCode': freq_code,\n",
        "            'clCode': cl_code,\n",
        "            'period': period,\n",
        "            'showHistory': str(show_history).lower()\n",
        "        }\n",
        "        if reporter_code:\n",
        "            params['reporterCode'] = reporter_code\n",
        "        df = self._fetch_data_task(endpoint, params, method='GET')\n",
        "        if df is not None:\n",
        "            logging.info('Metadata fetched successfully.')\n",
        "        return df\n",
        "\n",
        "    def fetch_comtrade_suv(self,\n",
        "                           period: str,\n",
        "                           cmd_code: str,\n",
        "                           flow_code: Optional[str] = None,\n",
        "                           qty_unit_code: Optional[int] = None) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches the Standard Unit Values (SUV) from the UN Comtrade API.\n",
        "        \"\"\"\n",
        "        logging.info('Fetching Standard Unit Values (SUV) from UN Comtrade API...')\n",
        "        endpoint = self.comtrade_suv_endpoint\n",
        "\n",
        "        params = {\n",
        "            'period': period,\n",
        "            'cmdCode': cmd_code\n",
        "        }\n",
        "        if flow_code:\n",
        "            params['flowCode'] = flow_code\n",
        "        if qty_unit_code:\n",
        "            params['qtyUnitCode'] = str(qty_unit_code)\n",
        "        df = self._fetch_data_task(endpoint, params, method='GET')\n",
        "        if df is not None:\n",
        "            logging.info('SUV data fetched successfully.')\n",
        "        return df\n",
        "\n",
        "    def fetch_comtrade_ais(self,\n",
        "                           country_area_code: str,\n",
        "                           vessel_type_code: Optional[str] = None,\n",
        "                           date_from: str = '',\n",
        "                           date_to: str = '',\n",
        "                           flow_code: Optional[str] = None) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches experimental data from the AIS (ships tracking movement) from the UN Comtrade API.\n",
        "        \"\"\"\n",
        "        logging.info('Fetching AIS data from UN Comtrade API...')\n",
        "        endpoint = self.comtrade_ais_endpoint\n",
        "        if not (date_from and date_to):\n",
        "            raise ValueError(\"Both date_from and date_to are required for AIS data retrieval.\")\n",
        "\n",
        "        params = {\n",
        "            'countryAreaCode': country_area_code,\n",
        "            'dateFrom': date_from,\n",
        "            'dateTo': date_to\n",
        "        }\n",
        "        if vessel_type_code:\n",
        "            params['vesselTypeCode'] = vessel_type_code\n",
        "        if flow_code:\n",
        "            params['flowCode'] = flow_code\n",
        "        df = self._fetch_data_task(endpoint, params, method='GET')\n",
        "        if df is not None:\n",
        "            logging.info('AIS data fetched successfully.')\n",
        "        return df\n",
        "\n",
        "    def fetch_commodity_reference(self, category: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches reference data (commodity categories, country codes, etc.) from the UN Comtrade API.\n",
        "        \"\"\"\n",
        "        logging.info(f'Fetching reference data for category {category} from UN Comtrade API...')\n",
        "        endpoint = self.comtrade_reference_endpoint\n",
        "        params = {\n",
        "            'category': category\n",
        "        }\n",
        "        df = self._fetch_data_task(endpoint, params, method='GET')\n",
        "        if df is not None:\n",
        "            logging.info('Reference data fetched successfully.')\n",
        "        return df\n",
        "\n",
        "    def commodity_lookup(self, search_terms: List[str], return_code: bool = False, return_char: bool = True, update: bool = False) -> Union[List[str], Dict[str, List[str]]]:\n",
        "        \"\"\"\n",
        "        Looks up commodities based on the given search terms. If `return_code` is True, returns a list or dictionary\n",
        "        of commodity codes. If `return_char` is True, returns a list or dictionary of commodity descriptions.\n",
        "        \"\"\"\n",
        "        logging.info(f'Looking up commodities for search terms: {search_terms}')\n",
        "        # If needed, update the reference data (not implemented fully here)\n",
        "\n",
        "        # We'll assume we have a reference data set for commodity classification code \"HS\"\n",
        "        commodity_ref = self.fetch_commodity_reference('cmd:HS')\n",
        "        if commodity_ref is None or commodity_ref.empty:\n",
        "            logging.warning('No commodity reference data available to search.')\n",
        "            return []\n",
        "\n",
        "        results = {}\n",
        "        for term in search_terms:\n",
        "            term_lower = term.lower()\n",
        "            matching_entries = commodity_ref[commodity_ref['text'].str.lower().str.contains(term_lower, na=False)]\n",
        "            if return_code and 'id' in matching_entries.columns:\n",
        "                codes = matching_entries['id'].astype(str).tolist()\n",
        "                if return_char:\n",
        "                    descriptions = matching_entries['text'].tolist()\n",
        "                    combined = [f\"{c} - {d}\" for c, d in zip(codes, descriptions)]\n",
        "                    results[term] = combined\n",
        "                else:\n",
        "                    results[term] = codes\n",
        "            else:\n",
        "                # If not returning codes, just return descriptions if available.\n",
        "                results[term] = matching_entries['text'].tolist() if 'text' in matching_entries.columns else []\n",
        "\n",
        "        empty_terms = [term for term, match_list in results.items() if not match_list]\n",
        "        if empty_terms:\n",
        "            logging.warning(f'No matching results found for input terms: {empty_terms}')\n",
        "        if len(search_terms) == 1:\n",
        "            # Return a list of matches if only one term was requested\n",
        "            return results[search_terms[0]]\n",
        "        return results\n",
        "\n",
        "    # Enhanced parameterization for the Census API, with dynamic parameters\n",
        "    def fetch_census_export_data_enhanced(self, **kwargs) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Enhanced method for fetching data from the Census API using dynamic parameters.\n",
        "        \"\"\"\n",
        "        logging.info('Fetching enhanced Census export data with dynamic parameters...')\n",
        "        # We'll use fetch_census_export_data under the hood with passed kwargs.\n",
        "        return self.fetch_census_export_data(**kwargs)\n",
        "\n",
        "    def fetch_census_import_data_enhanced(self, **kwargs) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Enhanced method for fetching data from the Census API using dynamic parameters.\n",
        "        \"\"\"\n",
        "        logging.info('Fetching enhanced Census import data with dynamic parameters...')\n",
        "        # We'll use fetch_census_import_data under the hood with passed kwargs.\n",
        "        return self.fetch_census_import_data(**kwargs)\n",
        "\n",
        "    # Additional concurrency and advanced features can be implemented as needed.\n",
        "\n",
        "    # Methods to fetch data from the World Bank, USITC, GTA, IMF, WTO, Trading Economics, and others can be included as they are from earlier code or further enhanced as needed.\n",
        "\n",
        "    def fetch_world_bank_data(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches GDP and trade indicators from the World Bank API for all countries.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching data from World Bank API...')\n",
        "            countries = [country.alpha_3 for country in pycountry.countries]\n",
        "            indicators = ['NY.GDP.MKTP.CD', 'NE.TRD.GNFS.ZS']  # GDP, Trade (% of GDP)\n",
        "            data_frames = []\n",
        "            for indicator in indicators:\n",
        "                df = wb.data.DataFrame(indicator, countries, mrv=10)\n",
        "                df = df.reset_index()\n",
        "                df['Indicator'] = indicator\n",
        "                data_frames.append(df)\n",
        "            combined_df = pd.concat(data_frames, ignore_index=True)\n",
        "            logging.info('World Bank data fetched successfully.')\n",
        "            return combined_df\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching World Bank data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_un_comtrade_data(self, reporter: str = 'all', partner: str = 'all', year: int = 2021, freq: str = 'A') -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches trade data from the UN Comtrade API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching data from UN Comtrade API...')\n",
        "            url = f'https://comtradeapi.un.org/public/v1/preview/{reporter}/{partner}/{year}/HS'\n",
        "            params = {\n",
        "                'reporterCode': reporter,\n",
        "                'partnerCode': partner,\n",
        "                'period': year,\n",
        "                'classification': 'HS',\n",
        "                'typeCode': 'C',\n",
        "                'freq': freq\n",
        "            }\n",
        "            response = self.session.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                # Parse JSON to DataFrame\n",
        "                trade_data = data.get('dataset', [])\n",
        "                if not trade_data:\n",
        "                    logging.warning('No trade data found in UN Comtrade response.')\n",
        "                    return None\n",
        "                df = pd.json_normalize(trade_data)\n",
        "                logging.info('UN Comtrade data fetched successfully.')\n",
        "                return df\n",
        "            else:\n",
        "                logging.error(f'UN Comtrade API Error: {response.status_code} - {response.text}')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching UN Comtrade data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_usitc_hts_data(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches tariff classifications and rates from USITC HTS API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching data from USITC HTS API...')\n",
        "            url = f'{self.usitc_hts_endpoint}/tariffs'\n",
        "            params = {\n",
        "                'api_key': self.usitc_api_key,\n",
        "                # Add additional parameters as needed\n",
        "            }\n",
        "            response = self.session.get(url, headers=self.usitc_headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                # Assuming data is a list of tariff records\n",
        "                df = pd.json_normalize(data)\n",
        "                logging.info('USITC HTS data fetched successfully.')\n",
        "                return df\n",
        "            else:\n",
        "                logging.error(f'USITC HTS API Error: {response.status_code} - {response.text}')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching USITC HTS data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_gta_data(self, limit: int = 1000, offset: int = 0, sorting: str = '-date_announced', filters: Dict[str, Any] = {}) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches active trade measures from Global Trade Alert (GTA) API.\n",
        "        Handles pagination by using limit and offset.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching data from GTA API...')\n",
        "            url = self.gta_endpoint\n",
        "            payload = {\n",
        "                'limit': limit,\n",
        "                'offset': offset,\n",
        "                'sorting': sorting,\n",
        "                'request_data': filters\n",
        "            }\n",
        "            response = self.session.post(url, headers=self.gta_headers, json=payload)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                df = pd.json_normalize(data.get('data', []))\n",
        "                logging.info('GTA data fetched successfully.')\n",
        "                return df\n",
        "            else:\n",
        "                logging.error(f'GTA API Error: {response.status_code} - {response.text}')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching GTA data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_imf_data(self, database_id: str, params: Dict[str, Any]) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches data from the IMF API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching data from IMF API...')\n",
        "            url = f'{self.imf_endpoint}/{database_id}'\n",
        "            response = self.session.get(url, headers=self.imf_headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                # Parse JSON to DataFrame\n",
        "                df = pd.json_normalize(data.get('data', []))\n",
        "                logging.info('IMF data fetched successfully.')\n",
        "                return df\n",
        "            else:\n",
        "                logging.error(f'IMF API Error: {response.status_code} - {response.text}')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching IMF data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_wto_data(self, endpoint: str, params: Dict[str, Any]) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches trade data from WTO API as a backup.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching data from WTO API...')\n",
        "            url = f'{self.wto_endpoint}/{endpoint}'\n",
        "            response = self.session.get(url, headers=self.wto_headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                df = pd.json_normalize(data.get('results', []))\n",
        "                logging.info('WTO data fetched successfully.')\n",
        "                return df\n",
        "            else:\n",
        "                logging.error(f'WTO API Error: {response.status_code} - {response.text}')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching WTO data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_trading_economics_data(self, indicator: str, country: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches economic indicators from Trading Economics API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching data from Trading Economics API...')\n",
        "            url = f'{self.trading_economics_endpoint}/historical/country/{country}/indicator/{indicator}'\n",
        "            params = {\n",
        "                'c': self.trading_economics_api_key,\n",
        "                's': start_date,\n",
        "                'e': end_date\n",
        "            }\n",
        "            response = self.session.get(url, headers=self.trading_economics_headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                df = pd.json_normalize(data)\n",
        "                logging.info('Trading Economics data fetched successfully.')\n",
        "                return df\n",
        "            else:\n",
        "                logging.error(f'Trading Economics API Error: {response.status_code} - {response.text}')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching Trading Economics data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_census_export_data(self, year: str, month: str, comm_lvl: str = 'HS6', commodity_codes: Optional[List[str]] = None, date_range: Optional[List[str]] = None) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches export data from the Census API with dynamic parameters.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching export data from Census API...')\n",
        "            params = {\n",
        "                'get': 'E_COMMODITY,ALL_VAL_MO,ALL_VAL_YR',\n",
        "                'time': f'{year}-{month}',\n",
        "                'COMM_LVL': comm_lvl,\n",
        "                'key': self.census_api_key\n",
        "            }\n",
        "            if commodity_codes:\n",
        "                params['E_COMMODITY'] = ','.join(commodity_codes)\n",
        "            if date_range:\n",
        "                params['time'] = '-'.join(date_range)\n",
        "            response = self.session.get(self.census_export_endpoint, headers=self.census_headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                df = pd.DataFrame(data[1:], columns=data[0])\n",
        "                logging.info('Census export data fetched successfully.')\n",
        "                return df\n",
        "            else:\n",
        "                logging.error(f'Census Export API Error: {response.status_code} - {response.text}')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching Census Export data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_census_import_data(self, year: str, month: str, comm_lvl: str = 'HS6', commodity_codes: Optional[List[str]] = None, date_range: Optional[List[str]] = None) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches import data from the Census API with dynamic parameters.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching import data from Census API...')\n",
        "            params = {\n",
        "                'get': 'I_COMMODITY,GEN_VAL_MO,GEN_VAL_YR',\n",
        "                'time': f'{year}-{month}',\n",
        "                'COMM_LVL': comm_lvl,\n",
        "                'key': self.census_api_key\n",
        "            }\n",
        "            if commodity_codes:\n",
        "                params['I_COMMODITY'] = ','.join(commodity_codes)\n",
        "            if date_range:\n",
        "                params['time'] = '-'.join(date_range)\n",
        "            response = self.session.get(self.census_import_endpoint, headers=self.census_headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                df = pd.DataFrame(data[1:], columns=data[0])\n",
        "                logging.info('Census import data fetched successfully.')\n",
        "                return df\n",
        "            else:\n",
        "                logging.error(f'Census Import API Error: {response.status_code} - {response.text}')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching Census Import data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_consolidated_screening_list_data(self, countries: Optional[List[str]] = None) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches data from the WTO Trade Facilitation Agreement Database (TFAD) API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching data from Consolidated Screening List API...')\n",
        "            url = self.consolidated_screening_list_endpoint\n",
        "            params = {}\n",
        "            if countries:\n",
        "                params['countries[]'] = countries  # Assuming the API expects a list\n",
        "            response = self.session.get(url, headers=self.consolidated_screening_list_headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                df = pd.json_normalize(data.get('data', []))\n",
        "                logging.info('Consolidated Screening List data fetched successfully.')\n",
        "                return df\n",
        "            else:\n",
        "                logging.error(f'Consolidated Screening List API Error: {response.status_code} - {response.text}')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching Consolidated Screening List data: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_developer_trade_gov_data(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches data from Developer.trade.gov API as a backup.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Fetching data from Developer.trade.gov API...')\n",
        "            # Placeholder for actual API call implementation\n",
        "            # Simulate data\n",
        "            data = {\n",
        "                'Country': ['USA', 'CHN', 'DEU', 'JPN'],\n",
        "                'Trade Volume': [600e9, 500e9, 200e9, 150e9],\n",
        "                'Year': [2021] * 4\n",
        "            }\n",
        "            df = pd.DataFrame(data)\n",
        "            logging.info('Developer.trade.gov data fetched successfully.')\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching Developer.trade.gov data: {e}')\n",
        "            return None\n",
        "\n",
        "    # =================================\n",
        "    # 1.2. Political Data Fetching Methods\n",
        "    # =================================\n",
        "\n",
        "    def fetch_political_parties(self, country: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches political parties and their ideologies for a given country using the Wikipedia API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info(f'Fetching political parties for {country} from Wikipedia API...')\n",
        "            search_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "            params = {\n",
        "                'action': 'query',\n",
        "                'list': 'categorymembers',\n",
        "                'cmtitle': f'Category:Political_parties_in_{country}',\n",
        "                'cmlimit': 500,\n",
        "                'format': 'json'\n",
        "            }\n",
        "            response = self.session.get(search_url, params=params)\n",
        "            self._handle_response_errors(response)\n",
        "            data = response.json()\n",
        "            parties = data.get('query', {}).get('categorymembers', [])\n",
        "            if not parties:\n",
        "                logging.warning(f'No political parties found for {country}.')\n",
        "                return None\n",
        "            # Extract party names and potentially fetch their ideologies\n",
        "            party_names = [party['title'] for party in parties]\n",
        "            # Placeholder: Fetch ideologies (requires further implementation, possibly scraping or another API)\n",
        "            ideologies = [self.fetch_party_ideology(party) for party in party_names]\n",
        "            df = pd.DataFrame({\n",
        "                'Party': party_names,\n",
        "                'Ideology': ideologies\n",
        "            })\n",
        "            logging.info(f'Fetched {len(df)} political parties for {country}.')\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching political parties for {country}: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_party_ideology(self, party_name: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Fetches the ideology of a given political party. This can be done via Wikipedia scraping or another reliable source.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info(f'Fetching ideology for party: {party_name}')\n",
        "            # Example using Wikipedia page summary\n",
        "            search_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{party_name.replace(' ', '_')}\"\n",
        "            response = self.session.get(search_url)\n",
        "            self._handle_response_errors(response)\n",
        "            data = response.json()\n",
        "            description = data.get('extract', '').lower()\n",
        "            # Simple keyword-based ideology extraction (for demonstration purposes)\n",
        "            if 'conservative' in description:\n",
        "                return 'Conservative'\n",
        "            elif 'liberal' in description:\n",
        "                return 'Liberal'\n",
        "            elif 'socialist' in description:\n",
        "                return 'Socialist'\n",
        "            elif 'green' in description:\n",
        "                return 'Green'\n",
        "            elif 'nationalist' in description:\n",
        "                return 'Nationalist'\n",
        "            else:\n",
        "                return 'Other'\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching ideology for party {party_name}: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_upcoming_elections(self, country: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches information about upcoming elections for a given country.\n",
        "        This can be sourced from APIs like ElectionGuide or scraped from reliable websites.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info(f'Fetching upcoming elections for {country}...')\n",
        "            # Placeholder: Using Wikipedia's Upcoming Elections page\n",
        "            search_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "            params = {\n",
        "                'action': 'query',\n",
        "                'titles': 'Upcoming_elections',\n",
        "                'prop': 'extracts',\n",
        "                'explaintext': True,\n",
        "                'format': 'json'\n",
        "            }\n",
        "            response = self.session.get(search_url, params=params)\n",
        "            self._handle_response_errors(response)\n",
        "            data = response.json()\n",
        "            pages = data.get('query', {}).get('pages', {})\n",
        "            for page_id, page_data in pages.items():\n",
        "                extract = page_data.get('extract', '')\n",
        "                # Parsing logic to extract election dates (requires advanced parsing or NLP)\n",
        "                # Placeholder: Return None\n",
        "                logging.warning('Election data parsing not implemented yet.')\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching upcoming elections for {country}: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_government_policies(self, country: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches current policies of the government in a given country.\n",
        "        This can be sourced from official government APIs or policy databases.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info(f'Fetching current policies for {country}...')\n",
        "            # Placeholder: Implement actual data fetching logic\n",
        "            # For demonstration, return a dummy DataFrame\n",
        "            policies = {\n",
        "                'Policy': ['Tariff Rate Adjustment', 'Trade Agreements', 'Domestic Industry Support'],\n",
        "                'Status': ['Increasing', 'Negotiating', 'Stable']\n",
        "            }\n",
        "            df = pd.DataFrame(policies)\n",
        "            logging.info(f'Fetched {len(df)} policies for {country}.')\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching policies for {country}: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_all_political_data(self, countries: List[str]) -> Dict[str, Dict[str, pd.DataFrame]]:\n",
        "        \"\"\"\n",
        "        Fetches all political-related data for a list of countries.\n",
        "        \"\"\"\n",
        "        political_data = {}\n",
        "        for country in countries:\n",
        "            parties = self.fetch_political_parties(country)\n",
        "            elections = self.fetch_upcoming_elections(country)\n",
        "            policies = self.fetch_government_policies(country)\n",
        "            political_data[country] = {\n",
        "                'Political_Parties': parties,\n",
        "                'Upcoming_Elections': elections,\n",
        "                'Current_Policies': policies\n",
        "            }\n",
        "        return political_data\n",
        "\n",
        "    # =================================\n",
        "    # 1.3. News and Social Media Data Fetching Methods\n",
        "    # =================================\n",
        "\n",
        "    def fetch_news_data(self, country: str, query: str = 'trade policy', from_date: str = None, to_date: str = None) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches news articles related to trade policies for a given country using NewsAPI.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info(f'Fetching news data for {country}...')\n",
        "            api_key = self.newsapi_key\n",
        "            if not api_key:\n",
        "                logging.error('NEWSAPI_KEY not set in environment variables.')\n",
        "                return None\n",
        "            url = 'https://newsapi.org/v2/everything'\n",
        "            params = {\n",
        "                'q': query,\n",
        "                'apiKey': api_key,\n",
        "                'language': 'en',\n",
        "                'sortBy': 'relevance',\n",
        "                'pageSize': 100\n",
        "            }\n",
        "            if from_date:\n",
        "                params['from'] = from_date\n",
        "            if to_date:\n",
        "                params['to'] = to_date\n",
        "            response = self.session.get(url, params=params)\n",
        "            self._handle_response_errors(response)\n",
        "            data = response.json()\n",
        "            articles = data.get('articles', [])\n",
        "            if not articles:\n",
        "                logging.warning(f'No news articles found for {country}.')\n",
        "                return None\n",
        "            df = pd.json_normalize(articles)\n",
        "            logging.info(f'Fetched {len(df)} news articles for {country}.')\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching news data for {country}: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_social_media_data(self, country: str, query: str = 'trade policy', max_tweets: int = 1000) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches tweets related to trade policies for a given country using Twitter API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info(f'Fetching social media data for {country}...')\n",
        "            api_key = self.twitter_api_key\n",
        "            api_secret = self.twitter_api_secret\n",
        "            access_token = self.twitter_access_token\n",
        "            access_secret = self.twitter_access_secret\n",
        "            if not all([api_key, api_secret, access_token, access_secret]):\n",
        "                logging.error('Twitter API credentials not set in environment variables.')\n",
        "                return None\n",
        "            # Initialize Tweepy (assuming it's installed)\n",
        "            import tweepy\n",
        "            auth = tweepy.OAuth1UserHandler(api_key, api_secret, access_token, access_secret)\n",
        "            api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "            tweets = tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(max_tweets)\n",
        "            tweet_data = [{\n",
        "                'created_at': tweet.created_at,\n",
        "                'user': tweet.user.screen_name,\n",
        "                'text': tweet.full_text,\n",
        "                'retweets': tweet.retweet_count,\n",
        "                'favorites': tweet.favorite_count\n",
        "            } for tweet in tweets]\n",
        "            if not tweet_data:\n",
        "                logging.warning(f'No tweets found for {country}.')\n",
        "                return None\n",
        "            df = pd.DataFrame(tweet_data)\n",
        "            logging.info(f'Fetched {len(df)} tweets for {country}.')\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching social media data for {country}: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_all_news_data(self, countries: List[str]) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches all news data for a list of countries.\n",
        "        \"\"\"\n",
        "        news_data = {}\n",
        "        for country in countries:\n",
        "            df = self.fetch_news_data(country)\n",
        "            if df is not None:\n",
        "                news_data[country] = df\n",
        "        return news_data\n",
        "\n",
        "    def fetch_all_social_media_data(self, countries: List[str]) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches all social media data for a list of countries.\n",
        "        \"\"\"\n",
        "        social_media_data = {}\n",
        "        for country in countries:\n",
        "            df = self.fetch_social_media_data(country)\n",
        "            if df is not None:\n",
        "                social_media_data[country] = df\n",
        "        return social_media_data\n",
        "\n",
        "    # =================================\n",
        "    # 1.4. Legislative Tracking Data Fetching Methods\n",
        "    # =================================\n",
        "\n",
        "    def fetch_legislation_data(self, country: str, status: str = 'active') -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches current legislation for a given country.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info(f'Fetching legislation data for {country}...')\n",
        "            # Placeholder: Implement actual data fetching logic using specific legislative APIs\n",
        "            # For demonstration, return a dummy DataFrame\n",
        "            legislation = {\n",
        "                'Bill_Name': ['Trade Tariff Adjustment Act', 'International Trade Cooperation Act'],\n",
        "                'Status': [status, status],\n",
        "                'Date_Introduced': ['2023-01-15', '2023-03-22']\n",
        "            }\n",
        "            df = pd.DataFrame(legislation)\n",
        "            logging.info(f'Fetched {len(df)} legislation records for {country}.')\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error fetching legislation data for {country}: {e}')\n",
        "            return None\n",
        "\n",
        "    def fetch_all_legislation_data(self, countries: List[str]) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Fetches all legislative data for a list of countries.\n",
        "        \"\"\"\n",
        "        legislation_data = {}\n",
        "        for country in countries:\n",
        "            df = self.fetch_legislation_data(country)\n",
        "            if df is not None:\n",
        "                legislation_data[country] = df\n",
        "        return legislation_data\n",
        "\n",
        "    # =================================\n",
        "    # 1.5. Fetch All Data Method\n",
        "    # =================================\n",
        "\n",
        "    def fetch_all_data(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Fetches data from all primary sources, using backups if needed.\n",
        "        Utilizes parallel processing to speed up data fetching.\n",
        "        \"\"\"\n",
        "        data = {}\n",
        "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            future_to_source = {}\n",
        "            for source_name, fetch_function in self.primary_data_sources.items():\n",
        "                if source_name in ['political_data', 'news_data', 'social_media_data', 'legislation_data']:\n",
        "                    # Assuming we have a list of countries to fetch political and media data for\n",
        "                    # Using World Bank data to get the list of countries\n",
        "                    world_bank_df = self.raw_data.get('world_bank')\n",
        "                    if world_bank_df is not None and not world_bank_df.empty:\n",
        "                        countries = world_bank_df['Country'].unique().tolist()\n",
        "                    else:\n",
        "                        countries = ['USA']  # Default to USA if no data\n",
        "                    future = executor.submit(fetch_function, countries)\n",
        "                    future_to_source[future] = source_name\n",
        "                else:\n",
        "                    future = executor.submit(fetch_function)\n",
        "                    future_to_source[future] = source_name\n",
        "            for future in as_completed(future_to_source):\n",
        "                source_name = future_to_source[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    if source_name in ['political_data', 'news_data', 'social_media_data', 'legislation_data']:\n",
        "                        # result is a dict with country as key\n",
        "                        data[source_name] = result\n",
        "                    elif result is not None and not result.empty:\n",
        "                        data[source_name] = result\n",
        "                        logging.info(f'{source_name} data fetched and added to dataset.')\n",
        "                    else:\n",
        "                        # Use backup sources if primary fails\n",
        "                        backup_function = self.backup_data_sources.get(source_name)\n",
        "                        if backup_function:\n",
        "                            backup_result = backup_function()\n",
        "                            if backup_result is not None and not backup_result.empty:\n",
        "                                data[source_name] = backup_result\n",
        "                                logging.info(f'Backup for {source_name} used successfully.')\n",
        "                except Exception as e:\n",
        "                    logging.error(f'Error fetching data for {source_name}: {e}')\n",
        "        return data\n",
        "\n",
        "# =================================\n",
        "# 2. Data Processing and Storage Module\n",
        "# =================================\n",
        "\n",
        "class DataProcessing:\n",
        "    \"\"\"\n",
        "    Cleans, normalizes, and stores the fetched data for efficient retrieval and analysis.\n",
        "    Employs data validation, concurrency, and integration with a robust database for scalability.\n",
        "    \"\"\"\n",
        "    def __init__(self, data: Dict[str, Any]):\n",
        "        self.raw_data = data\n",
        "        self.processed_data = {}\n",
        "        # Initialize database connection (PostgreSQL for scalability)\n",
        "        database_url = os.getenv('DATABASE_URL', 'sqlite:///trade_data.db')\n",
        "        self.engine = create_engine(database_url)\n",
        "        self.Session = sessionmaker(bind=self.engine)\n",
        "\n",
        "        # Initialize NLP models\n",
        "        self.sentiment_analyzer: Pipeline = pipeline('sentiment-analysis')\n",
        "        try:\n",
        "            self.nlp = spacy.load('en_core_web_sm')\n",
        "        except OSError:\n",
        "            # If the model is not found, download it\n",
        "            from spacy.cli import download\n",
        "            download('en_core_web_sm')\n",
        "            self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def analyze_sentiment(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Analyzes the sentiment of a given text and returns it as 'Positive', 'Negative', or 'Neutral'.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = self.sentiment_analyzer(text[:512])[0]  # Truncate to 512 tokens\n",
        "            label = result['label']\n",
        "            if label in ['POSITIVE', 'NEGATIVE']:\n",
        "                return label.capitalize()\n",
        "            return 'Neutral'\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error analyzing sentiment: {e}')\n",
        "            return 'Neutral'\n",
        "\n",
        "    def extract_entities(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extracts entities (e.g., political figures, parties) from the text using spaCy.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            doc = self.nlp(text)\n",
        "            entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON']]\n",
        "            return entities\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error extracting entities: {e}')\n",
        "            return []\n",
        "\n",
        "    def process_news_data(self, news_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Processes news articles to extract sentiments and entities.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Processing news data for sentiment and entity extraction...')\n",
        "            news_df['Sentiment'] = news_df['description'].apply(self.analyze_sentiment)\n",
        "            news_df['Entities'] = news_df['description'].apply(self.extract_entities)\n",
        "            logging.info('News data processed successfully.')\n",
        "            return news_df\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing news data: {e}')\n",
        "            return news_df\n",
        "\n",
        "    def process_social_media_data(self, tweets_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Processes tweets to extract sentiments and entities.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Processing social media data for sentiment and entity extraction...')\n",
        "            tweets_df['Sentiment'] = tweets_df['text'].apply(self.analyze_sentiment)\n",
        "            tweets_df['Entities'] = tweets_df['text'].apply(self.extract_entities)\n",
        "            logging.info('Social media data processed successfully.')\n",
        "            return tweets_df\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing social media data: {e}')\n",
        "            return tweets_df\n",
        "\n",
        "    def process_legislation_data(self, legislation_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Processes legislation data, potentially extracting sentiments or categorizing bills.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Processing legislation data...')\n",
        "            processed_legislation = {}\n",
        "            for country, df in legislation_data.items():\n",
        "                # Example: Extract keywords or categorize bills based on names\n",
        "                df['Category'] = df['Bill_Name'].apply(lambda x: 'Trade' if 'Trade' in x else 'Other')\n",
        "                processed_legislation[country] = df\n",
        "            logging.info('Legislation data processed successfully.')\n",
        "            return processed_legislation\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing legislation data: {e}')\n",
        "            return legislation_data\n",
        "\n",
        "    def clean_political_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and processes political data, including sentiment analysis of news and social media.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning political data...')\n",
        "            for country, data in self.raw_data.get('political_data', {}).items():\n",
        "                # Process news data\n",
        "                news_df = self.raw_data.get('news_data', {}).get(country)\n",
        "                if news_df is not None and not news_df.empty:\n",
        "                    processed_news = self.process_news_data(news_df)\n",
        "                    self.processed_data[f'{country}_news'] = processed_news\n",
        "\n",
        "                # Process social media data\n",
        "                tweets_df = self.raw_data.get('social_media_data', {}).get(country)\n",
        "                if tweets_df is not None and not tweets_df.empty:\n",
        "                    processed_tweets = self.process_social_media_data(tweets_df)\n",
        "                    self.processed_data[f'{country}_tweets'] = processed_tweets\n",
        "\n",
        "                # Process legislation data\n",
        "                legislation_df = self.raw_data.get('legislation_data', {}).get(country)\n",
        "                if legislation_df is not None and not legislation_df.empty:\n",
        "                    processed_legislation = self.process_legislation_data({country: legislation_df})\n",
        "                    self.processed_data[f'{country}_Legislation'] = processed_legislation.get(country)\n",
        "\n",
        "                # Existing cleaning methods\n",
        "                political_parties = data.get('Political_Parties')\n",
        "                upcoming_elections = data.get('Upcoming_Elections')\n",
        "                current_policies = data.get('Current_Policies')\n",
        "                if political_parties is not None and not political_parties.empty:\n",
        "                    self.processed_data[f'{country}_Political_Parties'] = political_parties\n",
        "                if upcoming_elections is not None and not upcoming_elections.empty:\n",
        "                    self.processed_data[f'{country}_Upcoming_Elections'] = upcoming_elections\n",
        "                if current_policies is not None and not current_policies.empty:\n",
        "                    self.processed_data[f'{country}_Current_Policies'] = current_policies\n",
        "            logging.info('Political data cleaned and processed successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error cleaning political data: {e}')\n",
        "\n",
        "    def clean_world_bank_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes World Bank data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning World Bank data...')\n",
        "            df = self.raw_data.get('world_bank')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No World Bank data to process.')\n",
        "                return\n",
        "            df = df.rename(columns={'economy': 'Country', 'time': 'Year', 'Value': 'Value'})\n",
        "            indicator_mapping = {\n",
        "                'NY.GDP.MKTP.CD': 'GDP',\n",
        "                'NE.TRD.GNFS.ZS': 'Trade_Percentage_GDP'\n",
        "            }\n",
        "            df['Indicator'] = df['Indicator'].map(indicator_mapping)\n",
        "            df_pivot = df.pivot_table(values='Value', index=['Country', 'Year'], columns='Indicator').reset_index()\n",
        "            self.processed_data['world_bank'] = df_pivot\n",
        "            logging.info('World Bank data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing World Bank data: {e}')\n",
        "\n",
        "    def clean_un_comtrade_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes UN Comtrade data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning UN Comtrade data...')\n",
        "            df = self.raw_data.get('un_comtrade')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No UN Comtrade data to process.')\n",
        "                return\n",
        "            # Example cleaning steps for UN Comtrade data structure\n",
        "            df = df.rename(columns={\n",
        "                'Reporter': 'Reporter',\n",
        "                'Partner': 'Partner',\n",
        "                'Commodity Code': 'Commodity_Code',\n",
        "                'Trade Value': 'Trade_Value',\n",
        "                'Year': 'Year'\n",
        "            })\n",
        "            df['Trade_Value'] = pd.to_numeric(df['Trade_Value'], errors='coerce')\n",
        "            df.dropna(subset=['Trade_Value'], inplace=True)\n",
        "            self.processed_data['un_comtrade'] = df\n",
        "            logging.info('UN Comtrade data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing UN Comtrade data: {e}')\n",
        "\n",
        "    def clean_usitc_hts_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes USITC HTS data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning USITC HTS data...')\n",
        "            df = self.raw_data.get('usitc_hts')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No USITC HTS data to process.')\n",
        "                return\n",
        "            df = df.rename(columns={\n",
        "                'HTS Code': 'HTS_Code',\n",
        "                'Description': 'Description',\n",
        "                'Tariff Rate': 'Tariff_Rate'\n",
        "            })\n",
        "            df['Tariff_Rate'] = pd.to_numeric(df['Tariff_Rate'], errors='coerce')\n",
        "            df.dropna(subset=['Tariff_Rate'], inplace=True)\n",
        "            self.processed_data['usitc_hts'] = df\n",
        "            logging.info('USITC HTS data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing USITC HTS data: {e}')\n",
        "\n",
        "    def clean_gta_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes GTA data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning GTA data...')\n",
        "            df = self.raw_data.get('gta')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No GTA data to process.')\n",
        "                return\n",
        "            # Example cleaning steps for GTA data structure\n",
        "            df = df.rename(columns={\n",
        "                'intervention_id': 'Intervention_ID',\n",
        "                'state_act_id': 'State_Act_ID',\n",
        "                'state_act_title': 'State_Act_Title',\n",
        "                'intervention_url': 'Intervention_URL',\n",
        "                'state_act_url': 'State_Act_URL',\n",
        "                'gta_evaluation': 'GTA_Evaluation',\n",
        "                'date_announced': 'Date_Announced',\n",
        "                'date_published': 'Date_Published',\n",
        "                'date_implemented': 'Date_Implemented',\n",
        "                'date_removed': 'Date_Removed',\n",
        "                'is_in_force': 'Is_In_Force'\n",
        "            })\n",
        "            # Convert date columns to datetime type for uniformity and ease of analysis\n",
        "            date_columns = ['Date_Announced', 'Date_Published', 'Date_Implemented', 'Date_Removed']\n",
        "            for col in date_columns:\n",
        "                if col in df.columns:\n",
        "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "            self.processed_data['gta'] = df\n",
        "            logging.info('GTA data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing GTA data: {e}')\n",
        "\n",
        "    def clean_imf_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes IMF data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning IMF data...')\n",
        "            df = self.raw_data.get('imf')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No IMF data to process.')\n",
        "                return\n",
        "            # Example cleaning steps for IMF data structure\n",
        "            df = df.rename(columns={\n",
        "                'Country': 'Country',\n",
        "                'Indicator': 'Indicator',\n",
        "                'Year': 'Year',\n",
        "                'Value': 'Value'\n",
        "            })\n",
        "            df['Value'] = pd.to_numeric(df['Value'], errors='coerce')\n",
        "            df.dropna(subset=['Value'], inplace=True)\n",
        "            self.processed_data['imf'] = df\n",
        "            logging.info('IMF data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing IMF data: {e}')\n",
        "\n",
        "    def clean_trading_economics_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes Trading Economics data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning Trading Economics data...')\n",
        "            df = self.raw_data.get('trading_economics')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No Trading Economics data to process.')\n",
        "                return\n",
        "            # Example cleaning steps for Trading Economics data structure\n",
        "            df = df.rename(columns={\n",
        "                'date': 'Date',\n",
        "                'value': 'Value',\n",
        "                'symbol': 'Symbol',\n",
        "                'country': 'Country'\n",
        "            })\n",
        "            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "            df['Value'] = pd.to_numeric(df['Value'], errors='coerce')\n",
        "            df.dropna(subset=['Value'], inplace=True)\n",
        "            self.processed_data['trading_economics'] = df\n",
        "            logging.info('Trading Economics data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing Trading Economics data: {e}')\n",
        "\n",
        "    def clean_census_export_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes Census export data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning Census Export data...')\n",
        "            df = self.raw_data.get('census_exports')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No Census Export data to process.')\n",
        "                return\n",
        "            df = df.rename(columns={\n",
        "                'E_COMMODITY': 'Commodity_Code',\n",
        "                'ALL_VAL_MO': 'All_Val_Month',\n",
        "                'ALL_VAL_YR': 'All_Val_Year'\n",
        "            })\n",
        "            df['All_Val_Month'] = pd.to_numeric(df['All_Val_Month'], errors='coerce')\n",
        "            df['All_Val_Year'] = pd.to_numeric(df['All_Val_Year'], errors='coerce')\n",
        "            df.dropna(subset=['All_Val_Month', 'All_Val_Year'], inplace=True)\n",
        "            self.processed_data['census_exports'] = df\n",
        "            logging.info('Census Export data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing Census Export data: {e}')\n",
        "\n",
        "    def clean_census_import_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes Census import data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning Census Import data...')\n",
        "            df = self.raw_data.get('census_imports')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No Census Import data to process.')\n",
        "                return\n",
        "            df = df.rename(columns={\n",
        "                'I_COMMODITY': 'Commodity_Code',\n",
        "                'GEN_VAL_MO': 'General_Val_Month',\n",
        "                'GEN_VAL_YR': 'General_Val_Year'\n",
        "            })\n",
        "            df['General_Val_Month'] = pd.to_numeric(df['General_Val_Month'], errors='coerce')\n",
        "            df['General_Val_Year'] = pd.to_numeric(df['General_Val_Year'], errors='coerce')\n",
        "            df.dropna(subset=['General_Val_Month', 'General_Val_Year'], inplace=True)\n",
        "            self.processed_data['census_imports'] = df\n",
        "            logging.info('Census Import data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing Census Import data: {e}')\n",
        "\n",
        "    def clean_consolidated_screening_list_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes Consolidated Screening List data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning Consolidated Screening List data...')\n",
        "            df = self.raw_data.get('consolidated_screening_list')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No Consolidated Screening List data to process.')\n",
        "                return\n",
        "            # Example cleaning steps\n",
        "            df = df.rename(columns={\n",
        "                'id': 'ID',\n",
        "                'country_name': 'Country_Name',\n",
        "                'country_code': 'Country_Code',\n",
        "                'trade_vol': 'Trade_Volume',\n",
        "                # Adjust as needed based on actual data structure from the API response\n",
        "            })\n",
        "            df['Trade_Volume'] = pd.to_numeric(df['Trade_Volume'], errors='coerce')\n",
        "            df.dropna(subset=['Trade_Volume'], inplace=True)\n",
        "            self.processed_data['consolidated_screening_list'] = df\n",
        "            logging.info('Consolidated Screening List data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing Consolidated Screening List data: {e}')\n",
        "\n",
        "    def clean_wto_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes WTO data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning WTO data...')\n",
        "            df = self.raw_data.get('wto_api')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No WTO data to process.')\n",
        "                return\n",
        "            # Example cleaning steps if the WTO data structure is known (placeholder)\n",
        "            self.processed_data['wto'] = df\n",
        "            logging.info('WTO data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing WTO data: {e}')\n",
        "\n",
        "    def clean_developer_trade_gov_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and normalizes data from Developer.trade.gov API.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Cleaning Developer.trade.gov data...')\n",
        "            df = self.raw_data.get('developer_trade_gov')\n",
        "            if df is None or df.empty:\n",
        "                logging.warning('No Developer.trade.gov data to process.')\n",
        "                return\n",
        "            # Example cleaning steps as placeholders for actual data structure from Developer.trade.gov API\n",
        "            df = df.rename(columns={\n",
        "                'Country': 'Country',\n",
        "                'Trade Volume': 'Trade_Volume',\n",
        "                'Year': 'Year'\n",
        "            })\n",
        "            df['Trade_Volume'] = pd.to_numeric(df['Trade_Volume'], errors='coerce')\n",
        "            df.dropna(subset=['Trade_Volume'], inplace=True)\n",
        "            self.processed_data['developer_trade_gov'] = df\n",
        "            logging.info('Developer.trade.gov data cleaned successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error processing Developer.trade_gov data: {e}')\n",
        "\n",
        "    def clean_all_data(self):\n",
        "        \"\"\"\n",
        "        Cleans and processes all data, including trade, political, news, and social media data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Starting comprehensive data cleaning and processing...')\n",
        "            # Clean trade-related data\n",
        "            self.clean_world_bank_data()\n",
        "            self.clean_un_comtrade_data()\n",
        "            self.clean_usitc_hts_data()\n",
        "            self.clean_gta_data()\n",
        "            self.clean_imf_data()\n",
        "            self.clean_trading_economics_data()\n",
        "            self.clean_census_export_data()\n",
        "            self.clean_census_import_data()\n",
        "            self.clean_consolidated_screening_list_data()\n",
        "            self.clean_wto_data()\n",
        "            self.clean_developer_trade_gov_data()\n",
        "\n",
        "            # Clean political and media-related data\n",
        "            self.clean_political_data()\n",
        "\n",
        "            logging.info('All data cleaned and processed successfully.')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error during comprehensive data cleaning: {e}')\n",
        "\n",
        "    def save_to_database(self):\n",
        "        \"\"\"\n",
        "        Saves processed data to the database, ensuring data integrity and logging relevant issues.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logging.info('Saving data to the database...')\n",
        "            for table_name, df in self.processed_data.items():\n",
        "                # Data validation: remove duplicates, missing values, etc.\n",
        "                if df.duplicated().any():\n",
        "                    df = df.drop_duplicates()\n",
        "                    logging.warning(f'Duplicates found and removed in {table_name} data.')\n",
        "                if df.isnull().values.any():\n",
        "                    df = df.dropna()\n",
        "                    logging.warning(f'Missing values found and removed in {table_name} data.')\n",
        "\n",
        "                df.to_sql(table_name, self.engine, if_exists='replace', index=False)\n",
        "                logging.info(f'Data saved to table: {table_name}')\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error saving data to database: {e}')\n",
        "\n",
        "    def process_all_data(self):\n",
        "        \"\"\"\n",
        "        Orchestrates the data cleaning and processing tasks for all data sources concurrently.\n",
        "        Uses concurrency for efficient data processing and stores the final cleaned data in a database.\n",
        "        \"\"\"\n",
        "        tasks = [\n",
        "            {'func': self.clean_world_bank_data},\n",
        "            {'func': self.clean_un_comtrade_data},\n",
        "            {'func': self.clean_usitc_hts_data},\n",
        "            {'func': self.clean_gta_data},\n",
        "            {'func': self.clean_imf_data},\n",
        "            {'func': self.clean_trading_economics_data},\n",
        "            {'func': self.clean_census_export_data},\n",
        "            {'func': self.clean_census_import_data},\n",
        "            {'func': self.clean_consolidated_screening_list_data},\n",
        "            {'func': self.clean_wto_data},\n",
        "            {'func': self.clean_developer_trade_gov_data},\n",
        "            {'func': self.clean_political_data},  # New cleaning task\n",
        "        ]\n",
        "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            futures = [executor.submit(task['func']) for task in tasks]\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    future.result()\n",
        "                except Exception as e:\n",
        "                    logging.error(f'Error in data processing task: {e}')\n",
        "        self.save_to_database()\n",
        "\n",
        "# =================================\n",
        "# 3. Agent-Based Modeling Module\n",
        "# =================================\n",
        "\n",
        "class GovernmentAgent(Agent):\n",
        "    \"\"\"\n",
        "    Government agents represent countries and their economic policies with advanced economic strategies and communications.\n",
        "    \"\"\"\n",
        "    def __init__(self, unique_id, model, country_code, economic_indicators, political_parties: Optional[pd.DataFrame], upcoming_elections: Optional[pd.DataFrame], current_policies: Optional[pd.DataFrame]):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.country_code = country_code\n",
        "        self.economic_indicators = economic_indicators  # e.g., GDP, trade balance, etc.\n",
        "        self.policies = {'Tariff_Rate': 0}  # Initialize with default tariff rate\n",
        "        self.trade_alliances = set()  # Countries this government has alliances with\n",
        "\n",
        "        # New Attributes\n",
        "        self.political_parties = political_parties  # DataFrame with Party and Ideology\n",
        "        self.upcoming_elections = upcoming_elections  # DataFrame with election details\n",
        "        self.current_policies = current_policies  # DataFrame with current policy statuses\n",
        "        self.governing_party = self.determine_governing_party()\n",
        "        self.policy_shift_probability = self.determine_policy_shift_probability()\n",
        "\n",
        "        # Attach news and social media data if available\n",
        "        self.news_data = self.model.processed_data.get(f'{self.country_code}_news')\n",
        "        self.tweets_data = self.model.processed_data.get(f'{self.country_code}_tweets')\n",
        "        self.legislation_data = self.model.processed_data.get(f'{self.country_code}_Legislation')\n",
        "\n",
        "    def determine_governing_party(self) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Determines the current governing party based on existing data.\n",
        "        \"\"\"\n",
        "        # Placeholder: Assume the first party is governing; implement actual logic based on data\n",
        "        if self.political_parties is not None and not self.political_parties.empty:\n",
        "            return self.political_parties.iloc[0]['Party']\n",
        "        return None\n",
        "\n",
        "    def determine_policy_shift_probability(self) -> float:\n",
        "        \"\"\"\n",
        "        Determines the probability of a policy shift based on factors like upcoming elections and economic indicators.\n",
        "        \"\"\"\n",
        "        base_probability = 0.05  # Base 5% chance of policy shift each step\n",
        "        # Increase probability if an upcoming election is near\n",
        "        if self.upcoming_elections is not None and not self.upcoming_elections.empty:\n",
        "            # Placeholder: Assuming 'Date_Upcoming_Election' column exists\n",
        "            # Here, we use the first upcoming election date\n",
        "            election_date = pd.to_datetime(self.upcoming_elections.iloc[0].get('Date_Upcoming_Election', pd.NaT))\n",
        "            if pd.notna(election_date):\n",
        "                days_until_election = (election_date - pd.Timestamp.today()).days\n",
        "                if days_until_election <= 180:  # 6 months\n",
        "                    base_probability += 0.10  # Additional 10% chance\n",
        "        # Modify based on economic indicators\n",
        "        gdp_growth = self.economic_indicators.get('GDP_Growth', 0)\n",
        "        if gdp_growth < 0:\n",
        "            base_probability += 0.05  # Additional 5% chance to shift policies during economic downturn\n",
        "        return min(base_probability, 1.0)\n",
        "\n",
        "    def implement_policy(self):\n",
        "        \"\"\"\n",
        "        Decides and implements policies based on economic indicators, political ideology, public sentiment, and other factors.\n",
        "        Includes dynamic tariff adjustments, responding to trade balances, and domestic industries protection.\n",
        "        \"\"\"\n",
        "        # Incorporate political ideology into policy decisions\n",
        "        ideology = self.political_parties[self.political_parties['Party'] == self.governing_party]['Ideology'].values\n",
        "        ideology = ideology[0] if len(ideology) > 0 else 'Other'\n",
        "\n",
        "        trade_percentage = self.economic_indicators.get('Trade_Percentage_GDP', 0)\n",
        "\n",
        "        # Analyze public sentiment from news and social media\n",
        "        public_sentiment = self.calculate_public_sentiment()\n",
        "\n",
        "        # Policy logic influenced by ideology and public sentiment\n",
        "        if ideology == 'Conservative':\n",
        "            if trade_percentage < 20 or public_sentiment == 'Negative':\n",
        "                self.policies['Tariff_Rate'] = min(self.policies.get('Tariff_Rate', 0) + 1, 25)\n",
        "                logging.info(f'Government {self.country_code} (Conservative): Increasing tariff rate to {self.policies[\"Tariff_Rate\"]}% due to low trade percentage or negative public sentiment.')\n",
        "            elif trade_percentage > 40 and public_sentiment == 'Positive':\n",
        "                self.policies['Tariff_Rate'] = max(self.policies.get('Tariff_Rate', 0) - 1, 0)\n",
        "                logging.info(f'Government {self.country_code} (Conservative): Decreasing tariff rate to {self.policies[\"Tariff_Rate\"]}% due to high trade percentage and positive public sentiment.')\n",
        "            else:\n",
        "                logging.info(f'Government {self.country_code} (Conservative): Maintaining tariff rate at {self.policies[\"Tariff_Rate\"]}%.')\n",
        "        elif ideology == 'Liberal':\n",
        "            if trade_percentage < 20 or public_sentiment == 'Negative':\n",
        "                self.policies['Tariff_Rate'] = min(self.policies.get('Tariff_Rate', 0) + 0.5, 20)\n",
        "                logging.info(f'Government {self.country_code} (Liberal): Slightly increasing tariff rate to {self.policies[\"Tariff_Rate\"]}% due to low trade percentage or negative public sentiment.')\n",
        "            elif trade_percentage > 40 and public_sentiment == 'Positive':\n",
        "                self.policies['Tariff_Rate'] = max(self.policies.get('Tariff_Rate', 0) - 2, 0)\n",
        "                logging.info(f'Government {self.country_code} (Liberal): Decreasing tariff rate to {self.policies[\"Tariff_Rate\"]}% due to high trade percentage and positive public sentiment.')\n",
        "            else:\n",
        "                logging.info(f'Government {self.country_code} (Liberal): Maintaining tariff rate at {self.policies[\"Tariff_Rate\"]}%.')\n",
        "        elif ideology == 'Mixed':\n",
        "            if trade_percentage < 20 or public_sentiment == 'Negative':\n",
        "                self.policies['Tariff_Rate'] = min(self.policies.get('Tariff_Rate', 0) + 0.5, 20)\n",
        "                logging.info(f'Government {self.country_code} (Mixed): Increasing tariff rate to {self.policies[\"Tariff_Rate\"]}% due to low trade percentage or negative public sentiment.')\n",
        "            elif trade_percentage > 40 and public_sentiment == 'Positive':\n",
        "                self.policies['Tariff_Rate'] = max(self.policies.get('Tariff_Rate', 0) - 1.5, 0)\n",
        "                logging.info(f'Government {self.country_code} (Mixed): Decreasing tariff rate to {self.policies[\"Tariff_Rate\"]}% due to high trade percentage and positive public sentiment.')\n",
        "            else:\n",
        "                logging.info(f'Government {self.country_code} (Mixed): Maintaining tariff rate at {self.policies[\"Tariff_Rate\"]}%.')\n",
        "        else:\n",
        "            # Default policy logic\n",
        "            if trade_percentage < 20 or public_sentiment == 'Negative':\n",
        "                self.policies['Tariff_Rate'] = min(self.policies.get('Tariff_Rate', 0) + 1, 25)\n",
        "                logging.info(f'Government {self.country_code} (Other): Increasing tariff rate to {self.policies[\"Tariff_Rate\"]}% due to low trade percentage or negative public sentiment.')\n",
        "            elif trade_percentage > 40 and public_sentiment == 'Positive':\n",
        "                self.policies['Tariff_Rate'] = max(self.policies.get('Tariff_Rate', 0) - 1, 0)\n",
        "                logging.info(f'Government {self.country_code} (Other): Decreasing tariff rate to {self.policies[\"Tariff_Rate\"]}% due to high trade percentage and positive public sentiment.')\n",
        "            else:\n",
        "                logging.info(f'Government {self.country_code} (Other): Maintaining tariff rate at {self.policies[\"Tariff_Rate\"]}%.')\n",
        "\n",
        "        # Implement policy shifts based on probability\n",
        "        if random.random() < self.policy_shift_probability:\n",
        "            self.shift_policy(ideology)\n",
        "\n",
        "    def shift_policy(self, ideology: str):\n",
        "        \"\"\"\n",
        "        Shifts policies based on political ideology and other factors.\n",
        "        \"\"\"\n",
        "        logging.info(f'Government {self.country_code}: Shifting policies based on ideology {ideology}.')\n",
        "        if ideology == 'Conservative':\n",
        "            # Implement conservative policy shifts, e.g., further tariffs, trade protection\n",
        "            old_tariff = self.policies['Tariff_Rate']\n",
        "            self.policies['Tariff_Rate'] = min(self.policies['Tariff_Rate'] + 1, 30)\n",
        "            logging.info(f'Government {self.country_code} (Conservative): Further increasing tariff rate from {old_tariff}% to {self.policies[\"Tariff_Rate\"]}%.')\n",
        "        elif ideology == 'Liberal':\n",
        "            # Implement liberal policy shifts, e.g., reducing tariffs, promoting free trade\n",
        "            old_tariff = self.policies['Tariff_Rate']\n",
        "            self.policies['Tariff_Rate'] = max(self.policies['Tariff_Rate'] - 2, 0)\n",
        "            logging.info(f'Government {self.country_code} (Liberal): Further decreasing tariff rate from {old_tariff}% to {self.policies[\"Tariff_Rate\"]}%.')\n",
        "        elif ideology == 'Mixed':\n",
        "            # Implement mixed policy shifts\n",
        "            old_tariff = self.policies['Tariff_Rate']\n",
        "            self.policies['Tariff_Rate'] = max(self.policies['Tariff_Rate'] - 1, 0)\n",
        "            logging.info(f'Government {self.country_code} (Mixed): Decreasing tariff rate from {old_tariff}% to {self.policies[\"Tariff_Rate\"]}%.')\n",
        "        else:\n",
        "            # Default or other ideologies\n",
        "            old_tariff = self.policies['Tariff_Rate']\n",
        "            self.policies['Tariff_Rate'] = max(self.policies['Tariff_Rate'] - 1, 0)\n",
        "            logging.info(f'Government {self.country_code} (Other): Decreasing tariff rate from {old_tariff}% to {self.policies[\"Tariff_Rate\"]}%.')\n",
        "\n",
        "    def implement_trade_agreements(self):\n",
        "        \"\"\"\n",
        "        Implements or renegotiates trade agreements based on current policies and alliances.\n",
        "        \"\"\"\n",
        "        # Placeholder for advanced trade agreement logic based on policies and alliances\n",
        "        pass\n",
        "\n",
        "    def calculate_public_sentiment(self) -> str:\n",
        "        \"\"\"\n",
        "        Calculates the overall public sentiment based on news and social media data.\n",
        "        Returns 'Positive', 'Negative', or 'Neutral'.\n",
        "        \"\"\"\n",
        "        sentiments = []\n",
        "        # Analyze news sentiment\n",
        "        if self.news_data is not None and not self.news_data.empty:\n",
        "            sentiments += self.news_data['Sentiment'].tolist()\n",
        "        # Analyze social media sentiment\n",
        "        if self.tweets_data is not None and not self.tweets_data.empty:\n",
        "            sentiments += self.tweets_data['Sentiment'].tolist()\n",
        "        if not sentiments:\n",
        "            return 'Neutral'\n",
        "        # Calculate the most common sentiment\n",
        "        sentiment_counts = pd.Series(sentiments).value_counts()\n",
        "        dominant_sentiment = sentiment_counts.idxmax()\n",
        "        return dominant_sentiment\n",
        "\n",
        "    def implement_policy_shift_logic(self, decision_df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Implements decisions from the decision logic DataFrame.\n",
        "        \"\"\"\n",
        "        for _, decision in decision_df.iterrows():\n",
        "            trigger_event = decision.get('Trigger_Event')\n",
        "            # Evaluate the trigger event; this requires defining how trigger events are represented\n",
        "            # For example, trigger_event could be a condition based on model state\n",
        "            # Here, we'll assume trigger_event is a Python expression as a string\n",
        "            try:\n",
        "                # Using eval is dangerous; ensure that trigger_event is sanitized and controlled\n",
        "                if eval(trigger_event, {\"model\": self.model, \"agent\": self}):\n",
        "                    self.execute_decision(decision)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error evaluating trigger event '{trigger_event}' for decision '{decision.get('Decision_ID')}': {e}\")\n",
        "\n",
        "    def execute_decision(self, decision):\n",
        "        \"\"\"\n",
        "        Executes the decision by updating policies or performing actions.\n",
        "\n",
        "        Args:\n",
        "            decision (pd.Series): A row from the decision logic DataFrame.\n",
        "        \"\"\"\n",
        "        decision_id = decision.get('Decision_ID')\n",
        "        description = decision.get('Description')\n",
        "        expected_outcome = decision.get('Expected_Outcome')\n",
        "        implementation_date = decision.get('Implementation_Date')\n",
        "\n",
        "        # Example implementation: Update tariff rate based on decision description\n",
        "        if 'increase tariff' in description.lower():\n",
        "            old_tariff = self.policies.get('Tariff_Rate', 0)\n",
        "            self.policies['Tariff_Rate'] = min(old_tariff + 1, 25)  # Increment tariff rate\n",
        "            logging.info(f\"Decision {decision_id}: {description} - Tariff rate increased from {old_tariff}% to {self.policies['Tariff_Rate']}%.\")\n",
        "        elif 'decrease tariff' in description.lower():\n",
        "            old_tariff = self.policies.get('Tariff_Rate', 0)\n",
        "            self.policies['Tariff_Rate'] = max(old_tariff - 1, 0)  # Decrement tariff rate\n",
        "            logging.info(f\"Decision {decision_id}: {description} - Tariff rate decreased from {old_tariff}% to {self.policies['Tariff_Rate']}%.\")\n",
        "        # Add more decision execution logic as needed\n",
        "\n",
        "    def implement_decisions(self, decision_df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Implements decisions from the decision logic DataFrame based on trigger events.\n",
        "        \"\"\"\n",
        "        self.implement_policy_shift_logic(decision_df)\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Defines the agent's behavior each simulation step, including policy implementation and reacting to events.\n",
        "        \"\"\"\n",
        "        self.implement_policy()\n",
        "        self.implement_trade_agreements()\n",
        "        # Implement decisions from decision logic\n",
        "        decision_df = self.model.decision_logics.get((self.country_code, self.governing_party))\n",
        "        if decision_df is not None and not decision_df.empty:\n",
        "            self.implement_decisions(decision_df)\n",
        "\n",
        "        # Check for upcoming elections within the prediction horizon (e.g., 6 months)\n",
        "        if self.upcoming_elections is not None and not self.upcoming_elections.empty:\n",
        "            election_date = pd.to_datetime(self.upcoming_elections.iloc[0].get('Date_Upcoming_Election', pd.NaT))\n",
        "            if pd.notna(election_date):\n",
        "                days_until_election = (election_date - pd.Timestamp.today()).days\n",
        "                if 0 < days_until_election <= 180:\n",
        "                    self.policy_shift_probability += 0.05  # Increase chance of policy shift as election approaches\n",
        "                    logging.info(f'Government {self.country_code}: Election upcoming in {days_until_election} days. Increased policy shift probability.')\n",
        "\n",
        "        # Ensure probability doesn't exceed 1\n",
        "        self.policy_shift_probability = min(self.policy_shift_probability, 1.0)\n",
        "\n",
        "        # Randomly decide to form a trade alliance or negotiate each step.\n",
        "        if random.random() < 0.05:  # 5% chance each step to form alliance or negotiate.\n",
        "            gov_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code != self.country_code]\n",
        "            if gov_agents:\n",
        "                partner = random.choice(gov_agents)\n",
        "                self.negotiate_trade_agreement(partner)\n",
        "\n",
        "        # Respond to random economic shocks, e.g., 1% chance.\n",
        "        if random.random() < 0.01:\n",
        "            shock = random.choice(['recession', 'boom'])\n",
        "            self.respond_to_shock(shock)\n",
        "\n",
        "    def negotiate_trade_agreement(self, partner_agent: 'GovernmentAgent'):\n",
        "        \"\"\"\n",
        "        Negotiates a trade agreement with another government agent, potentially reducing tariffs if both partners agree.\n",
        "        \"\"\"\n",
        "        # Example simple negotiation logic: if both countries have relatively low tariff rates, reduce further and form alliance.\n",
        "        if self.policies['Tariff_Rate'] < 10 and partner_agent.policies['Tariff_Rate'] < 10:\n",
        "            old_tariff_self = self.policies['Tariff_Rate']\n",
        "            old_tariff_partner = partner_agent.policies['Tariff_Rate']\n",
        "            self.policies['Tariff_Rate'] = max(self.policies['Tariff_Rate'] - 1, 0)\n",
        "            partner_agent.policies['Tariff_Rate'] = max(partner_agent.policies['Tariff_Rate'] - 1, 0)\n",
        "            self.form_trade_alliance(partner_agent.country_code)\n",
        "            partner_agent.form_trade_alliance(self.country_code)\n",
        "            logging.info(f'Government {self.country_code} and {partner_agent.country_code}: Negotiated a trade agreement. Tariff rates reduced from {old_tariff_self}% to {self.policies[\"Tariff_Rate\"]}% and from {old_tariff_partner}% to {partner_agent.policies[\"Tariff_Rate\"]}%.')\n",
        "\n",
        "    def respond_to_shock(self, shock_type: str):\n",
        "        \"\"\"\n",
        "        Responds to global economic shocks like recessions or booms by adjusting domestic policies.\n",
        "        \"\"\"\n",
        "        if shock_type == 'recession':\n",
        "            # Implement expansionary policy to stimulate economy, e.g., reduce tariffs to boost trade or invest in domestic sectors.\n",
        "            old_tariff = self.policies.get('Tariff_Rate', 0)\n",
        "            self.policies['Tariff_Rate'] = max(old_tariff - 2, 0)\n",
        "            logging.info(f'Government {self.country_code}: Responding to recession by decreasing tariff rate from {old_tariff}% to {self.policies[\"Tariff_Rate\"]}%.')\n",
        "        elif shock_type == 'boom':\n",
        "            # Implement contractionary policy to cool down economy, e.g., increase tariffs to protect domestic industries from overheating.\n",
        "            old_tariff = self.policies.get('Tariff_Rate', 0)\n",
        "            self.policies['Tariff_Rate'] = min(old_tariff + 2, 30)\n",
        "            logging.info(f'Government {self.country_code}: Responding to boom by increasing tariff rate from {old_tariff}% to {self.policies[\"Tariff_Rate\"]}%.')\n",
        "\n",
        "    def form_trade_alliance(self, partner_country_code: str):\n",
        "        \"\"\"\n",
        "        Forms a trade alliance with another country or modifies policy if the alliance already exists.\n",
        "        \"\"\"\n",
        "        if partner_country_code not in self.trade_alliances:\n",
        "            self.trade_alliances.add(partner_country_code)\n",
        "            logging.info(f'Government {self.country_code}: Formed a new trade alliance with {partner_country_code}.')\n",
        "        else:\n",
        "            # The alliance already exists, possibly strengthen it or apply advanced logic if needed.\n",
        "            logging.info(f'Government {self.country_code}: Already in trade alliance with {partner_country_code}, strengthening alliance.')\n",
        "\n",
        "class CompanyAgent(Agent):\n",
        "    \"\"\"\n",
        "    Company agents represent businesses in different industry sectors, with advanced strategies such as investing in industries, adjusting production, and supply chain management.\n",
        "    \"\"\"\n",
        "    def __init__(self, unique_id, model, name, industry_sector, country_code, financial_data):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.name = name\n",
        "        self.industry_sector = industry_sector\n",
        "        self.country_code = country_code\n",
        "        self.financial_data = financial_data\n",
        "        self.production_capacity = financial_data.get('Production_Capacity', 100)\n",
        "        self.supply_chain = []  # List of supplier company agents or countries where inputs are sourced from\n",
        "        self.financial_health = financial_data.get('Financial_Health', 100)\n",
        "        self.demand_preferences = {'domestic': 0.5, 'imported': 0.5}  # Simplified preference for domestic vs. imported goods if relevant\n",
        "\n",
        "    def make_sourcing_decision(self):\n",
        "        \"\"\"\n",
        "        Adjust sourcing decisions based on government policies, such as tariffs, and market conditions.\n",
        "        This simulates supply chain shifts in response to changing trade policies or other economic conditions.\n",
        "        \"\"\"\n",
        "        government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == self.country_code]\n",
        "        if government_agents:\n",
        "            tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)\n",
        "            if tariff_rate > 10:\n",
        "                # If tariffs are high, the company might increase local sourcing to avoid import costs.\n",
        "                self.demand_preferences['domestic'] += 0.1\n",
        "                self.demand_preferences['imported'] = max(self.demand_preferences['imported'] - 0.1, 0)\n",
        "                logging.info(f'Company {self.name} in {self.country_code}: Adjusting sourcing towards domestic suppliers due to high tariffs.')\n",
        "            else:\n",
        "                # If tariffs are low, the company might maintain or even increase foreign sourcing for cheaper imports.\n",
        "                self.demand_preferences['imported'] += 0.05\n",
        "                self.demand_preferences['domestic'] = max(self.demand_preferences['domestic'] - 0.05, 0)\n",
        "                logging.info(f'Company {self.name} in {self.country_code}: Adjusting sourcing towards imported goods due to low tariffs.')\n",
        "\n",
        "    def adjust_production(self):\n",
        "        \"\"\"\n",
        "        Adjust production capacity based on demand, supply chain status, and financial health.\n",
        "        \"\"\"\n",
        "        # Placeholder logic: production capacity changes in response to simulated demand.\n",
        "        demand = self.model.compute_demand(self.industry_sector)\n",
        "        old_capacity = self.production_capacity\n",
        "        if demand > self.production_capacity:\n",
        "            self.production_capacity += 10  # Increase capacity if demand is high.\n",
        "            logging.info(f'Company {self.name} in {self.country_code}: Increased production capacity from {old_capacity} to {self.production_capacity} due to rising demand.')\n",
        "        elif demand < self.production_capacity * 0.8:\n",
        "            self.production_capacity = max(self.production_capacity - 10, 50)  # Decrease capacity if demand is low.\n",
        "            logging.info(f'Company {self.name} in {self.country_code}: Decreased production capacity from {old_capacity} to {self.production_capacity} due to falling demand.')\n",
        "\n",
        "    def invest_in_industry(self, investment_amount: float):\n",
        "        \"\"\"\n",
        "        Invest in industry to improve the company's financial health or production capacity.\n",
        "        This simulates decisions like expanding factories, training employees, or R&D spending.\n",
        "        \"\"\"\n",
        "        old_financial_health = self.financial_health\n",
        "        old_capacity = self.production_capacity\n",
        "        self.financial_health += investment_amount * 0.1  # Example investment effect on financial health.\n",
        "        self.production_capacity += investment_amount * 0.05  # Example effect on production capacity.\n",
        "        logging.info(f'Company {self.name} in {self.country_code}: Invested {investment_amount}, financial health improved from {old_financial_health} to {self.financial_health} and capacity from {old_capacity} to {self.production_capacity}.')\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Defines the agent's behavior at each step, including sourcing decisions, production adjustments, and potential investments.\n",
        "        \"\"\"\n",
        "        self.make_sourcing_decision()\n",
        "        self.adjust_production()\n",
        "        # Example: Random investment decisions to simulate growth or adaptation strategies.\n",
        "        if random.random() < 0.02:  # 2% chance each step to invest.\n",
        "            investment = random.uniform(1000, 5000)\n",
        "            self.invest_in_industry(investment)\n",
        "\n",
        "class ConsumerAgent(Agent):\n",
        "    \"\"\"\n",
        "    Consumer agents represent individual or aggregate consumer behavior with advanced preferences and responses to policies.\n",
        "    \"\"\"\n",
        "    def __init__(self, unique_id, model, income_level, country_code):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.income_level = income_level  # 'Low', 'Medium', 'High' income category for simplified model.\n",
        "        self.country_code = country_code\n",
        "        self.demand_preferences = self.generate_preferences()  # Preferences for goods based on income level and policy.\n",
        "\n",
        "    def generate_preferences(self):\n",
        "        \"\"\"\n",
        "        Generates demand preferences based on income level. Higher income consumers prefer more luxury goods.\n",
        "        \"\"\"\n",
        "        if self.income_level == 'Low':\n",
        "            return {'essential_goods': 0.7, 'luxury_goods': 0.3}\n",
        "        elif self.income_level == 'Medium':\n",
        "            return {'essential_goods': 0.5, 'luxury_goods': 0.5}\n",
        "        else:  # High income consumers prefer more luxury goods.\n",
        "            return {'essential_goods': 0.3, 'luxury_goods': 0.7}\n",
        "\n",
        "    def adjust_consumption(self):\n",
        "        \"\"\"\n",
        "        Adjusts consumption behavior based on changing prices, income, and government policies like tariffs.\n",
        "        If tariffs are high on imported goods, consumers might buy more domestic goods.\n",
        "        \"\"\"\n",
        "        government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == self.country_code]\n",
        "        if government_agents:\n",
        "            tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)\n",
        "            if tariff_rate > 10:\n",
        "                # If tariffs on imported goods are high, reduce luxury goods consumption (assuming many are imported)\n",
        "                old_luxury_pref = self.demand_preferences['luxury_goods']\n",
        "                self.demand_preferences['luxury_goods'] *= 0.9  # reduce consumption of expensive imported luxuries.\n",
        "                logging.info(f'Consumer in {self.country_code}: Decreasing consumption of imported luxury goods from preference {old_luxury_pref} to {self.demand_preferences[\"luxury_goods\"]} due to high tariffs.')\n",
        "            else:\n",
        "                # If tariffs are low, possibly maintain or increase luxury goods consumption.\n",
        "                old_luxury_pref = self.demand_preferences['luxury_goods']\n",
        "                self.demand_preferences['luxury_goods'] *= 1.05\n",
        "                logging.info(f'Consumer in {self.country_code}: Slightly increasing consumption of luxury goods from preference {old_luxury_pref} to {self.demand_preferences[\"luxury_goods\"]} due to low tariffs.')\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Defines the agent's behavior at each step, adjusting consumption preferences if needed.\n",
        "        \"\"\"\n",
        "        self.adjust_consumption()\n",
        "\n",
        "class IntermediaryAgent(Agent):\n",
        "    \"\"\"\n",
        "    Intermediary agents represent logistics providers, financial institutions, or other services influencing trade.\n",
        "    They can provide services such as improving supply chain efficiency or offering financing solutions.\n",
        "    \"\"\"\n",
        "    def __init__(self, unique_id, model, service_type):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.service_type = service_type  # 'Logistics', 'Finance', etc.\n",
        "\n",
        "    def provide_service(self):\n",
        "        \"\"\"\n",
        "        Provides services to other agents based on demand, possibly affecting overall trade efficiency and costs.\n",
        "        \"\"\"\n",
        "        # Placeholder service provision logic. For example, logistics agents could reduce shipping times, finance agents could reduce financial constraints.\n",
        "        pass\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Defines the agent's behavior at each step. This can include adjusting fees or expanding services as needed.\n",
        "        \"\"\"\n",
        "        self.provide_service()\n",
        "\n",
        "class ConsumerAgent(Agent):\n",
        "    \"\"\"\n",
        "    Consumer agents represent individual or aggregate consumer behavior with advanced preferences and responses to policies.\n",
        "    \"\"\"\n",
        "    def __init__(self, unique_id, model, income_level, country_code):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.income_level = income_level  # 'Low', 'Medium', 'High' income category for simplified model.\n",
        "        self.country_code = country_code\n",
        "        self.demand_preferences = self.generate_preferences()  # Preferences for goods based on income level and policy.\n",
        "\n",
        "    def generate_preferences(self):\n",
        "        \"\"\"\n",
        "        Generates demand preferences based on income level. Higher income consumers prefer more luxury goods.\n",
        "        \"\"\"\n",
        "        if self.income_level == 'Low':\n",
        "            return {'essential_goods': 0.7, 'luxury_goods': 0.3}\n",
        "        elif self.income_level == 'Medium':\n",
        "            return {'essential_goods': 0.5, 'luxury_goods': 0.5}\n",
        "        else:  # High income consumers prefer more luxury goods.\n",
        "            return {'essential_goods': 0.3, 'luxury_goods': 0.7}\n",
        "\n",
        "    def adjust_consumption(self):\n",
        "        \"\"\"\n",
        "        Adjusts consumption behavior based on changing prices, income, and government policies like tariffs.\n",
        "        If tariffs are high on imported goods, consumers might buy more domestic goods.\n",
        "        \"\"\"\n",
        "        government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == self.country_code]\n",
        "        if government_agents:\n",
        "            tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)\n",
        "            if tariff_rate > 10:\n",
        "                # If tariffs on imported goods are high, reduce luxury goods consumption (assuming many are imported)\n",
        "                old_luxury_pref = self.demand_preferences['luxury_goods']\n",
        "                self.demand_preferences['luxury_goods'] *= 0.9  # reduce consumption of expensive imported luxuries.\n",
        "                logging.info(f'Consumer in {self.country_code}: Decreasing consumption of imported luxury goods from preference {old_luxury_pref} to {self.demand_preferences[\"luxury_goods\"]} due to high tariffs.')\n",
        "            else:\n",
        "                # If tariffs are low, possibly maintain or increase luxury goods consumption.\n",
        "                old_luxury_pref = self.demand_preferences['luxury_goods']\n",
        "                self.demand_preferences['luxury_goods'] *= 1.05\n",
        "                logging.info(f'Consumer in {self.country_code}: Slightly increasing consumption of luxury goods from preference {old_luxury_pref} to {self.demand_preferences[\"luxury_goods\"]} due to low tariffs.')\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Defines the agent's behavior at each step, adjusting consumption preferences if needed.\n",
        "        \"\"\"\n",
        "        self.adjust_consumption()\n",
        "\n",
        "class TradeModel(Model):\n",
        "    \"\"\"\n",
        "    The overall model that includes all agents and manages their interactions. Now enhanced to incorporate\n",
        "    advanced behaviors, data-driven initialization, and complex interactions between agents.\n",
        "    \"\"\"\n",
        "    def __init__(self, processed_data: Dict[str, pd.DataFrame], political_data: Dict[str, Dict[str, pd.DataFrame]], news_data: Dict[str, pd.DataFrame], social_media_data: Dict[str, pd.DataFrame], legislation_data: Dict[str, pd.DataFrame]):\n",
        "        super().__init__()\n",
        "        self.schedule = RandomActivation(self)\n",
        "        self.processed_data = processed_data  # Processed data from the DataProcessing module\n",
        "        self.political_data = political_data  # Political data fetched separately\n",
        "        self.news_data = news_data  # News data fetched separately\n",
        "        self.social_media_data = social_media_data  # Social media data fetched separately\n",
        "        self.legislation_data = legislation_data  # Legislation data fetched separately\n",
        "        self.datacollector = DataCollector(model_reporters={\"Trade_Volume\": self.compute_trade_volume})\n",
        "\n",
        "        # Assuming processed_data includes data from the World Bank to obtain a list of countries\n",
        "        country_codes = self.processed_data.get('world_bank', pd.DataFrame()).get('Country', []).unique()\n",
        "        if not isinstance(country_codes, np.ndarray):\n",
        "            country_codes = []\n",
        "        self.country_codes = country_codes\n",
        "        self.create_agents()\n",
        "        self.running = True\n",
        "        self.trade_volume_history = []\n",
        "\n",
        "        # Placeholder for decision logics\n",
        "        self.decision_logics = {}  # To be filled after GPT-4 integration\n",
        "\n",
        "        # Placeholder for future events\n",
        "        self.future_events = []  # List to store future events\n",
        "\n",
        "    def create_agents(self):\n",
        "        \"\"\"\n",
        "        Creates agents for the model, including government, company, consumer, and intermediary agents,\n",
        "        leveraging processed data to initialize their states.\n",
        "        \"\"\"\n",
        "        # Create government agents for all countries present in processed data or default set if data is unavailable.\n",
        "        for i, country_code in enumerate(self.country_codes):\n",
        "            economic_indicators = self.get_economic_indicators(country_code)\n",
        "            political_info = self.political_data.get(country_code, {})\n",
        "            political_parties = political_info.get('Political_Parties')\n",
        "            upcoming_elections = political_info.get('Upcoming_Elections')\n",
        "            current_policies = political_info.get('Current_Policies')\n",
        "            news = self.news_data.get(country_code)\n",
        "            tweets = self.social_media_data.get(country_code)\n",
        "            legislation = self.legislation_data.get(country_code)\n",
        "\n",
        "            government_agent = GovernmentAgent(\n",
        "                unique_id=i,\n",
        "                model=self,\n",
        "                country_code=country_code,\n",
        "                economic_indicators=economic_indicators,\n",
        "                political_parties=political_parties,\n",
        "                upcoming_elections=upcoming_elections,\n",
        "                current_policies=current_policies\n",
        "            )\n",
        "            # Attach news and social media data to the agent\n",
        "            government_agent.news_data = news\n",
        "            government_agent.tweets_data = tweets\n",
        "            # Attach legislation data if needed\n",
        "            government_agent.legislation_data = legislation\n",
        "            self.schedule.add(government_agent)\n",
        "\n",
        "        # Create company agents using hypothetical data or processed data. This can be data-driven if real data is available.\n",
        "        top_companies = self.get_top_companies()\n",
        "        for i, company in enumerate(top_companies, start=len(self.country_codes)):\n",
        "            company_agent = CompanyAgent(i, self, company['name'], company['sector'], company['country_code'], company['financial_data'])\n",
        "            self.schedule.add(company_agent)\n",
        "\n",
        "        # Create consumer agents representing populations. This can be scaled by population data, if available.\n",
        "        num_consumers = 100  # Adjust based on complexity and performance considerations\n",
        "        for i in range(len(self.country_codes) + len(top_companies), len(self.country_codes) + len(top_companies) + num_consumers):\n",
        "            income_level = random.choice(['Low', 'Medium', 'High'])\n",
        "            country_code = random.choice(self.country_codes) if len(self.country_codes) > 0 else 'USA'\n",
        "            consumer_agent = ConsumerAgent(i, self, income_level, country_code)\n",
        "            self.schedule.add(consumer_agent)\n",
        "\n",
        "        # Create intermediary agents like logistics providers, banks. Scale the number and roles as needed.\n",
        "        service_types = ['Logistics', 'Finance']\n",
        "        num_intermediaries = 50  # Can be adjusted based on performance\n",
        "        start_id = len(self.country_codes) + len(top_companies) + num_consumers\n",
        "        for i in range(start_id, start_id + num_intermediaries):\n",
        "            service_type = random.choice(service_types)\n",
        "            intermediary_agent = IntermediaryAgent(i, self, service_type)\n",
        "            self.schedule.add(intermediary_agent)\n",
        "\n",
        "    # Helper functions for economic indicators and top companies\n",
        "\n",
        "    def get_economic_indicators(self, country_code: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Retrieves economic indicators for a given country from processed data.\n",
        "        This can include GDP, trade balance, trade percentage, etc., using the processed World Bank data.\n",
        "        \"\"\"\n",
        "        df = self.processed_data.get('world_bank', pd.DataFrame())\n",
        "        if df.empty:\n",
        "            # Return default economic indicators if no data is available\n",
        "            return {'GDP': 0, 'Trade_Percentage_GDP': 0, 'GDP_Growth': 0}\n",
        "        country_data = df[df['Country'] == country_code]\n",
        "        if country_data.empty:\n",
        "            # Return default if no data for the given country is found\n",
        "            return {'GDP': 0, 'Trade_Percentage_GDP': 0, 'GDP_Growth': 0}\n",
        "        # Use the latest data row for economic indicators\n",
        "        latest_year = country_data['Year'].max()\n",
        "        latest_data = country_data[country_data['Year'] == latest_year].iloc[0].to_dict()\n",
        "        # Placeholder: Calculate GDP_Growth based on previous year data if available\n",
        "        previous_year = latest_year - 1\n",
        "        previous_data = country_data[country_data['Year'] == previous_year]\n",
        "        if not previous_data.empty:\n",
        "            previous_gdp = previous_data.iloc[0].get('GDP', 0)\n",
        "            current_gdp = latest_data.get('GDP', 0)\n",
        "            gdp_growth = ((current_gdp - previous_gdp) / previous_gdp) * 100 if previous_gdp != 0 else 0\n",
        "        else:\n",
        "            gdp_growth = 0\n",
        "        latest_data['GDP_Growth'] = gdp_growth\n",
        "        return latest_data\n",
        "\n",
        "    def get_top_companies(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Retrieves top companies data, either from real data if available or uses placeholder data for the simulation.\n",
        "        In a real scenario, this can be data-driven from processed data sources or an API.\n",
        "        \"\"\"\n",
        "        # Placeholder list of companies\n",
        "        companies = [\n",
        "            {\n",
        "                'name': 'CompanyA',\n",
        "                'sector': 'Manufacturing',\n",
        "                'country_code': 'USA',\n",
        "                'financial_data': {'Production_Capacity': 500, 'Financial_Health': 90}\n",
        "            },\n",
        "            {\n",
        "                'name': 'CompanyB',\n",
        "                'sector': 'Technology',\n",
        "                'country_code': 'CHN',\n",
        "                'financial_data': {'Production_Capacity': 300, 'Financial_Health': 85}\n",
        "            },\n",
        "            {\n",
        "                'name': 'CompanyC',\n",
        "                'sector': 'Automotive',\n",
        "                'country_code': 'DEU',\n",
        "                'financial_data': {'Production_Capacity': 400, 'Financial_Health': 88}\n",
        "            },\n",
        "            {\n",
        "                'name': 'CompanyD',\n",
        "                'sector': 'Electronics',\n",
        "                'country_code': 'JPN',\n",
        "                'financial_data': {'Production_Capacity': 350, 'Financial_Health': 92}\n",
        "            },\n",
        "            # Additional companies can be added with more data if needed\n",
        "        ]\n",
        "        return companies\n",
        "\n",
        "    def compute_trade_volume(self) -> float:\n",
        "        \"\"\"\n",
        "        Computes the total trade volume in the model based on agent interactions.\n",
        "        This can be extended to actually compute interactions between companies and government policies.\n",
        "        For demonstration, we'll simulate it with random values.\n",
        "        \"\"\"\n",
        "        # Placeholder for actual computation based on agent interactions\n",
        "        # Suppose trade volume is influenced by the average tariffs and the number of trade alliances\n",
        "        government_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgent)]\n",
        "        if not government_agents:\n",
        "            # If no government agents, return a default simulated value\n",
        "            return random.uniform(1e6, 1e9)\n",
        "        # Example: Compute some measure of trade volume from agents\n",
        "        average_tariff = np.mean([agent.policies['Tariff_Rate'] for agent in government_agents])\n",
        "        total_alliances = sum([len(agent.trade_alliances) for agent in government_agents])\n",
        "        # We'll simulate a relationship, e.g., higher alliances and lower tariffs = higher trade volume\n",
        "        trade_volume = (1e7 * len(government_agents)) * (1 + total_alliances / 100) * (1 - average_tariff / 100)\n",
        "        # Add randomness\n",
        "        trade_volume *= random.uniform(0.8, 1.2)\n",
        "        return trade_volume\n",
        "\n",
        "    def compute_demand(self, industry_sector: str) -> float:\n",
        "        \"\"\"\n",
        "        Computes demand for a given industry sector. In a real scenario, this can be data-driven from processed data.\n",
        "        For demonstration, we simulate a random demand influenced by sector performance.\n",
        "        \"\"\"\n",
        "        # Placeholder for demand computation logic - can be extended with actual data\n",
        "        base_demand = random.uniform(100, 1000)\n",
        "        # Adjust demand based on sector. Suppose each sector gets some multiplier\n",
        "        sector_multipliers = {\n",
        "            'Manufacturing': 1.1,\n",
        "            'Technology': 1.3,\n",
        "            'Automotive': 1.2,\n",
        "            'Electronics': 1.15\n",
        "        }\n",
        "        multiplier = sector_multipliers.get(industry_sector, 1.0)\n",
        "        # Additional factors could include overall economic indicators\n",
        "        # For simplicity, just multiplying base demand with sector multiplier\n",
        "        demand = base_demand * multiplier\n",
        "        return demand\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Advances the model by one step, allowing agents to act and computing trade volume and other metrics.\n",
        "        \"\"\"\n",
        "        self.schedule.step()\n",
        "        # Collect metrics\n",
        "        self.datacollector.collect(self)\n",
        "        trade_volume = self.compute_trade_volume()\n",
        "        self.trade_volume_history.append(trade_volume)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Resets the model for a new simulation run, clearing any previous state and data.\n",
        "        \"\"\"\n",
        "        self.schedule = RandomActivation(self)\n",
        "        self.datacollector = DataCollector(model_reporters={\"Trade_Volume\": self.compute_trade_volume})\n",
        "        self.create_agents()\n",
        "        self.trade_volume_history = []\n",
        "\n",
        "# =================================\n",
        "# 4. GPT-4 Integration Module\n",
        "# =================================\n",
        "\n",
        "def gather_context_for_party(processed_data, country_code, party_name):\n",
        "    \"\"\"\n",
        "    Gathers relevant data for a specific party within a country to create context for GPT-4.\n",
        "\n",
        "    Args:\n",
        "        processed_data (dict): The dictionary containing all processed data.\n",
        "        country_code (str): The ISO alpha-3 country code.\n",
        "        party_name (str): The name of the political party.\n",
        "\n",
        "    Returns:\n",
        "        str: A JSON-formatted string containing all relevant data.\n",
        "    \"\"\"\n",
        "    country_political_data = processed_data['political_data'].get(country_code, {})\n",
        "    political_parties = country_political_data.get('Political_Parties', pd.DataFrame())\n",
        "    upcoming_elections = country_political_data.get('Upcoming_Elections', pd.DataFrame())\n",
        "    current_policies = country_political_data.get('Current_Policies', pd.DataFrame())\n",
        "\n",
        "    news_df = processed_data['news_data'].get(country_code, pd.DataFrame())\n",
        "    social_media_df = processed_data['social_media_data'].get(country_code, pd.DataFrame())\n",
        "    legislation_df = processed_data['legislation_data'].get(country_code, pd.DataFrame())\n",
        "\n",
        "    # Extract party ideology\n",
        "    party_info = political_parties[political_parties['Party'] == party_name]\n",
        "    ideology = party_info['Ideology'].iloc[0] if not party_info.empty else 'Other'\n",
        "\n",
        "    # Compile recent news and social media sentiments\n",
        "    recent_news = news_df['Sentiment'].value_counts().to_dict() if not news_df.empty else {}\n",
        "    recent_social_media = social_media_df['Sentiment'].value_counts().to_dict() if not social_media_df.empty else {}\n",
        "\n",
        "    # Compile current policies\n",
        "    policies = current_policies.to_dict(orient='records') if not current_policies.empty else []\n",
        "\n",
        "    # Compile legislation\n",
        "    legislation = legislation_df.to_dict(orient='records') if not legislation_df.empty else []\n",
        "\n",
        "    # Compile upcoming elections\n",
        "    elections = upcoming_elections.to_dict(orient='records') if not upcoming_elections.empty else []\n",
        "\n",
        "    context = {\n",
        "        'Country': country_code,\n",
        "        'Party': party_name,\n",
        "        'Ideology': ideology,\n",
        "        'Recent_News_Sentiments': recent_news,\n",
        "        'Recent_Social_Media_Sentiments': recent_social_media,\n",
        "        'Current_Policies': policies,\n",
        "        'Legislation': legislation,\n",
        "        'Upcoming_Elections': elections\n",
        "    }\n",
        "\n",
        "    return json.dumps(context, indent=2)\n",
        "\n",
        "def generate_decision_logic(context_json):\n",
        "    \"\"\"\n",
        "    Generates decision logic for a political party using GPT-4 based on the provided context.\n",
        "\n",
        "    Args:\n",
        "        context_json (str): JSON-formatted string containing all relevant data for the party.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated decision logic in CSV or code format.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert political strategist. Based on the following context, generate a set of decision logics that the political party might undertake in the next 6 months. The output should be in CSV format with the following columns: Decision_ID, Description, Trigger_Event, Expected_Outcome, Implementation_Date.\n",
        "\n",
        "    Context:\n",
        "    {context_json}\n",
        "\n",
        "    Ensure that the decisions are unique to the party's ideology and current political landscape. The decision logics should account for various scenarios and potential events that could occur during the 6-month forecasting period.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates decision logics for political parties.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=1500,  # Adjust based on needs and token limits\n",
        "            temperature=0.7,\n",
        "            n=1,\n",
        "            stop=None\n",
        "        )\n",
        "        decision_logic = response.choices[0].message['content']\n",
        "        return decision_logic.strip()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating decision logic: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def parse_decision_logic(decision_logic_str):\n",
        "    \"\"\"\n",
        "    Parses the decision logic string into a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        decision_logic_str (str): The decision logic in CSV format.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The parsed decision logic.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use StringIO to read the CSV string\n",
        "        decision_df = pd.read_csv(io.StringIO(decision_logic_str))\n",
        "        return decision_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error parsing decision logic CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def save_decision_logic(decision_df, country_code, party_name, output_dir='decision_logics'):\n",
        "    \"\"\"\n",
        "    Saves the decision logic DataFrame as a CSV file.\n",
        "\n",
        "    Args:\n",
        "        decision_df (pd.DataFrame): The decision logic DataFrame.\n",
        "        country_code (str): The ISO alpha-3 country code.\n",
        "        party_name (str): The name of the political party.\n",
        "        output_dir (str): Directory to save the CSV files.\n",
        "    \"\"\"\n",
        "    if decision_df.empty:\n",
        "        logging.warning(f\"No decision logic to save for {party_name} in {country_code}.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    filename = f\"{country_code}_{party_name.replace(' ', '_')}_decision_logic.csv\"\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    decision_df.to_csv(filepath, index=False)\n",
        "    logging.info(f\"Decision logic saved to {filepath}.\")\n",
        "\n",
        "def load_decision_logics(decision_logic_dir='decision_logics'):\n",
        "    \"\"\"\n",
        "    Loads all decision logic CSV files from the specified directory.\n",
        "\n",
        "    Args:\n",
        "        decision_logic_dir (str): Directory containing the decision logic CSV files.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with keys as (country_code, party_name) and values as DataFrames.\n",
        "    \"\"\"\n",
        "    decision_logics = {}\n",
        "    for filename in os.listdir(decision_logic_dir):\n",
        "        if filename.endswith('_decision_logic.csv'):\n",
        "            parts = filename.split('_')\n",
        "            country_code = parts[0]\n",
        "            party_name = '_'.join(parts[1:-2])  # Adjust based on filename structure\n",
        "            filepath = os.path.join(decision_logic_dir, filename)\n",
        "            df = pd.read_csv(filepath)\n",
        "            decision_logics[(country_code, party_name)] = df\n",
        "            logging.info(f\"Loaded decision logic for {party_name} in {country_code} from {filepath}.\")\n",
        "    return decision_logics\n",
        "\n",
        "# =================================\n",
        "# 5. Synthesis of Future News and Social Media Posts\n",
        "# =================================\n",
        "\n",
        "    def generate_future_news(context_json, forecast_period='6 months'):\n",
        "        \"\"\"\n",
        "        Generates plausible future news articles that could impact the simulation.\n",
        "\n",
        "        Args:\n",
        "            context_json (str): JSON-formatted string containing all relevant data for the country/party.\n",
        "            forecast_period (str): The forecasting period (e.g., '6 months').\n",
        "\n",
        "        Returns:\n",
        "            list: A list of generated news articles as strings.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are a journalist tasked with writing plausible future news articles that could impact the political and economic landscape of the following context within the next {forecast_period}.\n",
        "\n",
        "        Context:\n",
        "        {context_json}\n",
        "\n",
        "        Generate 5 news articles with headlines and brief descriptions.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a creative and insightful assistant that generates realistic future news articles based on provided context.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=2000,  # Adjust based on needs\n",
        "                temperature=0.7,\n",
        "                n=1,\n",
        "                stop=None\n",
        "            )\n",
        "            articles = response.choices[0].message['content']\n",
        "            # Split articles assuming they are separated by newlines or numbering\n",
        "            # This might require more sophisticated parsing based on GPT-4's output format\n",
        "            article_list = [article.strip() for article in articles.split('\\n') if article.strip()]\n",
        "            return article_list\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error generating future news: {e}\")\n",
        "            return []\n",
        "\n",
        "    def generate_future_social_media(context_json, forecast_period='6 months'):\n",
        "        \"\"\"\n",
        "        Generates plausible future social media posts that could impact the simulation.\n",
        "\n",
        "        Args:\n",
        "            context_json (str): JSON-formatted string containing all relevant data for the country/party.\n",
        "            forecast_period (str): The forecasting period (e.g., '6 months').\n",
        "\n",
        "        Returns:\n",
        "            list: A list of generated social media posts as strings.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are a social media manager creating plausible future social media posts that reflect public sentiment and events within the next {forecast_period}.\n",
        "\n",
        "        Context:\n",
        "        {context_json}\n",
        "\n",
        "        Generate 10 social media posts (e.g., tweets) that could influence or reflect the public's view on trade policies.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a creative assistant that generates realistic future social media posts based on provided context.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=1500,  # Adjust based on needs\n",
        "                temperature=0.7,\n",
        "                n=1,\n",
        "                stop=None\n",
        "            )\n",
        "            posts = response.choices[0].message['content']\n",
        "            # Split posts assuming they are separated by newlines or numbering\n",
        "            # This might require more sophisticated parsing based on GPT-4's output format\n",
        "            post_list = [post.strip() for post in posts.split('\\n') if post.strip()]\n",
        "            return post_list\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error generating future social media posts: {e}\")\n",
        "            return []\n",
        "\n",
        "    def integrate_future_events(model, country_code, party_name, future_news, future_posts):\n",
        "        \"\"\"\n",
        "        Integrates future news and social media posts into the simulation model.\n",
        "\n",
        "        Args:\n",
        "            model (TradeModel): The simulation model instance.\n",
        "            country_code (str): The ISO alpha-3 country code.\n",
        "            party_name (str): The name of the political party.\n",
        "            future_news (list): List of future news articles.\n",
        "            future_posts (list): List of future social media posts.\n",
        "        \"\"\"\n",
        "        # Example: Schedule future events based on the forecast period\n",
        "        for news in future_news:\n",
        "            # Assign an implementation date within the next 6 months\n",
        "            implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))\n",
        "            # Add to model's event queue or relevant data structures\n",
        "            model.add_future_news(country_code, party_name, news, implementation_date)\n",
        "\n",
        "        for post in future_posts:\n",
        "            implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))\n",
        "            model.add_future_social_media(country_code, party_name, post, implementation_date)\n",
        "\n",
        "    # =================================\n",
        "    # 6. Simulation Model Enhancements\n",
        "    # =================================\n",
        "\n",
        "    def generate_and_assign_decision_logics(processed_data, model, decision_logic_dir='decision_logics'):\n",
        "        \"\"\"\n",
        "        Generates decision logics for all parties in all countries and assigns them to the simulation model.\n",
        "\n",
        "        Args:\n",
        "            processed_data (dict): The dictionary containing all processed data.\n",
        "            model (TradeModel): The simulation model instance.\n",
        "            decision_logic_dir (str): Directory to save the decision logic CSV files.\n",
        "        \"\"\"\n",
        "        decision_logics = {}\n",
        "        for country_code, political_info in processed_data['political_data'].items():\n",
        "            parties_df = political_info.get('Political_Parties', pd.DataFrame())\n",
        "            for _, party_row in parties_df.iterrows():\n",
        "                party_name = party_row['Party']\n",
        "                context_json = gather_context_for_party(processed_data, country_code, party_name)\n",
        "                decision_logic_csv = generate_decision_logic(context_json)\n",
        "                decision_logic_df = parse_decision_logic(decision_logic_csv)\n",
        "                save_decision_logic(decision_logic_df, country_code, party_name, decision_logic_dir)\n",
        "                decision_logics[(country_code, party_name)] = decision_logic_df\n",
        "\n",
        "                # Optionally, generate future events\n",
        "                future_news = generate_future_news(context_json)\n",
        "                future_posts = generate_future_social_media(context_json)\n",
        "                integrate_future_events(model, country_code, party_name, future_news, future_posts)\n",
        "\n",
        "        return decision_logics\n",
        "\n",
        "    # Enhance the TradeModel to handle future events\n",
        "    class TradeModelEnhanced(TradeModel):\n",
        "        \"\"\"\n",
        "        Enhanced TradeModel that can handle future news and social media events.\n",
        "        \"\"\"\n",
        "        def __init__(self, processed_data: Dict[str, pd.DataFrame], political_data: Dict[str, Dict[str, pd.DataFrame]], news_data: Dict[str, pd.DataFrame], social_media_data: Dict[str, pd.DataFrame], legislation_data: Dict[str, pd.DataFrame]):\n",
        "            super().__init__(processed_data, political_data, news_data, social_media_data, legislation_data)\n",
        "            self.future_news = []  # List of tuples: (country_code, party_name, news, implementation_date)\n",
        "            self.future_social_media = []  # List of tuples: (country_code, party_name, post, implementation_date)\n",
        "\n",
        "        def add_future_news(self, country_code, party_name, news, implementation_date):\n",
        "            \"\"\"\n",
        "            Adds a future news event to the model.\n",
        "\n",
        "            Args:\n",
        "                country_code (str): ISO alpha-3 country code.\n",
        "                party_name (str): Name of the political party.\n",
        "                news (str): News article content.\n",
        "                implementation_date (pd.Timestamp): Date when the news becomes active.\n",
        "            \"\"\"\n",
        "            self.future_news.append((country_code, party_name, news, implementation_date))\n",
        "            logging.info(f\"Scheduled future news for {party_name} in {country_code} on {implementation_date.date()}: {news[:60]}...\")\n",
        "\n",
        "        def add_future_social_media(self, country_code, party_name, post, implementation_date):\n",
        "            \"\"\"\n",
        "            Adds a future social media post event to the model.\n",
        "\n",
        "            Args:\n",
        "                country_code (str): ISO alpha-3 country code.\n",
        "                party_name (str): Name of the political party.\n",
        "                post (str): Social media post content.\n",
        "                implementation_date (pd.Timestamp): Date when the post becomes active.\n",
        "            \"\"\"\n",
        "            self.future_social_media.append((country_code, party_name, post, implementation_date))\n",
        "            logging.info(f\"Scheduled future social media post for {party_name} in {country_code} on {implementation_date.date()}: {post[:60]}...\")\n",
        "\n",
        "        def process_future_events(self):\n",
        "            \"\"\"\n",
        "            Processes future events that are due in the current simulation step.\n",
        "            \"\"\"\n",
        "            current_date = pd.Timestamp.today() + pd.Timedelta(days=self.schedule.steps)\n",
        "            # Process news\n",
        "            due_news = [event for event in self.future_news if event[3] <= current_date]\n",
        "            for event in due_news:\n",
        "                country_code, party_name, news, _ = event\n",
        "                # Integrate the news into the corresponding GovernmentAgent\n",
        "                gov_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == country_code]\n",
        "                for gov in gov_agents:\n",
        "                    if gov.governing_party == party_name:\n",
        "                        if gov.news_data is not None:\n",
        "                            new_row = {'description': news}\n",
        "                            gov.news_data = gov.news_data.append(new_row, ignore_index=True)\n",
        "                            # Reprocess the news data to update sentiments and entities\n",
        "                            gov.news_data = self.model.processed_data.get(f'{country_code}_news')\n",
        "                        else:\n",
        "                            # Create a new DataFrame if none exists\n",
        "                            gov.news_data = pd.DataFrame([{'description': news}])\n",
        "                            gov.news_data = self.model.processed_data.get(f'{country_code}_news')\n",
        "                # Remove the processed event\n",
        "                self.future_news.remove(event)\n",
        "                logging.info(f\"Processed future news for {party_name} in {country_code}: {news[:60]}...\")\n",
        "\n",
        "            # Process social media\n",
        "            due_posts = [event for event in self.future_social_media if event[3] <= current_date]\n",
        "            for event in due_posts:\n",
        "                country_code, party_name, post, _ = event\n",
        "                # Integrate the post into the corresponding GovernmentAgent\n",
        "                gov_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgent) and agent.country_code == country_code]\n",
        "                for gov in gov_agents:\n",
        "                    if gov.governing_party == party_name:\n",
        "                        if gov.tweets_data is not None:\n",
        "                            new_row = {'text': post}\n",
        "                            gov.tweets_data = gov.tweets_data.append(new_row, ignore_index=True)\n",
        "                            # Reprocess the tweets data to update sentiments and entities\n",
        "                            gov.tweets_data = self.model.processed_data.get(f'{country_code}_tweets')\n",
        "                        else:\n",
        "                            # Create a new DataFrame if none exists\n",
        "                            gov.tweets_data = pd.DataFrame([{'text': post}])\n",
        "                            gov.tweets_data = self.model.processed_data.get(f'{country_code}_tweets')\n",
        "                # Remove the processed event\n",
        "                self.future_social_media.remove(event)\n",
        "                logging.info(f\"Processed future social media post for {party_name} in {country_code}: {post[:60]}...\")\n",
        "\n",
        "        def step(self):\n",
        "            \"\"\"\n",
        "            Advances the model by one step, allowing agents to act and processing due future events.\n",
        "            \"\"\"\n",
        "            super().step()\n",
        "            self.process_future_events()\n",
        "            # Additional metrics or logging can be added here\n",
        "\n",
        "    # =================================\n",
        "    # 7. Additional Enhancements and Best Practices\n",
        "    # =================================\n",
        "\n",
        "    # The following section includes additional functions, error handling, and utilities to enhance the simulation's robustness and scalability.\n",
        "\n",
        "    def setup_logging(log_file='simulation.log', log_level=logging.INFO):\n",
        "        \"\"\"\n",
        "        Sets up the logging configuration.\n",
        "\n",
        "        Args:\n",
        "            log_file (str): The file to which logs will be written.\n",
        "            log_level (int): The logging level.\n",
        "        \"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=log_level,\n",
        "            format='%(asctime)s %(levelname)s:%(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(log_file),\n",
        "                logging.StreamHandler(sys.stdout)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def load_environment_variables(env_file='.env'):\n",
        "        \"\"\"\n",
        "        Loads environment variables from a .env file.\n",
        "\n",
        "        Args:\n",
        "            env_file (str): Path to the .env file.\n",
        "        \"\"\"\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv(env_file)\n",
        "\n",
        "    def initialize_simulation():\n",
        "        \"\"\"\n",
        "        Initializes the entire simulation workflow:\n",
        "        - Loads environment variables.\n",
        "        - Sets up logging.\n",
        "        - Acquires data.\n",
        "        - Processes data.\n",
        "        - Initializes the simulation model.\n",
        "        - Generates and assigns decision logics.\n",
        "        - Runs the simulation.\n",
        "        \"\"\"\n",
        "        # Load environment variables\n",
        "        load_environment_variables()\n",
        "\n",
        "        # Set up logging\n",
        "        setup_logging()\n",
        "\n",
        "        # Data Acquisition\n",
        "        data_acquisition = DataAcquisition()\n",
        "        raw_data = data_acquisition.fetch_all_data()\n",
        "\n",
        "        # Data Processing\n",
        "        data_processing = DataProcessing(raw_data)\n",
        "        data_processing.process_all_data()\n",
        "        processed_data = data_processing.processed_data\n",
        "\n",
        "        # Load decision logics (after generation)\n",
        "        # For the first run, decision logics may not exist. They should be generated and saved.\n",
        "\n",
        "        # Initialize the enhanced Trade Model\n",
        "        model = TradeModelEnhanced(\n",
        "            processed_data=processed_data,\n",
        "            political_data=raw_data.get('political_data', {}),\n",
        "            news_data=raw_data.get('news_data', {}),\n",
        "            social_media_data=raw_data.get('social_media_data', {}),\n",
        "            legislation_data=raw_data.get('legislation_data', {})\n",
        "        )\n",
        "\n",
        "        # Generate and assign decision logics\n",
        "        decision_logics = generate_and_assign_decision_logics(processed_data, model)\n",
        "        model.decision_logics = decision_logics\n",
        "\n",
        "        # Save the updated model state or any other necessary components\n",
        "        # This can include persisting the model to a file or database\n",
        "\n",
        "        return model\n",
        "\n",
        "    # =================================\n",
        "    # 8. Example Unit Test for Decision Logic Generation\n",
        "    # =================================\n",
        "\n",
        "    import unittest\n",
        "\n",
        "    class TestDecisionLogicGeneration(unittest.TestCase):\n",
        "        def setUp(self):\n",
        "            # Sample context\n",
        "            self.sample_context = json.dumps({\n",
        "                'Country': 'USA',\n",
        "                'Party': 'Democratic Party',\n",
        "                'Ideology': 'Liberal',\n",
        "                'Recent_News_Sentiments': {'Positive': 10, 'Negative': 5},\n",
        "                'Recent_Social_Media_Sentiments': {'Positive': 8, 'Negative': 7},\n",
        "                'Current_Policies': [{'Policy': 'Tariff Rate Adjustment', 'Status': 'Increasing'}],\n",
        "                'Legislation': [{'Bill_Name': 'Trade Cooperation Act', 'Status': 'active', 'Date_Introduced': '2023-03-22'}],\n",
        "                'Upcoming_Elections': [{'Date_Upcoming_Election': '2024-11-15'}]\n",
        "            })\n",
        "\n",
        "        def test_generate_decision_logic(self):\n",
        "            decision_logic = generate_decision_logic(self.sample_context)\n",
        "            self.assertTrue(len(decision_logic) > 0, \"Decision logic should not be empty.\")\n",
        "            decision_df = parse_decision_logic(decision_logic)\n",
        "            self.assertFalse(decision_df.empty, \"Parsed decision logic DataFrame should not be empty.\")\n",
        "            self.assertListEqual(list(decision_df.columns), ['Decision_ID', 'Description', 'Trigger_Event', 'Expected_Outcome', 'Implementation_Date'], \"DataFrame columns do not match expected structure.\")\n",
        "\n",
        "        def test_integrate_future_events(self):\n",
        "            # Initialize a mock model\n",
        "            mock_model = TradeModelEnhanced({}, {}, {}, {}, {})\n",
        "            future_news = [\"Headline: USA increases tariffs on steel imports.\", \"Headline: New trade agreement signed with EU.\"]\n",
        "            future_posts = [\"@user1 The new tariffs on steel are hurting our economy!\", \"@user2 Excited about the new trade deal with the EU!\"]\n",
        "            integrate_future_events(mock_model, 'USA', 'Democratic Party', future_news, future_posts)\n",
        "            self.assertEqual(len(mock_model.future_news), 2, \"There should be two future news events scheduled.\")\n",
        "            self.assertEqual(len(mock_model.future_social_media), 2, \"There should be two future social media posts scheduled.\")\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        unittest.main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "import re\n",
        "from typing import Dict, Any, List, Tuple, Optional\n",
        "from mesa import Agent, Model\n",
        "from mesa.time import RandomActivation\n",
        "from mesa.datacollection import DataCollector\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "import unittest\n",
        "from unittest.mock import patch, MagicMock\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# =================================\n",
        "# 1. Setup and Configuration\n",
        "# =================================\n",
        "\n",
        "# Ensure OpenAI API key is set\n",
        "def load_openai_api_key(env_file='.env'):\n",
        "    \"\"\"\n",
        "    Loads the OpenAI API key from the specified .env file.\n",
        "    \"\"\"\n",
        "    load_dotenv(env_file)\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        logging.error(\"OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
        "        sys.exit(1)\n",
        "    openai.api_key = api_key\n",
        "\n",
        "def setup_logging(log_file='simulation.log', log_level=logging.INFO):\n",
        "    \"\"\"\n",
        "    Sets up the logging configuration.\n",
        "\n",
        "    Args:\n",
        "        log_file (str): The file to which logs will be written.\n",
        "        log_level (int): The logging level.\n",
        "    \"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=log_level,\n",
        "        format='%(asctime)s %(levelname)s:%(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file),\n",
        "            logging.StreamHandler(sys.stdout)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# =================================\n",
        "# 2. Data Acquisition and Processing Modules\n",
        "# =================================\n",
        "\n",
        "class DataAcquisition:\n",
        "    \"\"\"\n",
        "    Handles data acquisition from various sources.\n",
        "    Placeholder for actual implementation.\n",
        "    \"\"\"\n",
        "    def fetch_all_data(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Fetches all necessary data from data sources.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: A dictionary containing all fetched data.\n",
        "        \"\"\"\n",
        "        # Placeholder implementation\n",
        "        # In a real scenario, implement data fetching from APIs, databases, or files.\n",
        "        return {\n",
        "            'world_bank': pd.DataFrame({\n",
        "                'Country': ['USA', 'CAN', 'CHN', 'DEU'],\n",
        "                'Year': [2023, 2023, 2023, 2023],\n",
        "                'GDP': [21000000000000, 1700000000000, 14000000000000, 4000000000000],\n",
        "                'Trade_Percentage_GDP': [25, 30, 35, 28],\n",
        "                'GDP_Growth': [2.3, 1.8, 5.5, 1.5]\n",
        "            }),\n",
        "            'political_data': {\n",
        "                'USA': {\n",
        "                    'Political_Parties': pd.DataFrame({\n",
        "                        'Party': ['Democratic Party', 'Republican Party'],\n",
        "                        'Ideology': ['Liberal', 'Conservative']\n",
        "                    }),\n",
        "                    'Upcoming_Elections': pd.DataFrame({\n",
        "                        'Date_Upcoming_Election': ['2024-11-05']\n",
        "                    }),\n",
        "                    'Current_Policies': pd.DataFrame({\n",
        "                        'Policy': ['Tariff Rate Adjustment', 'Climate Change Initiative'],\n",
        "                        'Status': ['Increasing', 'Active']\n",
        "                    })\n",
        "                },\n",
        "                'CAN': {\n",
        "                    'Political_Parties': pd.DataFrame({\n",
        "                        'Party': ['Liberal Party', 'Conservative Party'],\n",
        "                        'Ideology': ['Liberal', 'Conservative']\n",
        "                    }),\n",
        "                    'Upcoming_Elections': pd.DataFrame({\n",
        "                        'Date_Upcoming_Election': ['2025-10-20']\n",
        "                    }),\n",
        "                    'Current_Policies': pd.DataFrame({\n",
        "                        'Policy': ['Healthcare Reform', 'Climate Action Plan'],\n",
        "                        'Status': ['Active', 'Active']\n",
        "                    })\n",
        "                },\n",
        "                # Add more countries as needed\n",
        "            },\n",
        "            'news_data': {\n",
        "                'USA': pd.DataFrame({\n",
        "                    'description': [\n",
        "                        'The economy is booming with low unemployment rates.',\n",
        "                        'Public sentiment is turning negative due to rising inflation.',\n",
        "                        'The Democratic Party launches a new green energy initiative.'\n",
        "                    ],\n",
        "                    'Sentiment': ['Positive', 'Negative', 'Positive'],\n",
        "                    'Entities': [['economy', 'unemployment'], ['public sentiment', 'inflation'], ['Democratic Party', 'green energy']]\n",
        "                }),\n",
        "                'CAN': pd.DataFrame({\n",
        "                    'description': [\n",
        "                        'Canada\\'s healthcare reforms have received widespread support.',\n",
        "                        'Climate action plans are being accelerated by the Liberal Party.'\n",
        "                    ],\n",
        "                    'Sentiment': ['Positive', 'Positive'],\n",
        "                    'Entities': [['healthcare reforms'], ['Climate Action Plan', 'Liberal Party']]\n",
        "                }),\n",
        "                # Add more countries as needed\n",
        "            },\n",
        "            'social_media_data': {\n",
        "                'USA': pd.DataFrame({\n",
        "                    'text': [\n",
        "                        '@user1 Loving the new trade policies!',\n",
        "                        '@user2 Concerned about the recent tariff increases.',\n",
        "                        '@user3 Supportive of the green energy initiatives!'\n",
        "                    ],\n",
        "                    'Sentiment': ['Positive', 'Negative', 'Positive'],\n",
        "                    'Entities': [['trade policies'], ['tariff increases'], ['green energy initiatives']]\n",
        "                }),\n",
        "                'CAN': pd.DataFrame({\n",
        "                    'text': [\n",
        "                        '@canadian1 Excited about the healthcare reforms!',\n",
        "                        '@canadian2 Worried about the climate action delays.'\n",
        "                    ],\n",
        "                    'Sentiment': ['Positive', 'Negative'],\n",
        "                    'Entities': [['healthcare reforms'], ['climate action delays']]\n",
        "                }),\n",
        "                # Add more countries as needed\n",
        "            },\n",
        "            'legislation_data': {\n",
        "                'USA': pd.DataFrame({\n",
        "                    'Bill_Name': ['Trade Cooperation Act', 'Green Energy Promotion Act'],\n",
        "                    'Status': ['active', 'active'],\n",
        "                    'Date_Introduced': ['2023-03-22', '2022-07-15']\n",
        "                }),\n",
        "                'CAN': pd.DataFrame({\n",
        "                    'Bill_Name': ['Healthcare Improvement Act', 'Climate Action Enhancement Act'],\n",
        "                    'Status': ['active', 'active'],\n",
        "                    'Date_Introduced': ['2023-01-10', '2022-05-30']\n",
        "                }),\n",
        "                # Add more countries as needed\n",
        "            },\n",
        "            'Trade_Alliances': {\n",
        "                'USA': ['EU', 'CAN'],\n",
        "                'CAN': ['USA', 'MEX'],\n",
        "                'CHN': ['RUS', 'IND'],\n",
        "                'DEU': ['FRA', 'ITA'],\n",
        "                # Add more countries as needed\n",
        "            }\n",
        "        }\n",
        "\n",
        "class DataProcessing:\n",
        "    \"\"\"\n",
        "    Processes raw data into structured formats for the simulation.\n",
        "    \"\"\"\n",
        "    def __init__(self, raw_data: Dict[str, Any]):\n",
        "        self.raw_data = raw_data\n",
        "        self.processed_data = {}\n",
        "\n",
        "    def process_all_data(self):\n",
        "        \"\"\"\n",
        "        Processes all raw data into structured formats.\n",
        "        \"\"\"\n",
        "        # Process World Bank data (already in DataFrame)\n",
        "        self.processed_data['world_bank'] = self.raw_data.get('world_bank', pd.DataFrame())\n",
        "\n",
        "        # Process political data\n",
        "        self.processed_data['political_data'] = self.raw_data.get('political_data', {})\n",
        "\n",
        "        # Process news data\n",
        "        self.processed_data['news_data'] = self.raw_data.get('news_data', {})\n",
        "\n",
        "        # Process social media data\n",
        "        self.processed_data['social_media_data'] = self.raw_data.get('social_media_data', {})\n",
        "\n",
        "        # Process legislation data\n",
        "        self.processed_data['legislation_data'] = self.raw_data.get('legislation_data', {})\n",
        "\n",
        "        # Process trade alliances\n",
        "        self.processed_data['Trade_Alliances'] = self.raw_data.get('Trade_Alliances', {})\n",
        "\n",
        "        # Add more processing steps as needed\n",
        "\n",
        "# =================================\n",
        "# 3. Agent-Based Modeling Module\n",
        "# =================================\n",
        "\n",
        "class GovernmentAgentEnhanced(Agent):\n",
        "    \"\"\"\n",
        "    Enhanced Government agents with data-driven dispositions and a disposition-to-scenario-action matrix.\n",
        "    \"\"\"\n",
        "    def __init__(self, unique_id, model, country_code, economic_indicators, political_parties: pd.DataFrame,\n",
        "                 upcoming_elections: pd.DataFrame, current_policies: pd.DataFrame,\n",
        "                 disposition_matrix: pd.DataFrame, modifier_matrix: pd.DataFrame):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.country_code = country_code\n",
        "        self.economic_indicators = economic_indicators  # e.g., GDP, trade balance, etc.\n",
        "        self.policies = {'Tariff_Rate': 0, 'Subsidy_Rate': 0, 'Defense_Spending': 0}  # Initialize with default rates\n",
        "        self.trade_alliances = set(model.processed_data['Trade_Alliances'].get(country_code, []))  # Countries this government has alliances with\n",
        "\n",
        "        # Attributes\n",
        "        self.political_parties = political_parties  # DataFrame with Party and Ideology\n",
        "        self.upcoming_elections = upcoming_elections  # DataFrame with election details\n",
        "        self.current_policies = current_policies  # DataFrame with current policy statuses\n",
        "\n",
        "        self.governing_party = self.determine_governing_party()\n",
        "        self.policy_shift_probability = self.determine_policy_shift_probability()\n",
        "\n",
        "        # Attach news and social media data if available\n",
        "        self.news_data = self.model.processed_data.get(f'{self.country_code}_news')\n",
        "        self.tweets_data = self.model.processed_data.get(f'{self.country_code}_tweets')\n",
        "        self.legislation_data = self.model.processed_data.get(f'{self.country_code}_legislation_data')\n",
        "\n",
        "        # Disposition Matrix\n",
        "        self.disposition_matrix = disposition_matrix  # DataFrame with Decision_ID, Action, Scenario, Probability(%)\n",
        "\n",
        "        # Modifier Matrix\n",
        "        self.modifier_matrix = modifier_matrix  # DataFrame with Relation_Type, Relation_Name, Scenario, Modifier_Value\n",
        "\n",
        "    def determine_governing_party(self) -> str:\n",
        "        \"\"\"\n",
        "        Determines the current governing party based on existing data.\n",
        "        \"\"\"\n",
        "        if not self.political_parties.empty:\n",
        "            return self.political_parties.iloc[0]['Party']\n",
        "        return \"Unknown\"\n",
        "\n",
        "    def determine_policy_shift_probability(self) -> float:\n",
        "        \"\"\"\n",
        "        Determines the base probability of a policy shift based on factors like upcoming elections and economic indicators.\n",
        "        \"\"\"\n",
        "        base_probability = 0.05  # Base 5% chance of policy shift each step\n",
        "        # Increase probability if an upcoming election is near\n",
        "        if not self.upcoming_elections.empty:\n",
        "            election_date = pd.to_datetime(self.upcoming_elections.iloc[0]['Date_Upcoming_Election'])\n",
        "            days_until_election = (election_date - pd.Timestamp.today()).days\n",
        "            if days_until_election <= 180:  # 6 months\n",
        "                base_probability += 0.10  # Additional 10% chance\n",
        "        # Modify based on economic indicators\n",
        "        gdp_growth = self.economic_indicators.get('GDP_Growth', 0)\n",
        "        if gdp_growth < 0:\n",
        "            base_probability += 0.05  # Additional 5% chance to shift policies during economic downturn\n",
        "        return min(base_probability, 1.0)\n",
        "\n",
        "    def calculate_public_sentiment(self) -> str:\n",
        "        \"\"\"\n",
        "        Calculates the overall public sentiment based on news and social media data.\n",
        "        Returns 'Positive', 'Negative', or 'Neutral'.\n",
        "        \"\"\"\n",
        "        sentiments = []\n",
        "        # Analyze news sentiment\n",
        "        if self.news_data is not None and not self.news_data.empty:\n",
        "            sentiments += self.news_data['Sentiment'].tolist()\n",
        "        # Analyze social media sentiment\n",
        "        if self.tweets_data is not None and not self.tweets_data.empty:\n",
        "            sentiments += self.tweets_data['Sentiment'].tolist()\n",
        "        if not sentiments:\n",
        "            return 'Neutral'\n",
        "        # Calculate the most common sentiment\n",
        "        sentiment_counts = pd.Series(sentiments).value_counts()\n",
        "        dominant_sentiment = sentiment_counts.idxmax()\n",
        "        return dominant_sentiment\n",
        "\n",
        "    def implement_policies_based_on_dispositions(self, current_scenarios: List[str]):\n",
        "        \"\"\"\n",
        "        Implements policies based on the disposition-to-scenario-action probability matrix and applies modifiers.\n",
        "\n",
        "        Args:\n",
        "            current_scenarios (List[str]): List of current active scenarios in the simulation.\n",
        "        \"\"\"\n",
        "        for scenario in current_scenarios:\n",
        "            # Filter the disposition matrix for the current scenario\n",
        "            scenario_decisions = self.disposition_matrix[self.disposition_matrix['Scenario'] == scenario]\n",
        "            for _, decision in scenario_decisions.iterrows():\n",
        "                base_probability = decision['Probability(%)'] / 100.0\n",
        "                action = decision['Action']\n",
        "                # Calculate modifier based on relationships\n",
        "                modifier = self.calculate_modifier(action, scenario)\n",
        "                adjusted_probability = base_probability + modifier\n",
        "                adjusted_probability = max(min(adjusted_probability, 1.0), 0.0)  # Clamp between 0 and 1\n",
        "                if random.random() < adjusted_probability:\n",
        "                    self.execute_action(action)\n",
        "\n",
        "    def calculate_modifier(self, action: str, scenario: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the modifier based on relationships (partners, alliances, industries) for the given action and scenario.\n",
        "\n",
        "        Args:\n",
        "            action (str): The action being considered.\n",
        "            scenario (str): The current scenario.\n",
        "\n",
        "        Returns:\n",
        "            float: The modifier value to adjust the probability.\n",
        "        \"\"\"\n",
        "        modifier = 0.0\n",
        "        # Example: If the action involves forming alliances, and the country has strong alliances with certain partners\n",
        "        if 'Form Trade Alliances' in action or 'Finalize Trade Agreements' in action:\n",
        "            for alliance in self.trade_alliances:\n",
        "                # Find modifiers for alliances\n",
        "                mod_rows = self.modifier_matrix[\n",
        "                    (self.modifier_matrix['Relation_Type'] == 'Alliance') &\n",
        "                    (self.modifier_matrix['Relation_Name'] == alliance) &\n",
        "                    (self.modifier_matrix['Scenario'] == scenario)\n",
        "                ]\n",
        "                if not mod_rows.empty:\n",
        "                    modifier += mod_rows.iloc[0]['Modifier_Value']\n",
        "\n",
        "        # Example: If the action involves specific industries\n",
        "        if 'Tariffs' in action or 'Subsidies' in action:\n",
        "            for industry in ['Automotive', 'Technology']:\n",
        "                mod_rows = self.modifier_matrix[\n",
        "                    (self.modifier_matrix['Relation_Type'] == 'Industry') &\n",
        "                    (self.modifier_matrix['Relation_Name'] == industry) &\n",
        "                    (self.modifier_matrix['Scenario'] == scenario)\n",
        "                ]\n",
        "                if not mod_rows.empty:\n",
        "                    modifier += mod_rows.iloc[0]['Modifier_Value']\n",
        "\n",
        "        return modifier\n",
        "\n",
        "    def execute_action(self, action: str):\n",
        "        \"\"\"\n",
        "        Executes the specified action by updating policies or performing actions.\n",
        "\n",
        "        Args:\n",
        "            action (str): The action to be executed.\n",
        "        \"\"\"\n",
        "        action = action.lower()\n",
        "        if 'increase tariffs' in action:\n",
        "            old_tariff = self.policies.get('Tariff_Rate', 0)\n",
        "            self.policies['Tariff_Rate'] = min(old_tariff + 1, 30)  # Example increment\n",
        "            logging.info(f'Government {self.country_code}: Increased tariff rate from {old_tariff}% to {self.policies[\"Tariff_Rate\"]}% as per action \"{action}\".')\n",
        "        elif 'decrease tariffs' in action:\n",
        "            old_tariff = self.policies.get('Tariff_Rate', 0)\n",
        "            self.policies['Tariff_Rate'] = max(old_tariff - 1, 0)  # Example decrement\n",
        "            logging.info(f'Government {self.country_code}: Decreased tariff rate from {old_tariff}% to {self.policies[\"Tariff_Rate\"]}% as per action \"{action}\".')\n",
        "        elif 'form trade alliances' in action:\n",
        "            # Example: Randomly choose a partner country from defined list\n",
        "            potential_partners = [code for code in self.model.country_codes if code != self.country_code and code not in self.trade_alliances]\n",
        "            if potential_partners:\n",
        "                partner = random.choice(potential_partners)\n",
        "                self.form_trade_alliance(partner)\n",
        "        elif 'finalize trade agreements' in action:\n",
        "            # Implement trade agreements logic\n",
        "            logging.info(f'Government {self.country_code}: Finalizing trade agreements as per action \"{action}\".')\n",
        "        elif 'implement subsidies' in action:\n",
        "            old_subsidy = self.policies.get('Subsidy_Rate', 0)\n",
        "            self.policies['Subsidy_Rate'] = old_subsidy + 1  # Example increment\n",
        "            logging.info(f'Government {self.country_code}: Implemented subsidies increasing from {old_subsidy}% to {self.policies[\"Subsidy_Rate\"]}% as per action \"{action}\".')\n",
        "        elif 'reduce subsidies' in action:\n",
        "            old_subsidy = self.policies.get('Subsidy_Rate', 0)\n",
        "            self.policies['Subsidy_Rate'] = max(old_subsidy - 1, 0)  # Example decrement\n",
        "            logging.info(f'Government {self.country_code}: Reduced subsidies from {old_subsidy}% to {self.policies[\"Subsidy_Rate\"]}% as per action \"{action}\".')\n",
        "        elif 'implement climate change policies' in action:\n",
        "            logging.info(f'Government {self.country_code}: Implementing climate change policies as per action \"{action}\".')\n",
        "        elif 'enhance disaster response mechanisms' in action:\n",
        "            logging.info(f'Government {self.country_code}: Enhancing disaster response mechanisms as per action \"{action}\".')\n",
        "        elif 'promote renewable energy initiatives' in action:\n",
        "            logging.info(f'Government {self.country_code}: Promoting renewable energy initiatives as per action \"{action}\".')\n",
        "        elif 'launch election campaign' in action:\n",
        "            logging.info(f'Government {self.country_code}: Launching election campaign as per action \"{action}\".')\n",
        "        elif 'initiate policy reforms' in action:\n",
        "            logging.info(f'Government {self.country_code}: Initiating policy reforms as per action \"{action}\".')\n",
        "        elif 'handle political scandals' in action:\n",
        "            logging.info(f'Government {self.country_code}: Handling political scandals as per action \"{action}\".')\n",
        "        elif 'resign leadership' in action:\n",
        "            logging.info(f'Government {self.country_code}: Resigning leadership as per action \"{action}\".')\n",
        "        elif 'appoint new leaders' in action:\n",
        "            logging.info(f'Government {self.country_code}: Appointing new leaders as per action \"{action}\".')\n",
        "        elif 'address public protests' in action:\n",
        "            logging.info(f'Government {self.country_code}: Addressing public protests as per action \"{action}\".')\n",
        "        elif 'launch public awareness campaigns' in action:\n",
        "            logging.info(f'Government {self.country_code}: Launching public awareness campaigns as per action \"{action}\".')\n",
        "        elif 'implement social welfare programs' in action:\n",
        "            logging.info(f'Government {self.country_code}: Implementing social welfare programs as per action \"{action}\".')\n",
        "        elif 'increase defense spending' in action:\n",
        "            old_defense = self.policies.get('Defense_Spending', 0)\n",
        "            self.policies['Defense_Spending'] = old_defense + 1  # Example increment\n",
        "            logging.info(f'Government {self.country_code}: Increased defense spending from {old_defense} to {self.policies[\"Defense_Spending\"]} as per action \"{action}\".')\n",
        "        elif 'decrease defense spending' in action:\n",
        "            old_defense = self.policies.get('Defense_Spending', 0)\n",
        "            self.policies['Defense_Spending'] = max(old_defense - 1, 0)  # Example decrement\n",
        "            logging.info(f'Government {self.country_code}: Decreased defense spending from {old_defense} to {self.policies[\"Defense_Spending\"]} as per action \"{action}\".')\n",
        "        elif 'strengthen security measures' in action:\n",
        "            logging.info(f'Government {self.country_code}: Strengthening security measures as per action \"{action}\".')\n",
        "        # Add more actions as needed\n",
        "\n",
        "    class CompanyAgent(Agent):\n",
        "        \"\"\"\n",
        "        Company agents represent businesses in different industry sectors, with advanced strategies such as investing in industries, adjusting production, and supply chain management.\n",
        "        \"\"\"\n",
        "        def __init__(self, unique_id, model, name, industry_sector, country_code, financial_data):\n",
        "            super().__init__(unique_id, model)\n",
        "            self.name = name\n",
        "            self.industry_sector = industry_sector\n",
        "            self.country_code = country_code\n",
        "            self.financial_data = financial_data\n",
        "            self.production_capacity = financial_data.get('Production_Capacity', 100)\n",
        "            self.supply_chain = []  # List of supplier company agents or countries where inputs are sourced from\n",
        "            self.financial_health = financial_data.get('Financial_Health', 100)\n",
        "            self.demand_preferences = {'domestic': 0.5, 'imported': 0.5}  # Simplified preference for domestic vs. imported goods if relevant\n",
        "\n",
        "        def make_sourcing_decision(self):\n",
        "            \"\"\"\n",
        "            Adjust sourcing decisions based on government policies, such as tariffs, and market conditions.\n",
        "            This simulates supply chain shifts in response to changing trade policies or other economic conditions.\n",
        "            \"\"\"\n",
        "            government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == self.country_code]\n",
        "            if government_agents:\n",
        "                tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)\n",
        "                if tariff_rate > 10:\n",
        "                    # If tariffs are high, the company might increase local sourcing to avoid import costs.\n",
        "                    self.demand_preferences['domestic'] += 0.1\n",
        "                    self.demand_preferences['imported'] = max(self.demand_preferences['imported'] - 0.1, 0)\n",
        "                    logging.info(f'Company {self.name} in {self.country_code}: Adjusting sourcing towards domestic suppliers due to high tariffs.')\n",
        "                else:\n",
        "                    # If tariffs are low, the company might maintain or even increase foreign sourcing for cheaper imports.\n",
        "                    self.demand_preferences['imported'] += 0.05\n",
        "                    self.demand_preferences['domestic'] = max(self.demand_preferences['domestic'] - 0.05, 0)\n",
        "                    logging.info(f'Company {self.name} in {self.country_code}: Adjusting sourcing towards imported goods due to low tariffs.')\n",
        "\n",
        "        def adjust_production(self):\n",
        "            \"\"\"\n",
        "            Adjusts production capacity based on demand, supply chain status, and financial health.\n",
        "            \"\"\"\n",
        "            demand = self.model.compute_demand(self.industry_sector)\n",
        "            old_capacity = self.production_capacity\n",
        "            if demand > self.production_capacity:\n",
        "                self.production_capacity += 10  # Increase capacity if demand is high.\n",
        "                logging.info(f'Company {self.name} in {self.country_code}: Increased production capacity from {old_capacity} to {self.production_capacity} due to rising demand.')\n",
        "            elif demand < self.production_capacity * 0.8:\n",
        "                self.production_capacity = max(self.production_capacity - 10, 50)  # Decrease capacity if demand is low.\n",
        "                logging.info(f'Company {self.name} in {self.country_code}: Decreased production capacity from {old_capacity} to {self.production_capacity} due to falling demand.')\n",
        "\n",
        "        def invest_in_industry(self, investment_amount: float):\n",
        "            \"\"\"\n",
        "            Invest in industry to improve the company's financial health or production capacity.\n",
        "            This simulates decisions like expanding factories, training employees, or R&D spending.\n",
        "            \"\"\"\n",
        "            old_financial_health = self.financial_health\n",
        "            old_capacity = self.production_capacity\n",
        "            self.financial_health += investment_amount * 0.1  # Example investment effect on financial health.\n",
        "            self.production_capacity += investment_amount * 0.05  # Example effect on production capacity.\n",
        "            logging.info(f'Company {self.name} in {self.country_code}: Invested {investment_amount}, financial health improved from {old_financial_health} to {self.financial_health} and capacity from {old_capacity} to {self.production_capacity}.')\n",
        "\n",
        "        def step(self):\n",
        "            \"\"\"\n",
        "            Defines the agent's behavior at each step, including sourcing decisions, production adjustments, and potential investments.\n",
        "            \"\"\"\n",
        "            self.make_sourcing_decision()\n",
        "            self.adjust_production()\n",
        "            # Example: Random investment decisions to simulate growth or adaptation strategies.\n",
        "            if random.random() < 0.02:  # 2% chance each step to invest.\n",
        "                investment = random.uniform(1000, 5000)\n",
        "                self.invest_in_industry(investment)\n",
        "\n",
        "class ConsumerAgent(Agent):\n",
        "    \"\"\"\n",
        "    Consumer agents represent individual or aggregate consumer behavior with advanced preferences and responses to policies.\n",
        "    \"\"\"\n",
        "    def __init__(self, unique_id, model, income_level, country_code):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.income_level = income_level  # 'Low', 'Medium', 'High' income category for simplified model.\n",
        "        self.country_code = country_code\n",
        "        self.demand_preferences = self.generate_preferences()  # Preferences for goods based on income level and policy.\n",
        "\n",
        "    def generate_preferences(self):\n",
        "        \"\"\"\n",
        "        Generates demand preferences based on income level. Higher income consumers prefer more luxury goods.\n",
        "        \"\"\"\n",
        "        if self.income_level == 'Low':\n",
        "            return {'essential_goods': 0.7, 'luxury_goods': 0.3}\n",
        "        elif self.income_level == 'Medium':\n",
        "            return {'essential_goods': 0.5, 'luxury_goods': 0.5}\n",
        "        else:  # High income consumers prefer more luxury goods.\n",
        "            return {'essential_goods': 0.3, 'luxury_goods': 0.7}\n",
        "\n",
        "    def adjust_consumption(self):\n",
        "        \"\"\"\n",
        "        Adjusts consumption behavior based on changing prices, income, and government policies like tariffs.\n",
        "        If tariffs are high on imported goods, consumers might buy more domestic goods.\n",
        "        \"\"\"\n",
        "        government_agents = [agent for agent in self.model.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == self.country_code]\n",
        "        if government_agents:\n",
        "            tariff_rate = government_agents[0].policies.get('Tariff_Rate', 0)\n",
        "            if tariff_rate > 10:\n",
        "                # If tariffs on imported goods are high, reduce luxury goods consumption (assuming many are imported)\n",
        "                old_luxury_pref = self.demand_preferences['luxury_goods']\n",
        "                self.demand_preferences['luxury_goods'] *= 0.9  # reduce consumption of expensive imported luxuries.\n",
        "                logging.info(f'Consumer in {self.country_code}: Decreasing consumption of imported luxury goods from preference {old_luxury_pref} to {self.demand_preferences[\"luxury_goods\"]} due to high tariffs.')\n",
        "            else:\n",
        "                # If tariffs are low, possibly maintain or increase luxury goods consumption.\n",
        "                old_luxury_pref = self.demand_preferences['luxury_goods']\n",
        "                self.demand_preferences['luxury_goods'] *= 1.05\n",
        "                logging.info(f'Consumer in {self.country_code}: Slightly increasing consumption of luxury goods from preference {old_luxury_pref} to {self.demand_preferences[\"luxury_goods\"]} due to low tariffs.')\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Defines the agent's behavior at each step, adjusting consumption preferences if needed.\n",
        "        \"\"\"\n",
        "        self.adjust_consumption()\n",
        "\n",
        "class IntermediaryAgent(Agent):\n",
        "    \"\"\"\n",
        "    Intermediary agents represent logistics providers, financial institutions, or other services influencing trade.\n",
        "    They can provide services such as improving supply chain efficiency or offering financing solutions.\n",
        "    \"\"\"\n",
        "    def __init__(self, unique_id, model, service_type):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.service_type = service_type  # 'Logistics', 'Finance', etc.\n",
        "\n",
        "    def provide_service(self):\n",
        "        \"\"\"\n",
        "        Provides services to other agents based on demand, possibly affecting overall trade efficiency and costs.\n",
        "        \"\"\"\n",
        "        # Placeholder service provision logic. For example, logistics agents could reduce shipping times, finance agents could reduce financial constraints.\n",
        "        pass\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Defines the agent's behavior at each step. This can include adjusting fees or expanding services as needed.\n",
        "        \"\"\"\n",
        "        self.provide_service()\n",
        "\n",
        "class TradeModelEnhanced(Model):\n",
        "    \"\"\"\n",
        "    Enhanced TradeModel that incorporates data-driven dispositions and modifier matrices.\n",
        "    \"\"\"\n",
        "    def __init__(self, processed_data: Dict[str, pd.DataFrame], political_data: Dict[str, Dict[str, pd.DataFrame]], news_data: Dict[str, pd.DataFrame], social_media_data: Dict[str, pd.DataFrame], legislation_data: Dict[str, pd.DataFrame]):\n",
        "        super().__init__()\n",
        "        self.schedule = RandomActivation(self)\n",
        "        self.processed_data = processed_data  # Processed data from DataProcessing module\n",
        "        self.political_data = political_data  # Political data fetched separately\n",
        "        self.news_data = news_data  # News data fetched separately\n",
        "        self.social_media_data = social_media_data  # Social media data fetched separately\n",
        "        self.legislation_data = legislation_data  # Legislation data fetched separately\n",
        "        self.datacollector = DataCollector(model_reporters={\"Trade_Volume\": self.compute_trade_volume})\n",
        "\n",
        "        # Assuming processed_data includes data from the World Bank to obtain a list of countries\n",
        "        country_codes = self.processed_data.get('world_bank', pd.DataFrame()).get('Country', []).unique()\n",
        "        if not isinstance(country_codes, np.ndarray):\n",
        "            country_codes = []\n",
        "        self.country_codes = country_codes\n",
        "        self.create_agents()\n",
        "        self.running = True\n",
        "        self.trade_volume_history = []\n",
        "\n",
        "        # Placeholder for scenario-action tables\n",
        "        self.scenario_action_tables = {}  # To store Disposition and Modifier matrices per country and party\n",
        "\n",
        "        # Placeholder for future events\n",
        "        self.future_news = []  # List of tuples: (country_code, party_name, news, implementation_date)\n",
        "        self.future_social_media = []  # List of tuples: (country_code, party_name, post, implementation_date)\n",
        "\n",
        "        # Parameters for simulation branching\n",
        "        self.max_branching_depth = 3  # Adjust based on desired depth\n",
        "        self.current_branching_depth = 0  # Initialize current depth\n",
        "\n",
        "    def create_agents(self):\n",
        "        \"\"\"\n",
        "        Creates agents for the model, including government, company, consumer, and intermediary agents,\n",
        "        leveraging processed data to initialize their states.\n",
        "        \"\"\"\n",
        "        # Create government agents for all countries present in processed data or default set if data is unavailable.\n",
        "        for i, country_code in enumerate(self.country_codes):\n",
        "            economic_indicators = self.get_economic_indicators(country_code)\n",
        "            political_info = self.political_data.get(country_code, {})\n",
        "            political_parties = political_info.get('Political_Parties')\n",
        "            upcoming_elections = political_info.get('Upcoming_Elections')\n",
        "            current_policies = political_info.get('Current_Policies')\n",
        "            news = self.news_data.get(country_code)\n",
        "            tweets = self.social_media_data.get(country_code)\n",
        "            legislation = self.legislation_data.get(country_code)\n",
        "\n",
        "            if political_parties is None or political_parties.empty:\n",
        "                logging.warning(f\"No political parties data for {country_code}. Skipping government agent creation.\")\n",
        "                continue\n",
        "\n",
        "            governing_party = political_parties.iloc[0]['Party']\n",
        "            context_json = gather_context_for_party(self.processed_data, country_code, governing_party)\n",
        "\n",
        "            # Generate scenario-action matrices using GPT-4\n",
        "            disposition_df, modifier_df = generate_disposition_and_modifier_matrices(context_json, self.get_scenarios())\n",
        "\n",
        "            if disposition_df.empty or modifier_df.empty:\n",
        "                logging.warning(f\"Disposition or Modifier matrix for {governing_party} in {country_code} is empty. Skipping agent creation.\")\n",
        "                continue\n",
        "\n",
        "            government_agent = GovernmentAgentEnhanced(\n",
        "                unique_id=i,\n",
        "                model=self,\n",
        "                country_code=country_code,\n",
        "                economic_indicators=economic_indicators,\n",
        "                political_parties=political_parties,\n",
        "                upcoming_elections=upcoming_elections,\n",
        "                current_policies=current_policies,\n",
        "                disposition_matrix=disposition_df,\n",
        "                modifier_matrix=modifier_df\n",
        "            )\n",
        "            # Attach news and social media data to the agent\n",
        "            government_agent.news_data = news\n",
        "            government_agent.tweets_data = tweets\n",
        "            # Attach legislation data if needed\n",
        "            government_agent.legislation_data = legislation\n",
        "            self.schedule.add(government_agent)\n",
        "\n",
        "        # Create company agents using hypothetical data or processed data. This can be data-driven if real data is available.\n",
        "        top_companies = self.get_top_companies()\n",
        "        for i, company in enumerate(top_companies, start=len(self.country_codes)):\n",
        "            company_agent = CompanyAgent(i, self, company['name'], company['sector'], company['country_code'], company['financial_data'])\n",
        "            self.schedule.add(company_agent)\n",
        "\n",
        "        # Create consumer agents representing populations. This can be scaled by population data, if available.\n",
        "        num_consumers = 100  # Adjust based on complexity and performance considerations\n",
        "        for i in range(len(self.country_codes) + len(top_companies), len(self.country_codes) + len(top_companies) + num_consumers):\n",
        "            income_level = random.choice(['Low', 'Medium', 'High'])\n",
        "            country_code = random.choice(self.country_codes) if len(self.country_codes) > 0 else 'USA'\n",
        "            consumer_agent = ConsumerAgent(i, self, income_level, country_code)\n",
        "            self.schedule.add(consumer_agent)\n",
        "\n",
        "        # Create intermediary agents like logistics providers, banks. Scale the number and roles as needed.\n",
        "        service_types = ['Logistics', 'Finance']\n",
        "        num_intermediaries = 50  # Can be adjusted based on performance\n",
        "        start_id = len(self.country_codes) + len(top_companies) + num_consumers\n",
        "        for i in range(start_id, start_id + num_intermediaries):\n",
        "            service_type = random.choice(service_types)\n",
        "            intermediary_agent = IntermediaryAgent(i, self, service_type)\n",
        "            self.schedule.add(intermediary_agent)\n",
        "\n",
        "    def get_economic_indicators(self, country_code: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Retrieves economic indicators for a given country from processed data.\n",
        "        This can include GDP, trade balance, trade percentage, etc., using the processed World Bank data.\n",
        "        \"\"\"\n",
        "        df = self.processed_data.get('world_bank', pd.DataFrame())\n",
        "        if df.empty:\n",
        "            # Return default economic indicators if no data is available\n",
        "            return {'GDP': 0, 'Trade_Percentage_GDP': 0, 'GDP_Growth': 0}\n",
        "        country_data = df[df['Country'] == country_code]\n",
        "        if country_data.empty:\n",
        "            # Return default if no data for the given country is found\n",
        "            return {'GDP': 0, 'Trade_Percentage_GDP': 0, 'GDP_Growth': 0}\n",
        "        # Use the latest data row for economic indicators\n",
        "        latest_year = country_data['Year'].max()\n",
        "        latest_data = country_data[country_data['Year'] == latest_year].iloc[0].to_dict()\n",
        "        # Calculate GDP_Growth based on previous year data if available\n",
        "        previous_year = latest_year - 1\n",
        "        previous_data = country_data[country_data['Year'] == previous_year]\n",
        "        if not previous_data.empty:\n",
        "            previous_gdp = previous_data.iloc[0].get('GDP', 0)\n",
        "            current_gdp = latest_data.get('GDP', 0)\n",
        "            gdp_growth = ((current_gdp - previous_gdp) / previous_gdp) * 100 if previous_gdp != 0 else 0\n",
        "        else:\n",
        "            gdp_growth = 0\n",
        "        latest_data['GDP_Growth'] = gdp_growth\n",
        "        return latest_data\n",
        "\n",
        "    def get_top_companies(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Retrieves top companies data, either from real data if available or uses placeholder data for the simulation.\n",
        "        In a real scenario, this can be data-driven from processed data sources or an API.\n",
        "        \"\"\"\n",
        "        # Placeholder list of companies\n",
        "        companies = [\n",
        "            {\n",
        "                'name': 'CompanyA',\n",
        "                'sector': 'Manufacturing',\n",
        "                'country_code': 'USA',\n",
        "                'financial_data': {'Production_Capacity': 500, 'Financial_Health': 90}\n",
        "            },\n",
        "            {\n",
        "                'name': 'CompanyB',\n",
        "                'sector': 'Technology',\n",
        "                'country_code': 'CHN',\n",
        "                'financial_data': {'Production_Capacity': 300, 'Financial_Health': 85}\n",
        "            },\n",
        "            {\n",
        "                'name': 'CompanyC',\n",
        "                'sector': 'Automotive',\n",
        "                'country_code': 'DEU',\n",
        "                'financial_data': {'Production_Capacity': 400, 'Financial_Health': 88}\n",
        "            },\n",
        "            {\n",
        "                'name': 'CompanyD',\n",
        "                'sector': 'Electronics',\n",
        "                'country_code': 'JPN',\n",
        "                'financial_data': {'Production_Capacity': 350, 'Financial_Health': 92}\n",
        "            },\n",
        "            # Additional companies can be added with more data if needed\n",
        "        ]\n",
        "        return companies\n",
        "\n",
        "    def compute_trade_volume(self) -> float:\n",
        "        \"\"\"\n",
        "        Computes the total trade volume in the model based on agent interactions.\n",
        "        This can be extended to actually compute interactions between companies and government policies.\n",
        "        For demonstration, we'll simulate it with random values.\n",
        "        \"\"\"\n",
        "        # Placeholder for actual computation based on agent interactions\n",
        "        # Suppose trade volume is influenced by the average tariffs and the number of trade alliances\n",
        "        government_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgentEnhanced)]\n",
        "        if not government_agents:\n",
        "            # If no government agents, return a default simulated value\n",
        "            return random.uniform(1e6, 1e9)\n",
        "        # Example: Compute some measure of trade volume from agents\n",
        "        average_tariff = np.mean([agent.policies['Tariff_Rate'] for agent in government_agents])\n",
        "        total_alliances = sum([len(agent.trade_alliances) for agent in government_agents])\n",
        "        # We'll simulate a relationship, e.g., higher alliances and lower tariffs = higher trade volume\n",
        "        trade_volume = (1e7 * len(government_agents)) * (1 + total_alliances / 100) * (1 - average_tariff / 100)\n",
        "        # Add randomness\n",
        "        trade_volume *= random.uniform(0.8, 1.2)\n",
        "        return trade_volume\n",
        "\n",
        "    def compute_demand(self, industry_sector: str) -> float:\n",
        "        \"\"\"\n",
        "        Computes demand for a given industry sector. In a real scenario, this can be data-driven from processed data.\n",
        "        For demonstration, we simulate a random demand influenced by sector performance.\n",
        "        \"\"\"\n",
        "        # Placeholder for demand computation logic - can be extended with actual data\n",
        "        base_demand = random.uniform(100, 1000)\n",
        "        # Adjust demand based on sector. Suppose each sector gets some multiplier\n",
        "        sector_multipliers = {\n",
        "            'Manufacturing': 1.1,\n",
        "            'Technology': 1.3,\n",
        "            'Automotive': 1.2,\n",
        "            'Electronics': 1.15\n",
        "        }\n",
        "        multiplier = sector_multipliers.get(industry_sector, 1.0)\n",
        "        # Additional factors could include overall economic indicators\n",
        "        # For simplicity, just multiplying base demand with sector multiplier\n",
        "        demand = base_demand * multiplier\n",
        "        return demand\n",
        "\n",
        "    def get_scenarios(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Retrieves the list of defined scenarios from the processed data or uses a default set.\n",
        "        \"\"\"\n",
        "        # Placeholder for actual scenario retrieval logic\n",
        "        return [\n",
        "            'Recession',\n",
        "            'Economic Boom',\n",
        "            'Inflation Surge',\n",
        "            'Trade Deficit Increase',\n",
        "            'Upcoming Elections',\n",
        "            'Political Scandals',\n",
        "            'Leadership Changes',\n",
        "            'Public Protests',\n",
        "            'Shifts in Public Sentiment',\n",
        "            'Natural Disasters',\n",
        "            'Climate Change Policies'\n",
        "        ]\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Advances the model by one step, allowing agents to act and processing due future events.\n",
        "        \"\"\"\n",
        "        self.schedule.step()\n",
        "        # Collect metrics\n",
        "        self.datacollector.collect(self)\n",
        "        trade_volume = self.compute_trade_volume()\n",
        "        self.trade_volume_history.append(trade_volume)\n",
        "        # Process future events\n",
        "        self.process_future_events()\n",
        "        # Additional metrics or logging can be added here\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Resets the model for a new simulation run, clearing any previous state and data.\n",
        "        \"\"\"\n",
        "        self.schedule = RandomActivation(self)\n",
        "        self.datacollector = DataCollector(model_reporters={\"Trade_Volume\": self.compute_trade_volume})\n",
        "        self.create_agents()\n",
        "        self.trade_volume_history = []\n",
        "\n",
        "    def add_future_news(self, country_code, party_name, news, implementation_date):\n",
        "        \"\"\"\n",
        "        Adds a future news event to the model.\n",
        "\n",
        "        Args:\n",
        "            country_code (str): ISO alpha-3 country code.\n",
        "            party_name (str): Name of the political party.\n",
        "            news (str): News article content.\n",
        "            implementation_date (pd.Timestamp): Date when the news becomes active.\n",
        "        \"\"\"\n",
        "        self.future_news.append((country_code, party_name, news, implementation_date))\n",
        "        logging.info(f\"Scheduled future news for {party_name} in {country_code} on {implementation_date.date()}: {news[:60]}...\")\n",
        "\n",
        "    def add_future_social_media(self, country_code, party_name, post, implementation_date):\n",
        "        \"\"\"\n",
        "        Adds a future social media post event to the model.\n",
        "\n",
        "        Args:\n",
        "            country_code (str): ISO alpha-3 country code.\n",
        "            party_name (str): Name of the political party.\n",
        "            post (str): Social media post content.\n",
        "            implementation_date (pd.Timestamp): Date when the post becomes active.\n",
        "        \"\"\"\n",
        "        self.future_social_media.append((country_code, party_name, post, implementation_date))\n",
        "        logging.info(f\"Scheduled future social media post for {party_name} in {country_code} on {implementation_date.date()}: {post[:60]}...\")\n",
        "\n",
        "    def process_future_events(self):\n",
        "        \"\"\"\n",
        "        Processes future events that are due in the current simulation step.\n",
        "        \"\"\"\n",
        "        current_date = pd.Timestamp.today() + pd.Timedelta(days=self.schedule.steps)\n",
        "        # Process news\n",
        "        due_news = [event for event in self.future_news if event[3] <= current_date]\n",
        "        for event in due_news:\n",
        "            country_code, party_name, news, _ = event\n",
        "            # Integrate the news into the corresponding GovernmentAgent\n",
        "            gov_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == country_code]\n",
        "            for gov in gov_agents:\n",
        "                if gov.governing_party == party_name:\n",
        "                    if gov.news_data is not None:\n",
        "                        new_row = {'description': news, 'Sentiment': 'Neutral', 'Entities': []}  # Sentiment and Entities can be processed further\n",
        "                        gov.news_data = gov.news_data.append(new_row, ignore_index=True)\n",
        "                    else:\n",
        "                        # Create a new DataFrame if none exists\n",
        "                        gov.news_data = pd.DataFrame([{'description': news, 'Sentiment': 'Neutral', 'Entities': []}])\n",
        "            # Remove the processed event\n",
        "            self.future_news.remove(event)\n",
        "            logging.info(f\"Processed future news for {party_name} in {country_code}: {news[:60]}...\")\n",
        "\n",
        "        # Process social media\n",
        "        due_posts = [event for event in self.future_social_media if event[3] <= current_date]\n",
        "        for event in due_posts:\n",
        "            country_code, party_name, post, _ = event\n",
        "            # Integrate the post into the corresponding GovernmentAgent\n",
        "            gov_agents = [agent for agent in self.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == country_code]\n",
        "            for gov in gov_agents:\n",
        "                if gov.governing_party == party_name:\n",
        "                    if gov.tweets_data is not None:\n",
        "                        new_row = {'text': post, 'Sentiment': 'Neutral', 'Entities': []}  # Sentiment and Entities can be processed further\n",
        "                        gov.tweets_data = gov.tweets_data.append(new_row, ignore_index=True)\n",
        "                    else:\n",
        "                        # Create a new DataFrame if none exists\n",
        "                        gov.tweets_data = pd.DataFrame([{'text': post, 'Sentiment': 'Neutral', 'Entities': []}])\n",
        "            # Remove the processed event\n",
        "            self.future_social_media.remove(event)\n",
        "            logging.info(f\"Processed future social media post for {party_name} in {country_code}: {post[:60]}...\")\n",
        "\n",
        "    def run_batch(self, batch_size: int, branching_depth: int = 1):\n",
        "        \"\"\"\n",
        "        Runs a batch of simulation steps and generates future events based on outcomes.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): Number of simulation steps to run in this batch.\n",
        "            branching_depth (int): Current depth of branching.\n",
        "        \"\"\"\n",
        "        for _ in range(batch_size):\n",
        "            self.step()\n",
        "\n",
        "        # Analyze outcomes\n",
        "        trade_volumes = self.trade_volume_history[-batch_size:]\n",
        "        mean_volume = np.mean(trade_volumes)\n",
        "        median_volume = np.median(trade_volumes)\n",
        "        std_volume = np.std(trade_volumes)\n",
        "\n",
        "        # Identify 1-sigma away outcomes\n",
        "        lower_bound = mean_volume - std_volume\n",
        "        upper_bound = mean_volume + std_volume\n",
        "        notable_outliers = [vol for vol in trade_volumes if vol < (mean_volume - 2 * std_volume) or vol > (mean_volume + 2 * std_volume)]\n",
        "\n",
        "        # Generate events based on mean, median, and outliers\n",
        "        events = []\n",
        "        if mean_volume:\n",
        "            events.append(('Mean Outcome', mean_volume))\n",
        "        if median_volume:\n",
        "            events.append(('Median Outcome', median_volume))\n",
        "        if std_volume:\n",
        "            events.append(('1 Sigma Below', lower_bound))\n",
        "            events.append(('1 Sigma Above', upper_bound))\n",
        "        if notable_outliers:\n",
        "            events.append(('Notable Outliers', notable_outliers))\n",
        "\n",
        "        for event_type, outcome in events:\n",
        "            context_json = self.construct_event_context(event_type, outcome)\n",
        "            future_news = generate_future_news(context_json)\n",
        "            future_posts = generate_future_social_media(context_json)\n",
        "            # Branching based on outcome\n",
        "            if branching_depth < self.max_branching_depth:\n",
        "                for news in future_news:\n",
        "                    # Assign an implementation date within the next 6 months\n",
        "                    implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))\n",
        "                    # Add to model's event queue\n",
        "                    self.add_future_news('USA', 'Democratic Party', news, implementation_date)\n",
        "                for post in future_posts:\n",
        "                    implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))\n",
        "                    self.add_future_social_media('USA', 'Democratic Party', post, implementation_date)\n",
        "                logging.info(f\"Generated and integrated future events for event type '{event_type}' at branching depth {branching_depth}.\")\n",
        "            else:\n",
        "                logging.info(f\"Maximum branching depth reached. No further events generated for event type '{event_type}'.\")\n",
        "\n",
        "    def construct_event_context(self, event_type: str, outcome: Any) -> str:\n",
        "        \"\"\"\n",
        "        Constructs a context JSON string based on the event type and outcome.\n",
        "\n",
        "        Args:\n",
        "            event_type (str): The type of event (e.g., 'Mean Outcome').\n",
        "            outcome (Any): The outcome associated with the event.\n",
        "\n",
        "        Returns:\n",
        "            str: A JSON-formatted string containing the context.\n",
        "        \"\"\"\n",
        "        context = {\n",
        "            'Event_Type': event_type,\n",
        "            'Outcome': outcome,\n",
        "            'Country': 'USA',  # Assuming USA for simplicity; adjust as needed\n",
        "            'Party': 'Democratic Party'  # Assuming Democratic Party; adjust as needed\n",
        "        }\n",
        "        return json.dumps(context, indent=2)\n",
        "\n",
        "    # =================================\n",
        "    # 4. GPT-4 Integration Module\n",
        "    # =================================\n",
        "\n",
        "    def gather_context_for_party(processed_data, country_code, party_name):\n",
        "        \"\"\"\n",
        "        Gathers relevant data for a specific party within a country to create context for GPT-4.\n",
        "\n",
        "        Args:\n",
        "            processed_data (dict): The dictionary containing all processed data.\n",
        "            country_code (str): The ISO alpha-3 country code.\n",
        "            party_name (str): The name of the political party.\n",
        "\n",
        "        Returns:\n",
        "            str: A JSON-formatted string containing all relevant data.\n",
        "        \"\"\"\n",
        "        country_political_data = processed_data['political_data'].get(country_code, {})\n",
        "        political_parties = country_political_data.get('Political_Parties', pd.DataFrame())\n",
        "        upcoming_elections = country_political_data.get('Upcoming_Elections', pd.DataFrame())\n",
        "        current_policies = country_political_data.get('Current_Policies', pd.DataFrame())\n",
        "\n",
        "        news_df = processed_data['news_data'].get(country_code, pd.DataFrame())\n",
        "        social_media_df = processed_data['social_media_data'].get(country_code, pd.DataFrame())\n",
        "        legislation_df = processed_data['legislation_data'].get(country_code, pd.DataFrame())\n",
        "\n",
        "        # Extract party ideology\n",
        "        party_info = political_parties[political_parties['Party'] == party_name]\n",
        "        ideology = party_info['Ideology'].iloc[0] if not party_info.empty else 'Other'\n",
        "\n",
        "        # Compile recent news and social media sentiments\n",
        "        recent_news = news_df['Sentiment'].value_counts().to_dict() if not news_df.empty else {}\n",
        "        recent_social_media = social_media_df['Sentiment'].value_counts().to_dict() if not social_media_df.empty else {}\n",
        "\n",
        "        # Compile current policies\n",
        "        policies = current_policies.to_dict(orient='records') if not current_policies.empty else []\n",
        "\n",
        "        # Compile legislation\n",
        "        legislation = legislation_df.to_dict(orient='records') if not legislation_df.empty else []\n",
        "\n",
        "        # Compile upcoming elections\n",
        "        elections = upcoming_elections.to_dict(orient='records') if not upcoming_elections.empty else []\n",
        "\n",
        "        context = {\n",
        "            'Country': country_code,\n",
        "            'Party': party_name,\n",
        "            'Ideology': ideology,\n",
        "            'Recent_News_Sentiments': recent_news,\n",
        "            'Recent_Social_Media_Sentiments': recent_social_media,\n",
        "            'Current_Policies': policies,\n",
        "            'Legislation': legislation,\n",
        "            'Upcoming_Elections': elections\n",
        "        }\n",
        "\n",
        "        return json.dumps(context, indent=2)\n",
        "\n",
        "    def generate_disposition_and_modifier_matrices(context_json: str, scenarios: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Generates both the disposition matrix and the modifier matrix using GPT-4.\n",
        "\n",
        "        Args:\n",
        "            context_json (str): JSON-formatted string containing all relevant data for the party/government.\n",
        "            scenarios (List[str]): List of defined scenarios.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[pd.DataFrame, pd.DataFrame]: Disposition Matrix and Modifier Matrix DataFrames.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert political strategist tasked with generating two matrices for a political party based on the provided context. The matrices should adhere strictly to the defined lists below.\n",
        "\n",
        "        ### Comprehensive Lists\n",
        "\n",
        "        **1. Scenarios**\n",
        "\n",
        "        - **Economic Scenarios:**\n",
        "          - Recession\n",
        "          - Economic Boom\n",
        "          - Inflation Surge\n",
        "          - Trade Deficit Increase\n",
        "\n",
        "        - **Political Scenarios:**\n",
        "          - Upcoming Elections\n",
        "          - Political Scandals\n",
        "          - Leadership Changes\n",
        "\n",
        "        - **Social Scenarios:**\n",
        "          - Public Protests\n",
        "          - Shifts in Public Sentiment\n",
        "\n",
        "        - **Environmental Scenarios:**\n",
        "          - Natural Disasters\n",
        "          - Climate Change Policies\n",
        "\n",
        "        **2. Actions**\n",
        "\n",
        "        - **Economic Actions:**\n",
        "          - Increase Tariffs\n",
        "          - Decrease Tariffs\n",
        "          - Implement Subsidies\n",
        "          - Reduce Subsidies\n",
        "          - Increase Taxes\n",
        "          - Decrease Taxes\n",
        "          - Stimulate Economic Growth\n",
        "          - Implement Austerity Measures\n",
        "\n",
        "        - **Political Actions:**\n",
        "          - Launch Election Campaign\n",
        "          - Initiate Policy Reforms\n",
        "          - Handle Political Scandals\n",
        "          - Resign Leadership\n",
        "          - Appoint New Leaders\n",
        "\n",
        "        - **Social Actions:**\n",
        "          - Address Public Protests\n",
        "          - Launch Public Awareness Campaigns\n",
        "          - Implement Social Welfare Programs\n",
        "\n",
        "        - **Environmental Actions:**\n",
        "          - Implement Climate Change Policies\n",
        "          - Enhance Disaster Response Mechanisms\n",
        "          - Promote Renewable Energy Initiatives\n",
        "\n",
        "        - **Trade Actions:**\n",
        "          - Form Trade Alliances\n",
        "          - Finalize Trade Agreements\n",
        "          - Impose Trade Sanctions\n",
        "          - Lift Trade Sanctions\n",
        "\n",
        "        - **Defense and Security Actions:**\n",
        "          - Increase Defense Spending\n",
        "          - Decrease Defense Spending\n",
        "          - Strengthen Security Measures\n",
        "\n",
        "        **3. Parties/Countries/Industries/Companies**\n",
        "\n",
        "        - **Countries (ISO Alpha-3 Codes):**\n",
        "          - USA (United States of America)\n",
        "          - CAN (Canada)\n",
        "          - CHN (China)\n",
        "          - DEU (Germany)\n",
        "          - FRA (France)\n",
        "          - JPN (Japan)\n",
        "          - GBR (United Kingdom)\n",
        "          - ITA (Italy)\n",
        "          - RUS (Russia)\n",
        "          - IND (India)\n",
        "          - BRA (Brazil)\n",
        "          - AUS (Australia)\n",
        "          - KOR (South Korea)\n",
        "          - ESP (Spain)\n",
        "          - MEX (Mexico)\n",
        "\n",
        "        - **Political Parties:**\n",
        "          - **USA:**\n",
        "            - Democratic Party (Liberal)\n",
        "            - Republican Party (Conservative)\n",
        "          - **CAN:**\n",
        "            - Liberal Party (Liberal)\n",
        "            - Conservative Party (Conservative)\n",
        "          - **GER:**\n",
        "            - Christian Democratic Union (Conservative)\n",
        "            - Social Democratic Party (Liberal)\n",
        "          - *(Add more as needed)*\n",
        "\n",
        "        - **Industries:**\n",
        "          - Automotive\n",
        "          - Technology\n",
        "          - Energy\n",
        "          - Healthcare\n",
        "          - Manufacturing\n",
        "          - Agriculture\n",
        "          - Finance\n",
        "          - Telecommunications\n",
        "          - Pharmaceuticals\n",
        "          - Construction\n",
        "\n",
        "        - **Companies:**\n",
        "          - **Automotive:**\n",
        "            - General Motors (USA)\n",
        "            - Toyota (JPN)\n",
        "            - Volkswagen (DEU)\n",
        "          - **Technology:**\n",
        "            - Apple (USA)\n",
        "            - Samsung (KOR)\n",
        "            - Huawei (CHN)\n",
        "          - **Energy:**\n",
        "            - ExxonMobil (USA)\n",
        "            - Shell (NLD)\n",
        "            - BP (GBR)\n",
        "          - **Healthcare:**\n",
        "            - Pfizer (USA)\n",
        "            - Roche (CHE)\n",
        "            - Johnson & Johnson (USA)\n",
        "          - *(Add more as needed)*\n",
        "\n",
        "        ### Context\n",
        "\n",
        "        {context_json}\n",
        "\n",
        "        ### Scenarios\n",
        "\n",
        "        - {\"\\n- \".join(scenarios)}\n",
        "\n",
        "        ### Instructions\n",
        "\n",
        "        Based on the above context and scenarios, generate two CSV-formatted tables strictly using the terms defined in the Comprehensive Lists.\n",
        "\n",
        "        1. **Disposition Matrix** with columns: Decision_ID, Action, Scenario, Probability(%).\n",
        "        2. **Modifier Matrix** with columns: Relation_Type, Relation_Name, Scenario, Modifier_Value.\n",
        "\n",
        "        **Ensure that:**\n",
        "\n",
        "        - Each decision is unique and reflects the party's ideology, current policies, economic indicators, public sentiment, existing trade alliances, and relationships with partners and industries.\n",
        "        - Modifier values are normalized (e.g., 0.1 for +10%, -0.05 for -5%) and reflect how specific relationships influence the probability of actions under each scenario.\n",
        "        - **Only** use the scenarios, actions, parties, countries, industries, and companies defined in the Comprehensive Lists.\n",
        "\n",
        "        ### Output\n",
        "        ```\n",
        "        ### Disposition Matrix\n",
        "        Decision_ID,Action,Scenario,Probability(%)\n",
        "        1,Increase Tariffs,Recession,70\n",
        "        2,Form Trade Alliances,Economic Boom,50\n",
        "        3,Finalize Trade Agreements,Trade Deficit Increase,60\n",
        "        4,Decrease Tariffs,Economic Boom,30\n",
        "        5,Implement Subsidies,Recession,40\n",
        "        6,Implement Climate Change Policies,Climate Change Policies,80\n",
        "        7,Launch Election Campaign,Upcoming Elections,75\n",
        "        8,Handle Political Scandal,Political Scandals,65\n",
        "        9,Address Public Protests,Public Protests,55\n",
        "        10,Enhance Disaster Response,Natural Disasters,60\n",
        "\n",
        "        ### Modifier Matrix\n",
        "        Relation_Type,Relation_Name,Scenario,Modifier_Value\n",
        "        Alliance,EU,Recession,0.05\n",
        "        Alliance,CAN,Economic Boom,0.10\n",
        "        Industry,Automotive,Recession,0.15\n",
        "        Industry,Technology,Climate Change Policies,-0.10\n",
        "        ```\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates disposition matrices for political parties.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=3000,  # Adjust based on token usage\n",
        "                temperature=0.7,\n",
        "                n=1,\n",
        "                stop=[\"```\n",
        "```\"]\n",
        "            )\n",
        "            full_output = response.choices[0].message['content'].strip()\n",
        "\n",
        "            # Use regular expressions to extract the CSV sections\n",
        "            disposition_pattern = r\"### Disposition Matrix\\n(.*?)\\n\\n### Modifier Matrix\"\n",
        "            modifier_pattern = r\"### Modifier Matrix\\n(.*)\"\n",
        "            closing_pattern = r\"```\n",
        "```\"\n",
        "\n",
        "            disposition_match = re.search(disposition_pattern, full_output, re.DOTALL)\n",
        "            modifier_match = re.search(modifier_pattern, full_output, re.DOTALL)\n",
        "\n",
        "            if disposition_match and modifier_match:\n",
        "                disposition_csv = disposition_match.group(1).strip()\n",
        "                modifier_csv = modifier_match.group(1).strip()\n",
        "\n",
        "                # Parse Disposition Matrix\n",
        "                try:\n",
        "                    disposition_df = pd.read_csv(io.StringIO(disposition_csv))\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error parsing Disposition Matrix CSV: {e}\")\n",
        "                    disposition_df = pd.DataFrame()\n",
        "\n",
        "                # Parse Modifier Matrix\n",
        "                try:\n",
        "                    modifier_df = pd.read_csv(io.StringIO(modifier_csv))\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error parsing Modifier Matrix CSV: {e}\")\n",
        "                    modifier_df = pd.DataFrame()\n",
        "\n",
        "                # Validate DataFrames\n",
        "                expected_disposition_columns = ['Decision_ID', 'Action', 'Scenario', 'Probability(%)']\n",
        "                expected_modifier_columns = ['Relation_Type', 'Relation_Name', 'Scenario', 'Modifier_Value']\n",
        "\n",
        "                if not all(col in disposition_df.columns for col in expected_disposition_columns):\n",
        "                    logging.error(\"Disposition CSV does not contain the expected columns.\")\n",
        "                    disposition_df = pd.DataFrame()\n",
        "\n",
        "                if not all(col in modifier_df.columns for col in expected_modifier_columns):\n",
        "                    logging.error(\"Modifier CSV does not contain the expected columns.\")\n",
        "                    modifier_df = pd.DataFrame()\n",
        "\n",
        "                return disposition_df, modifier_df\n",
        "            else:\n",
        "                logging.error(\"Expected sections '### Disposition Matrix' and '### Modifier Matrix' not found in the response.\")\n",
        "                return pd.DataFrame(), pd.DataFrame()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error generating disposition and modifier matrices: {e}\")\n",
        "            return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # =================================\n",
        "    # 5. Synthesis of Future News and Social Media Posts\n",
        "    # =================================\n",
        "\n",
        "    def generate_future_news(context_json, forecast_period='6 months'):\n",
        "        \"\"\"\n",
        "        Generates plausible future news articles that could impact the simulation.\n",
        "\n",
        "        Args:\n",
        "            context_json (str): JSON-formatted string containing all relevant data for the country/party.\n",
        "            forecast_period (str): The forecasting period (e.g., '6 months').\n",
        "\n",
        "        Returns:\n",
        "            list: A list of generated news articles as strings.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are a journalist tasked with writing plausible future news articles that could impact the political and economic landscape of the following context within the next {forecast_period}.\n",
        "\n",
        "        Context:\n",
        "        {context_json}\n",
        "\n",
        "        Generate 5 news articles with headlines and brief descriptions.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a creative and insightful assistant that generates realistic future news articles based on provided context.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=2000,  # Adjust based on needs\n",
        "                temperature=0.7,\n",
        "                n=1,\n",
        "                stop=None\n",
        "            )\n",
        "            articles = response.choices[0].message['content']\n",
        "            # Split articles assuming they are separated by newlines or numbering\n",
        "            # This might require more sophisticated parsing based on GPT-4's output format\n",
        "            article_list = [article.strip() for article in articles.split('\\n') if article.strip()]\n",
        "            return article_list\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error generating future news: {e}\")\n",
        "            return []\n",
        "\n",
        "    def generate_future_social_media(context_json, forecast_period='6 months'):\n",
        "        \"\"\"\n",
        "        Generates plausible future social media posts that could impact the simulation.\n",
        "\n",
        "        Args:\n",
        "            context_json (str): JSON-formatted string containing all relevant data for the country/party.\n",
        "            forecast_period (str): The forecasting period (e.g., '6 months').\n",
        "\n",
        "        Returns:\n",
        "            list: A list of generated social media posts as strings.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are a social media manager creating plausible future social media posts that reflect public sentiment and events within the next {forecast_period}.\n",
        "\n",
        "        Context:\n",
        "        {context_json}\n",
        "\n",
        "        Generate 10 social media posts (e.g., tweets) that could influence or reflect the public's view on trade policies.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a creative assistant that generates realistic future social media posts based on provided context.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=1500,  # Adjust based on needs\n",
        "                temperature=0.7,\n",
        "                n=1,\n",
        "                stop=None\n",
        "            )\n",
        "            posts = response.choices[0].message['content']\n",
        "            # Split posts assuming they are separated by newlines or numbering\n",
        "            # This might require more sophisticated parsing based on GPT-4's output format\n",
        "            post_list = [post.strip() for post in posts.split('\\n') if post.strip()]\n",
        "            return post_list\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error generating future social media posts: {e}\")\n",
        "            return []\n",
        "\n",
        "    def integrate_future_events(model, country_code, party_name, future_news, future_posts):\n",
        "        \"\"\"\n",
        "        Integrates future news and social media posts into the simulation model.\n",
        "\n",
        "        Args:\n",
        "            model (TradeModelEnhanced): The simulation model instance.\n",
        "            country_code (str): The ISO alpha-3 country code.\n",
        "            party_name (str): The name of the political party.\n",
        "            future_news (list): List of future news articles.\n",
        "            future_posts (list): List of future social media posts.\n",
        "        \"\"\"\n",
        "        # Example: Schedule future events based on the forecast period\n",
        "        for news in future_news:\n",
        "            # Assign an implementation date within the next 6 months\n",
        "            implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))\n",
        "            # Add to model's event queue or relevant data structures\n",
        "            model.add_future_news(country_code, party_name, news, implementation_date)\n",
        "\n",
        "        for post in future_posts:\n",
        "            implementation_date = pd.Timestamp.today() + pd.Timedelta(days=random.randint(1, 180))\n",
        "            model.add_future_social_media(country_code, party_name, post, implementation_date)\n",
        "\n",
        "# =================================\n",
        "# 6. Simulation Model Enhancements\n",
        "# =================================\n",
        "\n",
        "def generate_and_assign_scenario_action_tables(processed_data, model):\n",
        "    \"\"\"\n",
        "    Generates scenario-action probability and modifier tables for all parties in all countries and assigns them to the simulation model.\n",
        "\n",
        "    Args:\n",
        "        processed_data (dict): The dictionary containing all processed data.\n",
        "        model (TradeModelEnhanced): The simulation model instance.\n",
        "    \"\"\"\n",
        "    for country_code, political_info in processed_data['political_data'].items():\n",
        "        parties_df = political_info.get('Political_Parties', pd.DataFrame())\n",
        "        for _, party_row in parties_df.iterrows():\n",
        "            party_name = party_row['Party']\n",
        "            context_json = gather_context_for_party(processed_data, country_code, party_name)\n",
        "            disposition_df, modifier_df = generate_disposition_and_modifier_matrices(context_json, model.get_scenarios())\n",
        "            if disposition_df.empty or modifier_df.empty:\n",
        "                logging.warning(f\"Failed to generate scenario-action tables for {party_name} in {country_code}.\")\n",
        "                continue\n",
        "            # Assign tables to the corresponding GovernmentAgent\n",
        "            gov_agents = [agent for agent in model.schedule.agents if isinstance(agent, GovernmentAgentEnhanced) and agent.country_code == country_code and agent.governing_party == party_name]\n",
        "            for gov in gov_agents:\n",
        "                gov.disposition_matrix = disposition_df\n",
        "                gov.modifier_matrix = modifier_df\n",
        "                logging.info(f\"Assigned scenario-action tables to {party_name} in {country_code}.\")\n",
        "\n",
        "    # Generate future events based on initial tables\n",
        "    for country_code, political_info in processed_data['political_data'].items():\n",
        "        parties_df = political_info.get('Political_Parties', pd.DataFrame())\n",
        "        for _, party_row in parties_df.iterrows():\n",
        "            party_name = party_row['Party']\n",
        "            context_json = gather_context_for_party(processed_data, country_code, party_name)\n",
        "            future_news = generate_future_news(context_json)\n",
        "            future_posts = generate_future_social_media(context_json)\n",
        "            integrate_future_events(model, country_code, party_name, future_news, future_posts)\n",
        "\n",
        "# =================================\n",
        "# 7. Additional Enhancements and Best Practices\n",
        "# =================================\n",
        "\n",
        "def initialize_simulation():\n",
        "    \"\"\"\n",
        "    Initializes the entire simulation workflow:\n",
        "    - Loads environment variables.\n",
        "    - Sets up logging.\n",
        "    - Acquires data.\n",
        "    - Processes data.\n",
        "    - Initializes the simulation model.\n",
        "    - Generates and assigns scenario-action tables.\n",
        "    - Runs the simulation.\n",
        "    \"\"\"\n",
        "    # Load environment variables\n",
        "    load_openai_api_key()\n",
        "\n",
        "    # Set up logging\n",
        "    setup_logging()\n",
        "\n",
        "    # Data Acquisition\n",
        "    data_acquisition = DataAcquisition()\n",
        "    raw_data = data_acquisition.fetch_all_data()\n",
        "\n",
        "    # Data Processing\n",
        "    data_processing = DataProcessing(raw_data)\n",
        "    data_processing.process_all_data()\n",
        "    processed_data = data_processing.processed_data\n",
        "\n",
        "    # Initialize the enhanced Trade Model\n",
        "    model = TradeModelEnhanced(\n",
        "        processed_data=processed_data,\n",
        "        political_data=raw_data.get('political_data', {}),\n",
        "        news_data=raw_data.get('news_data', {}),\n",
        "        social_media_data=raw_data.get('social_media_data', {}),\n",
        "        legislation_data=raw_data.get('legislation_data', {})\n",
        "    )\n",
        "\n",
        "    # Generate and assign scenario-action tables\n",
        "    generate_and_assign_scenario_action_tables(processed_data, model)\n",
        "\n",
        "    # Initialize branching parameters\n",
        "    model.current_branching_depth = 0\n",
        "\n",
        "    return model\n",
        "\n",
        "# =================================\n",
        "# 8. Comprehensive Unit Testing\n",
        "# =================================\n",
        "\n",
        "class TestTradeModelEnhanced(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Sample processed data\n",
        "        self.processed_data = {\n",
        "            'world_bank': pd.DataFrame({\n",
        "                'Country': ['USA'],\n",
        "                'Year': [2023],\n",
        "                'GDP': [21000000000000],\n",
        "                'Trade_Percentage_GDP': [25],\n",
        "                'GDP_Growth': [2.3]\n",
        "            }),\n",
        "            'political_data': {\n",
        "                'USA': {\n",
        "                    'Political_Parties': pd.DataFrame({\n",
        "                        'Party': ['Democratic Party', 'Republican Party'],\n",
        "                        'Ideology': ['Liberal', 'Conservative']\n",
        "                    }),\n",
        "                    'Upcoming_Elections': pd.DataFrame({\n",
        "                        'Date_Upcoming_Election': ['2024-11-05']\n",
        "                    }),\n",
        "                    'Current_Policies': pd.DataFrame({\n",
        "                        'Policy': ['Tariff Rate Adjustment', 'Climate Change Initiative'],\n",
        "                        'Status': ['Increasing', 'Active']\n",
        "                    })\n",
        "                }\n",
        "            },\n",
        "            'news_data': {\n",
        "                'USA': pd.DataFrame({\n",
        "                    'description': [\n",
        "                        'The economy is booming with low unemployment rates.',\n",
        "                        'Public sentiment is turning negative due to rising inflation.',\n",
        "                        'The Democratic Party launches a new green energy initiative.'\n",
        "                    ],\n",
        "                    'Sentiment': ['Positive', 'Negative', 'Positive'],\n",
        "                    'Entities': [['economy', 'unemployment'], ['public sentiment', 'inflation'], ['Democratic Party', 'green energy']]\n",
        "                })\n",
        "            },\n",
        "            'social_media_data': {\n",
        "                'USA': pd.DataFrame({\n",
        "                    'text': [\n",
        "                        '@user1 Loving the new trade policies!',\n",
        "                        '@user2 Concerned about the recent tariff increases.',\n",
        "                        '@user3 Supportive of the green energy initiatives!'\n",
        "                    ],\n",
        "                    'Sentiment': ['Positive', 'Negative', 'Positive'],\n",
        "                    'Entities': [['trade policies'], ['tariff increases'], ['green energy initiatives']]\n",
        "                })\n",
        "            },\n",
        "            'legislation_data': {\n",
        "                'USA': pd.DataFrame({\n",
        "                    'Bill_Name': ['Trade Cooperation Act', 'Green Energy Promotion Act'],\n",
        "                    'Status': ['active', 'active'],\n",
        "                    'Date_Introduced': ['2023-03-22', '2022-07-15']\n",
        "                })\n",
        "            },\n",
        "            'Trade_Alliances': {\n",
        "                'USA': ['EU', 'CAN']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Define scenarios\n",
        "        self.scenarios = [\n",
        "            'Recession',\n",
        "            'Economic Boom',\n",
        "            'Inflation Surge',\n",
        "            'Trade Deficit Increase',\n",
        "            'Upcoming Elections',\n",
        "            'Political Scandals',\n",
        "            'Leadership Changes',\n",
        "            'Public Protests',\n",
        "            'Shifts in Public Sentiment',\n",
        "            'Natural Disasters',\n",
        "            'Climate Change Policies'\n",
        "        ]\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = TradeModelEnhanced(\n",
        "            processed_data=self.processed_data,\n",
        "            political_data=self.processed_data.get('political_data', {}),\n",
        "            news_data=self.processed_data.get('news_data', {}),\n",
        "            social_media_data=self.processed_data.get('social_media_data', {}),\n",
        "            legislation_data=self.processed_data.get('legislation_data', {})\n",
        "        )\n",
        "\n",
        "    @patch('openai.ChatCompletion.create')\n",
        "    def test_generate_disposition_and_modifier_matrices(self, mock_create):\n",
        "        # Mock GPT-4 response with both matrices\n",
        "        mock_csv = \"\"\"### Disposition Matrix\n",
        "Decision_ID,Action,Scenario,Probability(%)\n",
        "1,Increase Tariffs,Recession,70\n",
        "2,Form Trade Alliances,Economic Boom,50\n",
        "3,Finalize Trade Agreements,Trade Deficit Increase,60\n",
        "4,Decrease Tariffs,Economic Boom,30\n",
        "5,Implement Subsidies,Recession,40\n",
        "6,Implement Climate Change Policies,Climate Change Policies,80\n",
        "7,Launch Election Campaign,Upcoming Elections,75\n",
        "8,Handle Political Scandal,Political Scandals,65\n",
        "9,Address Public Protests,Public Protests,55\n",
        "10,Enhance Disaster Response,Natural Disasters,60\n",
        "\n",
        "### Modifier Matrix\n",
        "Relation_Type,Relation_Name,Scenario,Modifier_Value\n",
        "Alliance,EU,Recession,0.05\n",
        "Alliance,CAN,Economic Boom,0.10\n",
        "Industry,Automotive,Recession,0.15\n",
        "Industry,Technology,Climate Change Policies,-0.10\n",
        "\"\"\"\n",
        "        mock_response = MagicMock()\n",
        "        mock_response.choices = [MagicMock()]\n",
        "        mock_response.choices[0].message = {'content': mock_csv}\n",
        "        mock_create.return_value = mock_response\n",
        "\n",
        "        # Compile context\n",
        "        context_json = gather_context_for_party(self.processed_data, 'USA', 'Democratic Party')\n",
        "\n",
        "        # Generate matrices\n",
        "        disposition_df, modifier_df = generate_disposition_and_modifier_matrices(context_json, self.scenarios)\n",
        "\n",
        "        # Assertions\n",
        "        self.assertFalse(disposition_df.empty, \"Disposition DataFrame should not be empty.\")\n",
        "        self.assertFalse(modifier_df.empty, \"Modifier DataFrame should not be empty.\")\n",
        "        self.assertListEqual(list(disposition_df.columns), ['Decision_ID', 'Action', 'Scenario', 'Probability(%)'], \"Disposition DataFrame columns mismatch.\")\n",
        "        self.assertListEqual(list(modifier_df.columns), ['Relation_Type', 'Relation_Name', 'Scenario', 'Modifier_Value'], \"Modifier DataFrame columns mismatch.\")\n",
        "        self.assertEqual(len(disposition_df), 10, \"Disposition DataFrame should have 10 rows.\")\n",
        "        self.assertEqual(len(modifier_df), 4, \"Modifier DataFrame should have 4 rows.\")\n",
        "        self.assertEqual(disposition_df.iloc[0]['Action'], 'Increase Tariffs', \"First action should be 'Increase Tariffs'.\")\n",
        "        self.assertEqual(modifier_df.iloc[0]['Modifier_Value'], 0.05, \"First modifier value should be 0.05.\")\n",
        "\n",
        "    @patch('openai.ChatCompletion.create')\n",
        "    def test_agent_decision_making(self, mock_create):\n",
        "        # Mock GPT-4 response with both matrices\n",
        "        mock_csv = \"\"\"### Disposition Matrix\n",
        "Decision_ID,Action,Scenario,Probability(%)\n",
        "1,Increase Tariffs,Recession,70\n",
        "2,Form Trade Alliances,Economic Boom,50\n",
        "3,Finalize Trade Agreements,Trade Deficit Increase,60\n",
        "4,Decrease Tariffs,Economic Boom,30\n",
        "5,Implement Subsidies,Recession,40\n",
        "6,Implement Climate Change Policies,Climate Change Policies,80\n",
        "7,Launch Election Campaign,Upcoming Elections,75\n",
        "8,Handle Political Scandal,Political Scandals,65\n",
        "9,Address Public Protests,Public Protests,55\n",
        "10,Enhance Disaster Response,Natural Disasters,60\n",
        "\n",
        "### Modifier Matrix\n",
        "Relation_Type,Relation_Name,Scenario,Modifier_Value\n",
        "Alliance,EU,Recession,0.05\n",
        "Alliance,CAN,Economic Boom,0.10\n",
        "Industry,Automotive,Recession,0.15\n",
        "Industry,Technology,Climate Change Policies,-0.10\n",
        "\"\"\"\n",
        "        mock_response = MagicMock()\n",
        "        mock_response.choices = [MagicMock()]\n",
        "        mock_response.choices[0].message = {'content': mock_csv}\n",
        "        mock_create.return_value = mock_response\n",
        "\n",
        "        # Compile context\n",
        "        context_json = gather_context_for_party(self.processed_data, 'USA', 'Democratic Party')\n",
        "\n",
        "        # Generate matrices\n",
        "        disposition_df, modifier_df = generate_disposition_and_modifier_matrices(context_json, self.scenarios)\n",
        "\n",
        "        # Initialize agent\n",
        "        agent = GovernmentAgentEnhanced(\n",
        "            unique_id=123456,\n",
        "            model=self.model,\n",
        "            country_code='USA',\n",
        "            economic_indicators=self.processed_data['world_bank'][self.processed_data['world_bank']['Country'] == 'USA'].iloc[0].to_dict(),\n",
        "            political_parties=self.processed_data['political_data']['USA']['Political_Parties'],\n",
        "            upcoming_elections=self.processed_data['political_data']['USA']['Upcoming_Elections'],\n",
        "            current_policies=self.processed_data['political_data']['USA']['Current_Policies'],\n",
        "            disposition_matrix=disposition_df,\n",
        "            modifier_matrix=modifier_df\n",
        "        )\n",
        "\n",
        "        # Mock current scenarios\n",
        "        current_scenarios = ['Recession', 'Economic Boom']\n",
        "\n",
        "        # Mock execute_action method\n",
        "        with patch.object(agent, 'execute_action') as mock_execute_action:\n",
        "            # Seed random number generator for reproducibility\n",
        "            random.seed(0)\n",
        "            agent.implement_policies_based_on_dispositions(current_scenarios)\n",
        "            # Check if execute_action was called\n",
        "            self.assertTrue(mock_execute_action.called, \"execute_action should be called based on disposition probabilities and modifiers.\")\n",
        "            # Further assertions can be made based on the actions expected\n",
        "\n",
        "    def tearDown(self):\n",
        "        # Clean up any temporary files or resources if needed\n",
        "        pass\n",
        "\n",
        "# =================================\n",
        "# 9. Main Execution\n",
        "# =================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Initialize simulation\n",
        "    model = initialize_simulation()\n",
        "\n",
        "    # Define simulation parameters\n",
        "    total_batches = 5  # Number of batches to run\n",
        "    batch_size = 50     # Number of simulation steps per batch\n",
        "    branching_depth = 1  # Initial branching depth\n",
        "\n",
        "    for batch in range(total_batches):\n",
        "        logging.info(f\"Starting simulation batch {batch + 1}/{total_batches} at branching depth {branching_depth}.\")\n",
        "        model.run_batch(batch_size, branching_depth)\n",
        "        branching_depth += 1  # Increment depth for next batch\n",
        "        if branching_depth > model.max_branching_depth:\n",
        "            logging.info(f\"Reached maximum branching depth of {model.max_branching_depth}. Stopping further branching.\")\n",
        "            break\n",
        "\n",
        "    # Optionally, run unit tests\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
      ],
      "metadata": {
        "id": "HzN3qMenh-mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1YY-3cd7-uK42XZ1bHws936RDFHUIT2Xu?usp=sharing\n",
        "\n",
        "https://chatgpt.com/share/6736d800-5424-8004-90d7-b516affd8735\n",
        "\n",
        "https://chatgpt.com/c/6735612d-3208-8004-a824-c0a96a53b270"
      ],
      "metadata": {
        "id": "8WzTc5JxiCLL"
      }
    }
  ]
}